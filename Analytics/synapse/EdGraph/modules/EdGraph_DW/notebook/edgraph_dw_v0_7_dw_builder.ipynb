{
    "nbformat": 4,
    "nbformat_minor": 2,
    "metadata": {
        "save_output": true,
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "import copy\r\n",
                "import pyspark.sql.types as T\r\n",
                "import re\r\n",
                "from pyspark.sql import functions as f\r\n",
                "\r\n",
                "\r\n",
                "class EdgraphDWHBuilder:\r\n",
                "    def __init__(self, original_metadata, stage3_db_name,stage_3_path, partitioning, spark, oea, logger, error_logger,lakeTableOverwrite,entity_freq_processor):\r\n",
                "        self.original_metadata = original_metadata\r\n",
                "        self.processed_metadata = copy.deepcopy(original_metadata)\r\n",
                "        self.stage_3_path = stage_3_path\r\n",
                "        self.stage3_db_name = stage3_db_name\r\n",
                "        self.partitioning = partitioning\r\n",
                "        self.spark = spark\r\n",
                "        self.oea = oea\r\n",
                "        self.logger = logger\r\n",
                "        self.base_tables = []\r\n",
                "        self.error_logger = error_logger\r\n",
                "        self.pipelineExecutionId = 'TEST_' + error_logger.generate_random_alphanumeric(10)\r\n",
                "        self.lakeTableOverwrite = lakeTableOverwrite\r\n",
                "        self.entity_freq_processor = entity_freq_processor\r\n",
                "\r\n",
                "    def return_primary_key(self, table_name, columns):\r\n",
                "        # TODO: Revise Logic\r\n",
                "        if table_name == 'DimSchool':\r\n",
                "            primary_key = 'SchoolHKey'\r\n",
                "        elif table_name == 'DimStudentRace':\r\n",
                "            primary_key = 'StudentRaceHKey'\r\n",
                "        elif table_name == 'DimDate':\r\n",
                "            primary_key = 'DateKey'\r\n",
                "        elif table_name == 'DimSchoolYear':\r\n",
                "            primary_key = 'SchoolYearShort'\r\n",
                "        elif table_name == 'DimLearningStandard':\r\n",
                "            primary_key = ['LearningStandardHKey', 'ParentLearningStandardHKey']\r\n",
                "        elif table_name == 'DimObjectiveAssessment':\r\n",
                "            primary_key = ['ObjectiveAssessmentHKey', 'ParentObjectiveAssessmentHKey']\r\n",
                "        elif table_name == 'UserAuthorization':\r\n",
                "            primary_key = 'UserKey'\r\n",
                "        elif table_name == 'Parameter':\r\n",
                "            primary_key = 'ParameterId'\r\n",
                "        elif table_name == 'Goals':\r\n",
                "            primary_key = 'GoalId'\r\n",
                "        elif table_name == 'DescriptorConstant':\r\n",
                "            primary_key = 'DescriptorConstantId'\r\n",
                "        elif table_name == 'ExecutionDuration':\r\n",
                "            primary_key = 'ExecutionDurationId'\r\n",
                "        elif table_name == 'ExplicitStudentDataAuthorization':\r\n",
                "            primary_key = 'UserKey'\r\n",
                "        elif table_name == 'ExecutionAudit':\r\n",
                "            primary_key = 'ExecutionAuditId'\r\n",
                "        else:\r\n",
                "            table_name_mask = (table_name[:4].lower() == 'fact') or (table_name == 'DimAssessmentSection') or (table_name == 'DataAuthorization') or (table_name == 'DimObjectiveAssessmentPerformanceLevel') or (table_name == 'DimAssessmentPerformanceLevel') or (table_name == 'DimAssessmentAcademicSubject') or (table_name == 'DimAssessmentAssessedGradeLevel')\r\n",
                "            if table_name_mask:\r\n",
                "                fact_pk = []\r\n",
                "                for column in columns:\r\n",
                "                    if column.lower().endswith('skey'):\r\n",
                "                        primary_key = column\r\n",
                "                        fact_pk.append(primary_key)\r\n",
                "                    else:\r\n",
                "                        pass\r\n",
                "                return fact_pk\r\n",
                "\r\n",
                "            else:\r\n",
                "                for column in columns:\r\n",
                "                    if column.lower().endswith('hkey'):\r\n",
                "                        primary_key = column\r\n",
                "                        break\r\n",
                "                    else:\r\n",
                "                        primary_key = 'PLACEHOLDER'\r\n",
                "            \r\n",
                "        return primary_key\r\n",
                "    \r\n",
                "    def insert_default_record(self, \r\n",
                "                            df, \r\n",
                "                            hKeyDefaults=[\"1900\", \"Not Specified\", \"NotSpecified\"]):\r\n",
                "        default_values = {}\r\n",
                "        binary_cols = []\r\n",
                "        hKey_cols = []\r\n",
                "        for column in df.columns:\r\n",
                "            # FIXME 08-02-2024 (added separate default record condition for schoolyear and \"date\")\r\n",
                "            if column.lower() == \"schoolyear\":\r\n",
                "                default_values[column] = '1900'\r\n",
                "            elif column.lower().endswith(\"date\"):\r\n",
                "                default_values[column] = \"1900-01-01\"\r\n",
                "            elif column.lower().endswith(\"hkey\"):\r\n",
                "                hKey_cols.append(column)\r\n",
                "            elif column.lower().endswith(\"skey\"):\r\n",
                "                default_values[column] = -1\r\n",
                "            else:\r\n",
                "                column_dtype = df.schema[column].dataType\r\n",
                "                if isinstance(column_dtype, T.StringType):\r\n",
                "                    default_values[column] = \"Not Specified\"\r\n",
                "                elif isinstance(column_dtype, T.TimestampType):\r\n",
                "                    default_values[column] = \"1900-01-01\"\r\n",
                "                elif isinstance(column_dtype, (T.IntegerType, T.LongType, T.ShortType, T.ByteType)):\r\n",
                "                    default_values[column] = -1\r\n",
                "                elif isinstance(column_dtype, T.DoubleType):\r\n",
                "                    default_values[column] = -1\r\n",
                "                elif isinstance(column_dtype, T.BooleanType):\r\n",
                "                    default_values[column] = False\r\n",
                "                elif isinstance(column_dtype, T.BinaryType):\r\n",
                "                    binary_cols.append(column)\r\n",
                "                else:\r\n",
                "                    default_values[column] = 'Not Specified'\r\n",
                "\r\n",
                "        default_record_df = self.spark.createDataFrame([default_values])\r\n",
                "        \r\n",
                "        for hKey_col in hKey_cols:\r\n",
                "        # TODO: Revise Logic for HKey\r\n",
                "            default_record_df = default_record_df.withColumn(hKey_col, F.sha2(F.concat(*[F.lit(val) for val in hKeyDefaults]), 256))\r\n",
                "\r\n",
                "        for binary_col in binary_cols:\r\n",
                "        # TODO: Revise Logic for Binary\r\n",
                "            default_record_df = default_record_df.withColumn(binary_col, F.sha2(F.concat(*[F.lit(val) for val in hKeyDefaults]), 256))        \r\n",
                "        \r\n",
                "        default_df = df.unionByName(default_record_df)\r\n",
                "        return default_df\r\n",
                "\r\n",
                "    def return_schema_tables_in_order(self, schema_metadata):\r\n",
                "        sorted_tables = sorted(schema_metadata['tables'].items(), key=lambda x: x[1]['table_order'])\r\n",
                "        sorted_tables_dict = dict(sorted_tables)\r\n",
                "\r\n",
                "        for table_name, table_value in sorted_tables_dict.items():\r\n",
                "            if table_value['table_order'] < 0:\r\n",
                "                self.base_tables.append(table_name)\r\n",
                "            else:\r\n",
                "                break\r\n",
                "        sorted_schema_metadata = {\r\n",
                "            #'parameters': schema_metadata['parameters'],\r\n",
                "            'tables': sorted_tables_dict\r\n",
                "        }\r\n",
                "        return sorted_schema_metadata\r\n",
                "\r\n",
                "    def return_table_queries_in_order(self, table_metadata):\r\n",
                "        sorted_queries = sorted(table_metadata['queries'], key=lambda x: x['order'])\r\n",
                "        sorted_query_strings = [query['query'] for query in sorted_queries]\r\n",
                "        sorted_query_params = [query.get('query_params', False) for query in sorted_queries]\r\n",
                "        return sorted_query_strings, sorted_query_params\r\n",
                "\r\n",
                "    def reorder_metadata_schemas(self):\r\n",
                "        sorted_schemas = {}\r\n",
                "        for schema_name in self.processed_metadata['metadata']['build-assets']['schemas'].keys():\r\n",
                "            sorted_schema_metadata = self.return_schema_tables_in_order(\r\n",
                "                self.processed_metadata['metadata']['build-assets']['schemas'][schema_name]\r\n",
                "            )\r\n",
                "            sorted_schemas[schema_name] = sorted_schema_metadata\r\n",
                "        self.processed_metadata['metadata']['build-assets']['schemas'] = sorted_schemas\r\n",
                "\r\n",
                "    def return_schema_queries_in_order(self, schema_name):\r\n",
                "        schema_metadata = self.processed_metadata['metadata']['build-assets']['schemas'][schema_name]\r\n",
                "        sorted_queries = {}\r\n",
                "        sorted_params = {}\r\n",
                "        for table_name, table_metadata in schema_metadata['tables'].items():\r\n",
                "            sorted_table_queries, sorted_table_queries_params = self.return_table_queries_in_order(table_metadata)\r\n",
                "            sorted_queries[table_name] = sorted_table_queries\r\n",
                "            sorted_params[table_name] = sorted_table_queries_params\r\n",
                "        return sorted_queries, sorted_params\r\n",
                "\r\n",
                "    def parameterize_table_queries(self, schema_queries, schema_queries_params, table_name, query_params=None, **kwargs):\r\n",
                "        if query_params is None:\r\n",
                "            query_params = {}\r\n",
                "\r\n",
                "        queries = schema_queries[table_name]\r\n",
                "        params = schema_queries_params[table_name]\r\n",
                "        parameterized_queries = []\r\n",
                "        for index, query in enumerate(queries):\r\n",
                "            \r\n",
                "            # query = query.replace(\"{stage3_db_name}.dbo_\", 'dbo_vw_')\r\n",
                "            # query = query.replace(\"{stage3_db_name}.config_\", 'config_vw_')\r\n",
                "            \r\n",
                "            # query = query.replace(\"{stage3_db_name}.auth_\", 'auth_vw_')\r\n",
                "            # query = query.replace(\"{stage3_db_name}.\", 'dbo_vw_')\r\n",
                "            # #query = query.replace(\"{stage3_db_name}.EducationOrganization\", 'dbo_vw_EducationOrganization')\r\n",
                "\r\n",
                "            # query = query.replace(\"{base_table_db_name}.dbo_\", 'dbo_vw_')\r\n",
                "            # query = query.replace(\"{stage3_db_name}.\", 'dbo_vw_')\r\n",
                "            # query = query.replace(\"{base_table_db_name}.config_\", 'config_vw_')\r\n",
                "            # query = query.replace(\"{base_table_db_name}.auth_\", 'auth_vw_')\r\n",
                "            #query = query.replace(\"{base_table_db_name}.EducationOrganization\", 'dbo_vw_EducationOrganization')\r\n",
                "            for key, value in kwargs.items():\r\n",
                "                if key != 'query_params':\r\n",
                "                    query = query.replace(f\"{{{key}}}\", value)\r\n",
                "            if (params[index]) and (query_params):\r\n",
                "                for key, value in query_params.items():\r\n",
                "                    query = query.replace(f\"{{{key}}}\", value)\r\n",
                "\r\n",
                "            query = query.replace(f\"{stage3_db_name}.dbo_\", 'dbo_vw_')\r\n",
                "            query = query.replace(f\"{stage3_db_name}.config_\", 'config_vw_')\r\n",
                "            \r\n",
                "            query = query.replace(f\"{stage3_db_name}.auth_\", 'auth_vw_')\r\n",
                "            query = query.replace(f\"{stage3_db_name}.\", 'dbo_vw_')\r\n",
                "            #query = query.replace(\"{stage3_db_name}.EducationOrganization\", 'dbo_vw_EducationOrganization')\r\n",
                "\r\n",
                "            query = query.replace(f\"{base_table_db_name}.dbo_\", 'dbo_vw_')\r\n",
                "            query = query.replace(f\"{stage3_db_name}.\", 'dbo_vw_')\r\n",
                "            query = query.replace(f\"{base_table_db_name}.config_\", 'config_vw_')\r\n",
                "            query = query.replace(f\"{base_table_db_name}.auth_\", 'auth_vw_')\r\n",
                "            parameterized_queries.append(query)\r\n",
                "        return parameterized_queries\r\n",
                "\r\n",
                "    def execute_table_queries(self, schema_name, table_name, queries,stage3_db_name, surrogate_key = True, insertion_type = 'append', explain = False):\r\n",
                "        # TODO: 2024-02-23: Error Logging for etl and staging queries \r\n",
                "        staging_list = self.generate_staging_list(len(queries), schema_name)\r\n",
                "        for step_prefix, query in zip(staging_list,queries):\r\n",
                "            self.execute_query(table_name, step_prefix,query,stage3_db_name, surrogate_key, insertion_type, explain)\r\n",
                "\r\n",
                "    def execute_query(self, table_name, step_prefix, query,stage3_db_name, surrogate_key = True, insertion_type = 'append', explain = False): \r\n",
                "        # TODO: 2024-02-23: Error Logging for etl and staging queries\r\n",
                "        start_time = datetime.now()\r\n",
                "        logger.info(f'Creating Temporary View - {step_prefix}_{table_name}')  \r\n",
                "        #logger.info(query)\r\n",
                "        query = query.replace(\"from\", \"FROM\")\r\n",
                "        query = query.replace(\"From\", \"FROM\")\r\n",
                "        query = query.replace(\",FROM\", ' FROM') \r\n",
                "        query = query.replace(\", FROM\", ' FROM')\r\n",
                "        query = query.replace(\",\\nFROM\", ' FROM')\r\n",
                "        query = query.replace(\",\\n\\nFROM\", ' FROM')\r\n",
                "        query = query.replace(\",\\n\\n\\nFROM\", ' FROM')\r\n",
                "        query = query.replace(\",\\n\\n\\n\\nFROM\", ' FROM')\r\n",
                "        df = self.spark.sql(query)\r\n",
                "        \r\n",
                "        # if not(step_prefix == 'fact_vw' or table_name[:4].lower() == 'fact' or table_name.lower() == 'dimdate' or step_prefix == 'config_vw' or step_prefix == 'config' or step_prefix == 'auth' or step_prefix == 'auth_vw'):\r\n",
                "        #         df = self.insert_default_record(df = df, \r\n",
                "        #                                     hKeyDefaults = [\"1900\", \"Not Specified\", \"Not Specified\"])\r\n",
                "        if table_name !='DimDate':\r\n",
                "            df = self.create_record_hash_column(df,essential_columns)\r\n",
                "\r\n",
                "        columns = df.columns \r\n",
                "        primary_key = self.return_primary_key(table_name, columns)\r\n",
                "\r\n",
                "        \r\n",
                "            \r\n",
                "\r\n",
                "        if table_name in self.base_tables:\r\n",
                "            insertion_type = 'overwrite'\r\n",
                "            primary_key = 'lakeId'\r\n",
                "        #Testing Change\r\n",
                "\r\n",
                "        table_path = f\"{self.stage_3_path}/{step_prefix[:-3]}/{table_name}\"\r\n",
                "        table_url = oea.to_url(table_path)\r\n",
                "\r\n",
                "        if DeltaTable.isDeltaTable(spark, table_url):\r\n",
                "        #Upsert Logic for HKey columns - 04-01-2024\r\n",
                "            for column in columns:\r\n",
                "                if column.lower().endswith('hkey'):\r\n",
                "                    insertion_type = 'upsert_EdGraph'\r\n",
                "\r\n",
                "            if insertion_type == 'upsert_EdGraph':\r\n",
                "                print(insertion_type)\r\n",
                "                skey_column = table_name.replace('Dim','')\r\n",
                "                skey_column = skey_column.replace('Fact','')\r\n",
                "                print(skey_column)\r\n",
                "                df_sink = spark.read.format('delta').load(table_url)\r\n",
                "                max_skey = df_sink.agg({f\"{skey_column}Skey\": \"max\"}).collect()[0][0]\r\n",
                "                print(max_skey)\r\n",
                "                df_sink = df_sink.drop(*[skey_column+'Skey','RECORD_HASH'])\r\n",
                "                df = df.drop(*[skey_column+'Skey','RECORD_HASH'])\r\n",
                "                df_subtract = df.subtract(df_sink) #diff dataframe\r\n",
                "                print(primary_key)\r\n",
                "                print(type(primary_key))\r\n",
                "                print(df_subtract.orderBy(\"ClassroomPositionHkey\").show())\r\n",
                "                print(df_sink.orderBy(\"ClassroomPositionHkey\").show())\r\n",
                "                print(df.orderBy(\"ClassroomPositionHkey\").show())\r\n",
                "                if type(primary_key) == 'list':\r\n",
                "                    df_modified_rows = df_subtract.join(df_sink, how='leftsemi', on=primary_key) #will print the modified lines\r\n",
                "                    df_new_rows      = df_subtract.join(df_sink, how='leftanti', on=primary_key) #will print the new lines\r\n",
                "                else:\r\n",
                "                    df_modified_rows = df_subtract.join(df_sink, how='leftsemi', on=primary_key) #will print the modified lines\r\n",
                "                    print('modified_lines')\r\n",
                "                    df_modified_rows = df_modified_rows.withColumn(f\"{skey_column}Skey\", lit(None).cast(IntegerType()))\r\n",
                "\r\n",
                "                    print(df_modified_rows.show())\r\n",
                "                    df_new_rows      = df_subtract.join(df_sink, how='leftanti', on=primary_key) #will print the new lines\r\n",
                "                    print('new rows')\r\n",
                "                    df_new_rows = df_new_rows.withColumn(f\"{skey_column}Skey\", lit(None).cast(IntegerType()))\r\n",
                "                    print(df_new_rows.show())\r\n",
                "\r\n",
                "                df = df_new_rows\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "        if ((insertion_type == 'append') or (insertion_type=='upsert_EdGraph')) and (step_prefix == 'dbo_vw') and (surrogate_key == True):\r\n",
                "            logger.info(f'[APPEND MODE ACTIVE FOR QUERY CONSTRUCT]: {step_prefix} ::: {table_name} ::: {primary_key}')\r\n",
                "            if type(primary_key) == list:\r\n",
                "                pk_statement = self.oea.return_pk_statement(primary_key)\r\n",
                "                skey = list()\r\n",
                "                for pk_component in primary_key:\r\n",
                "                    sk_component = pk_component[:-4] + 'SKey'\r\n",
                "                    skey.append(sk_component)\r\n",
                "            else:\r\n",
                "                skey = primary_key[:-4] + 'SKey'\r\n",
                "                if 'hkey' in primary_key.lower():\r\n",
                "                    surrogate_key = True\r\n",
                "                pk_statement = self.oea.return_pk_statement([primary_key])\r\n",
                "            \r\n",
                "            #2024-01-30 Change\r\n",
                "            table_name_mask = (table_name[:4].lower() == 'fact') or (table_name == 'DimAssessmentSection') or (table_name == 'DataAuthorization') or (table_name == 'DimObjectiveAssessmentPerformanceLevel') or (table_name == 'DimAssessmentPerformanceLevel') or (table_name == 'DimAssessmentAcademicSubject') or (table_name == 'DimAssessmentAssessedGradeLevel')\r\n",
                "            if table_name_mask:\r\n",
                "                surrogate_key = False\r\n",
                "\r\n",
                "            if surrogate_key:\r\n",
                "                # FIXME 2024-03-15 Fix for SKey under review Monotonically v/s PK\r\n",
                "                if type(primary_key) == list:\r\n",
                "                    for index, pk_component in enumerate(primary_key):\r\n",
                "                        sk_component = skey[index]\r\n",
                "                        #df = df.withColumn('row_id_label', (F.monotonically_increasing_id()))\r\n",
                "                        df = df.withColumn('row_id_label', (F.col(pk_component)))\r\n",
                "                        windowSpec = W.orderBy(\"row_id_label\")\r\n",
                "                        df = df.withColumn(\"row_id_label\", F.row_number().over(windowSpec))\r\n",
                "                        \r\n",
                "                        df = df.withColumn(sk_component, F.when((F.col(pk_component).isNull()) | (F.col(sk_component) == -1), -1).otherwise(F.col('row_id_label')))\r\n",
                "                        df = df.drop('row_id_label')\r\n",
                "                else:\r\n",
                "                    #df = df.withColumn('row_id_label', (F.monotonically_increasing_id()))\r\n",
                "                    df = df.withColumn('row_id_label', (F.col(primary_key)))\r\n",
                "                    windowSpec = W.orderBy(\"row_id_label\")\r\n",
                "                    df = df.withColumn(\"row_id_label\", F.row_number().over(windowSpec))\r\n",
                "                    \r\n",
                "                    \r\n",
                "                    \r\n",
                "                    df = df.withColumn(skey, F.when((F.col(primary_key).isNull()) | (F.col(skey) == -1), -1).otherwise(F.col('row_id_label')))\r\n",
                "                    df = df.withColumn(skey, F.when(F.col(skey) == -1, F.col(skey)).otherwise(F.col(skey) + max_skey))\r\n",
                "                    df = df.drop('row_id_label')\r\n",
                "\r\n",
                "        if explain and (step_prefix == 'dbo_vw' or step_prefix == 'config_vw' or step_prefix == 'auth_vw'):\r\n",
                "            df.explain()\r\n",
                "        df.createOrReplaceTempView(f\"{step_prefix}_{table_name}\")\r\n",
                "\r\n",
                "        if insertion_type == 'upsert_EdGraph':\r\n",
                "            df_modified_rows.createOrReplaceTempView(f\"{step_prefix}_{table_name}_modified\")\r\n",
                "\r\n",
                "        end_time = datetime.now()\r\n",
                "        numInputRows = df.count()\r\n",
                "\r\n",
                "        log_data = error_logger.create_log_dict(uniqueId = self.error_logger.generate_random_alphanumeric(10), # Generate a random 10-character alphanumeric value\r\n",
                "                                                pipelineExecutionId = self.pipelineExecutionId,#'TEST_1234',#executionId,\r\n",
                "                                                sparkSessionId = self.spark.sparkContext.applicationId,\r\n",
                "                                                stageName = \"edgraph-dwh: Migrate From S2R To Edgraph DWH Creating Spark Views\",\r\n",
                "                                                schemaFormat = 'edgraph-dwh',\r\n",
                "                                                entityType = 'Spark View',\r\n",
                "                                                entityName = table_name,\r\n",
                "                                                numInputRows = numInputRows,\r\n",
                "                                                totalNumOutputRows = 0,\r\n",
                "                                                numTargetRowsInserted = 0,\r\n",
                "                                                numTargetRowsUpdated = 0,\r\n",
                "                                                numRecordsSkipped = 0,\r\n",
                "                                                numRecordsDeleted = 0,\r\n",
                "                                                start_time = start_time,\r\n",
                "                                                end_time = end_time,\r\n",
                "                                                insertionType = 'Creating Spark Views')\r\n",
                "        error_logger.consolidate_logs(log_data,'entity')\r\n",
                "\r\n",
                "\r\n",
                "        #spark.catalog.cacheTable(f\"{step_prefix}_{table_name}\")\r\n",
                "    def threaded_dump_to_stage3_delta(self,input_tuple):\r\n",
                "        table_name, step_prefix, schema_queries_in_order, essential_columns, surrogate_key = input_tuple\r\n",
                "        \r\n",
                "        try:\r\n",
                "            status = 'Successful'\r\n",
                "            if step_prefix in ['dbo_vw']:\r\n",
                "                self.dump_to_stage3_delta_lake(step_prefix, table_name,essential_columns, surrogate_key = surrogate_key)\r\n",
                "                self.add_to_lake_db_stage3(step_prefix, table_name, overwrite = self.lakeTableOverwrite)\r\n",
                "                self.entity_freq_processor.update_lookup_df(table_name,status)\r\n",
                "            if step_prefix in ['config_vw', 'auth_vw']:\r\n",
                "                # # TODO: To Be Reviewed for skey\r\n",
                "                self.dump_to_stage3_delta_lake(step_prefix, table_name,essential_columns, surrogate_key = False)\r\n",
                "                self.add_to_lake_db_stage3(step_prefix, table_name, overwrite = self.lakeTableOverwrite)\r\n",
                "                self.entity_freq_processor.update_lookup_df(table_name,status)\r\n",
                "        except Exception as e:\r\n",
                "                logger.error(f\"An error occurred: {e}\")\r\n",
                "                status = \"Unsuccessful\"\r\n",
                "                self.entity_freq_processor.update_lookup_df(table_name,status)\r\n",
                "        \r\n",
                "\r\n",
                "    def upsert_with_logging(self,\r\n",
                "                            table_name, \r\n",
                "                            df, \r\n",
                "                            df_modified,\r\n",
                "                            destination_path, \r\n",
                "                            primary_key, \r\n",
                "                            partitioning, \r\n",
                "                            partitioning_cols,\r\n",
                "                            surrogate_key,\r\n",
                "                            insertion_type = 'upsert'):\r\n",
                "        \r\n",
                "        logger.info(f'--- PRIMARY KEY: {primary_key}')\r\n",
                "        start_time = datetime.now()\r\n",
                "\r\n",
                "        # FIXME: 2024-03-14: upsert is a misnomer\r\n",
                "        if insertion_type == 'upsert_EdGraph':\r\n",
                "            numInputRows, numOutputRows, numTargetRowsInserted, numTargetRowsUpdated = self.oea.upsert_EdGraph(df = df,\r\n",
                "                                                                    df_modified = df_modified, \r\n",
                "                                                                    destination_path = destination_path,#f\"{sink_general_path}\",\r\n",
                "                                                                    primary_key = primary_key,\r\n",
                "                                                                    partitioning = partitioning,\r\n",
                "                                                                    partitioning_cols = partitioning_cols,\r\n",
                "                                                                    de_duplicate = False)\r\n",
                "        if insertion_type == 'upsert':\r\n",
                "            #FIX ME - 2024-01-31  - 2024-03-07 Review again \r\n",
                "            numInputRows, numOutputRows, numTargetRowsInserted, numTargetRowsUpdated = self.oea.overwrite(df = df, \r\n",
                "                                                                    destination_path = destination_path,#f\"{sink_general_path}\",\r\n",
                "                                                                    primary_key = primary_key,\r\n",
                "                                                                    partitioning = partitioning,\r\n",
                "                                                                    partitioning_cols = partitioning_cols,\r\n",
                "                                                                    surrogate_key = False,\r\n",
                "                                                                    de_duplicate = False)\r\n",
                "        elif insertion_type == 'overwrite':\r\n",
                "            numInputRows, numOutputRows, numTargetRowsInserted, numTargetRowsUpdated = self.oea.overwrite(df = df, \r\n",
                "                                                                    destination_path = destination_path,#f\"{sink_general_path}\",\r\n",
                "                                                                    primary_key = primary_key,\r\n",
                "                                                                    partitioning = partitioning,\r\n",
                "                                                                    partitioning_cols = partitioning_cols,\r\n",
                "                                                                    surrogate_key = False,\r\n",
                "                                                                    de_duplicate = True)\r\n",
                "                      \r\n",
                "        end_time = datetime.now()\r\n",
                "        # NOTE: 2024-02-23: @Harsh - FYI\r\n",
                "\r\n",
                "        log_data = error_logger.create_log_dict(uniqueId = self.error_logger.generate_random_alphanumeric(10), # Generate a random 10-character alphanumeric value\r\n",
                "                                                pipelineExecutionId = self.pipelineExecutionId,#'TEST_1234',#executionId,\r\n",
                "                                                sparkSessionId = self.spark.sparkContext.applicationId,\r\n",
                "                                                stageName = \"edgraph-dwh: Migrate From S2R To Edgraph DWH Wrting to Delta\",\r\n",
                "                                                schemaFormat = 'edgraph-dwh',\r\n",
                "                                                entityType = destination_path.split('/')[-2],\r\n",
                "                                                entityName = table_name,\r\n",
                "                                                numInputRows = numInputRows,\r\n",
                "                                                totalNumOutputRows = numOutputRows,\r\n",
                "                                                numTargetRowsInserted = numTargetRowsInserted,\r\n",
                "                                                numTargetRowsUpdated = numTargetRowsUpdated,\r\n",
                "                                                numRecordsSkipped = 0,\r\n",
                "                                                numRecordsDeleted = 0,\r\n",
                "                                                start_time = start_time,\r\n",
                "                                                end_time = end_time,\r\n",
                "                                                insertionType = insertion_type)\r\n",
                "        error_logger.consolidate_logs(log_data,'entity')\r\n",
                "    \r\n",
                "    def create_record_hash_column(self, df,essential_columns):\r\n",
                "        # TODO: Under DEV\r\n",
                "        df_cols = [col for col in df.columns if col not in essential_columns]\r\n",
                "        non_struct_array_cols = [col for col in df_cols if not isinstance(df.schema[col].dataType, (StructType, ArrayType))]\r\n",
                "        non_struct_array_cols = non_struct_array_cols # FIXME: UNDER DEV\r\n",
                "        \r\n",
                "        logger.info(f'Creating hash column - RECORD_HASH - of the following columns: {non_struct_array_cols}')\r\n",
                "        record_hash_expr = [f.col(key_component).cast('string') for key_component in non_struct_array_cols]\r\n",
                "        df = df.withColumn(\"RECORD_HASH\",F.sha2(F.concat(*[F.concat(F.coalesce(column, F.lit('')), F.lit('_')) for column in record_hash_expr]), 256))\r\n",
                "        return df\r\n",
                "        \r\n",
                "\r\n",
                "    def get_df_latest_records_by_join(self, df, destination_path, func_enabled = False):\r\n",
                "        if not func_enabled:\r\n",
                "            return df\r\n",
                "        else:\r\n",
                "            logger.info('[EDFIOEACHILD REFINEMENT RECORD HASHING] JOIN BASED COMPARSIONS / DELTA COMPARISONS BEFORE UPSERT IS ENABLED')\r\n",
                "            df = df.withColumnRenamed('RECORD_VERSION', 'RECORD_VERSION_LEFT')\r\n",
                "            \r\n",
                "            df_destination = self.load(destination_path)            \r\n",
                "            df.createOrReplaceTempView('temp_vw_df_source_table')\r\n",
                "            df_destination.createOrReplaceTempView('temp_vw_df_destination_table')\r\n",
                "\r\n",
                "            query = f\"\"\"SELECT temp_vw_df_source_table.*, \r\n",
                "                               temp_vw_df_destination_table.RECORD_VERSION\r\n",
                "                        FROM temp_vw_df_source_table \r\n",
                "                        LEFT JOIN temp_vw_df_destination_table \r\n",
                "                            ON temp_vw_df_source_table.NATURAL_KEY_HASH = temp_vw_df_destination_table.NATURAL_KEY_HASH\r\n",
                "                        WHERE (temp_vw_df_source_table.RECORD_HASH != temp_vw_df_destination_table.RECORD_HASH)\r\n",
                "                        OR (temp_vw_df_destination_table.RECORD_HASH IS NULL)\r\n",
                "                    \"\"\"\r\n",
                "            df_joined = spark.sql(query)\r\n",
                "            df_joined = df_joined.withColumn('RECORD_VERSION', F.col('RECORD_VERSION') + 1)\r\n",
                "            df_joined = df_joined.drop('RECORD_VERSION_LEFT')\r\n",
                "\r\n",
                "            logger.info(f\"[EDFIOEACHILD REFINEMENT RECORD HASHING] --- NUM ROWS (SOURCE DELTA LAKE) - {df.count()}\")\r\n",
                "            logger.info(f\"[EDFIOEACHILD REFINEMENT RECORD HASHING] --- NUM ROWS (DESTINATION DELTA LAKE) - {df_destination.count()}\")\r\n",
                "            logger.info(f'[EDFIOEACHILD REFINEMENT RECORD HASHING] --- NUM ROWS (ACTUALLY MODIFIED) - {df_joined.count()}')\r\n",
                "            return df_joined\r\n",
                "\r\n",
                "    def dump_to_stage3_delta_lake(self, step_prefix, table_name,essential_columns, surrogate_key = True):\r\n",
                "        table_path = f\"{self.stage_3_path}/{step_prefix[:-3]}/{table_name}\"\r\n",
                "        table_url = self.oea.to_url(table_path)\r\n",
                "\r\n",
                "        df = self.spark.sql(f'select * from {step_prefix}_{table_name}')\r\n",
                "        \r\n",
                "\r\n",
                "        \r\n",
                "\r\n",
                "        #TODO: ADD primary key in the MetadataProcessor\r\n",
                "        columns = df.columns\r\n",
                "        primary_key = self.return_primary_key(table_name, columns)\r\n",
                "        # if table_name in ['FactSectionAttendance', 'FactSchoolAttendance']:\r\n",
                "        #     primary_key.append('EventDate')\r\n",
                "        # if table_name in ['FactSchoolEnrollment']:\r\n",
                "        #     primary_key.append('EntryDate')\r\n",
                "        insertion_type = 'None'\r\n",
                "\r\n",
                "        table_path = f\"{self.stage_3_path}/{step_prefix[:-3]}/{table_name}\"\r\n",
                "        table_url = oea.to_url(table_path)\r\n",
                "        if DeltaTable.isDeltaTable(spark, table_url):\r\n",
                "            \r\n",
                "            for column in columns:\r\n",
                "                if column.lower().endswith('hkey'):\r\n",
                "                    insertion_type = 'upsert_EdGraph'\r\n",
                "        \r\n",
                "        if insertion_type =='upsert_EdGraph':\r\n",
                "            pass\r\n",
                "\r\n",
                "        elif table_name in self.base_tables:\r\n",
                "            insertion_type = 'overwrite'\r\n",
                "            primary_key = 'lakeId'\r\n",
                "        else:\r\n",
                "            insertion_type = 'upsert'\r\n",
                "            #FIX TRY 2024-03-15\r\n",
                "            if not(step_prefix == 'fact_vw' or table_name[:4].lower() == 'fact' or table_name.lower() == 'dimdate' or step_prefix == 'config_vw' or step_prefix == 'config' or step_prefix == 'auth' or step_prefix == 'auth_vw'):\r\n",
                "                # df = self.insert_default_record(df = df, \r\n",
                "                #                            hKeyDefaults = [\"1900\", \"Not Specified\", \"Not Specified\"])\r\n",
                "                pass\r\n",
                "\r\n",
                "        for column in columns:\r\n",
                "            if column.lower().endswith('hkey'):\r\n",
                "                insertion_type = 'upsert_EdGraph'\r\n",
                "                \r\n",
                "        if insertion_type == 'upsert_EdGraph':\r\n",
                "            df_modified = spark.sql(f'select * from {step_prefix}_{table_name}_modified')\r\n",
                "        else:\r\n",
                "            df_modified = None\r\n",
                "\r\n",
                "\r\n",
                "        # FIXME: 2024-01-30  (TEMP FIX) TO PREVENT SKEY OVERWRITES\r\n",
                "        table_name_mask = (table_name[:4].lower() == 'fact') or (table_name == 'DimAssessmentSection') or (table_name == 'DataAuthorization') or (table_name == 'DimObjectiveAssessmentPerformanceLevel') or (table_name == 'DimAssessmentPerformanceLevel') or (table_name == 'DimAssessmentAcademicSubject') or (table_name == 'DimAssessmentAssessedGradeLevel')\r\n",
                "        if table_name_mask:\r\n",
                "            surrogate_key = False\r\n",
                "        self.upsert_with_logging(table_name = table_name, \r\n",
                "                                 df = df,\r\n",
                "                                 df_modified = df_modified,\r\n",
                "                                 destination_path = table_path, \r\n",
                "                                 primary_key = primary_key, \r\n",
                "                                 partitioning = self.partitioning, \r\n",
                "                                 partitioning_cols = [],\r\n",
                "                                 surrogate_key = surrogate_key,\r\n",
                "                                 insertion_type = insertion_type)\r\n",
                "    \r\n",
                "    def add_to_lake_db_stage3(self, step_prefix, table_name, overwrite = True):\r\n",
                "        spark.sql(f'CREATE DATABASE IF NOT EXISTS {self.stage3_db_name}')\r\n",
                "        table_prefix = step_prefix[:-3]\r\n",
                "        table_path = f\"{self.stage_3_path}/{table_prefix}/{table_name}\"\r\n",
                "        table_url = self.oea.to_url(table_path)\r\n",
                "\r\n",
                "        table_prefix = f'{table_prefix}_'\r\n",
                "        if table_name in self.base_tables:\r\n",
                "            table_prefix = ''\r\n",
                "\r\n",
                "        logger.info(f'Adding the table - {table_prefix}{table_name} to Lake DB')\r\n",
                "        if overwrite:\r\n",
                "            spark.sql(F\"DROP TABLE IF EXISTS {self.stage3_db_name}.{table_prefix}{table_name}\")\r\n",
                "        spark.sql(f\"CREATE TABLE IF NOT EXISTS {self.stage3_db_name}.{table_prefix}{table_name} using DELTA location '{table_url}'\")\r\n",
                "\r\n",
                "    def process_metadata(self):\r\n",
                "        self.reorder_metadata_schemas()\r\n",
                "\r\n",
                "    def generate_staging_list(self, length, schema_name):\r\n",
                "        staging_list = [\"etl_vw\"]\r\n",
                "        if length > 1:\r\n",
                "            staging_list.append(\"staging_vw\")\r\n",
                "        for i in range(3, length):\r\n",
                "            staging_list.append(f'staging_vw_{chr(ord(\"a\") + i - 2)}_')\r\n",
                "        staging_list.append(schema_name)\r\n",
                "\r\n",
                "        final_prefix = f'{schema_name}_vw'\r\n",
                "        staging_list[-1] = final_prefix\r\n",
                "        # FIXME: Automation has bug\r\n",
                "        if length == 2:\r\n",
                "            staging_list = ['etl_vw', final_prefix]\r\n",
                "        if length == 1:\r\n",
                "            staging_list = [final_prefix]\r\n",
                "        return staging_list"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "class SemanticViewsBuilder(EdgraphDWHBuilder):\r\n",
                "    def __init__(self, original_metadata, stage3_db_name, stage_3_path, partitioning, spark, oea, logger, error_logger,lakeTableOverwrite):\r\n",
                "        super().__init__(original_metadata, stage3_db_name, stage_3_path, partitioning, spark, oea, logger, error_logger,lakeTableOverwrite)\r\n",
                "    \r\n",
                "    def set_server_creds(self, server_name, database_name, user_name, password, driver='ODBC Driver 18 for SQL Server'):\r\n",
                "        self.server_name = server_name\r\n",
                "        self.database_name = database_name\r\n",
                "        self.user_name = user_name\r\n",
                "        self.password = password\r\n",
                "        self.driver = driver\r\n",
                "        self.connection = self.connect_to_database()\r\n",
                "\r\n",
                "    def connect_to_database(self):\r\n",
                "        connection_string = f\"DRIVER={{{self.driver}}};SERVER={self.server_name};DATABASE={self.database_name};UID={self.user_name};PWD={self.password};\"\r\n",
                "        return pyodbc.connect(connection_string)\r\n",
                "    \r\n",
                "    def execute_table_queries(self, schema_name, table_name, queries):\r\n",
                "        staging_list = self.generate_staging_list(len(queries), schema_name)\r\n",
                "        for step_prefix, query in zip(staging_list,queries):\r\n",
                "            self.execute_query(query)\r\n",
                "\r\n",
                "    def execute_query(self, query, isResult = False):\r\n",
                "        query = query.replace(\"from\", \"FROM\")\r\n",
                "        query = query.replace(\"From\", \"FROM\")\r\n",
                "        query = query.replace(\",FROM\", ' FROM') \r\n",
                "        query = query.replace(\", FROM\", ' FROM')\r\n",
                "        query = query.replace(\",\\nFROM\", ' FROM')\r\n",
                "        query = query.replace(\",\\n\\nFROM\", ' FROM')\r\n",
                "        query = query.replace(\",\\n\\n\\nFROM\", ' FROM')\r\n",
                "        query = query.replace(\",\\n\\n\\n\\nFROM\", ' FROM')\r\n",
                "\r\n",
                "        cursor = self.connection.cursor()\r\n",
                "        cursor.execute(query)\r\n",
                "        if isResult:\r\n",
                "            result = int(cursor.fetchone()[0])\r\n",
                "        else:\r\n",
                "            result = None\r\n",
                "        self.connection.commit()\r\n",
                "        cursor.close()\r\n",
                "        return result\r\n",
                "\r\n",
                "    def create_schema_if_not_exists(self, schema):\r\n",
                "        # NOTE: Under Dev & Review\r\n",
                "        query = f\"SELECT count(*) FROM sys.schemas WHERE name = N'{schema}';\"        \r\n",
                "        isSchema = self.execute_query(query, isResult = True)\r\n",
                "        if isSchema == 0:\r\n",
                "            logger.info(f'CREATING SCHEMA {schema}')\r\n",
                "            query = f\"CREATE SCHEMA [{schema}] AUTHORIZATION [dbo]\"\r\n",
                "            self.execute_query(query)\r\n",
                "    \r\n",
                "    def drop_view(self, schema, view_name):\r\n",
                "        query = f\"DROP VIEW IF EXISTS [{schema}].[{view_name}]\"\r\n",
                "        self.execute_query(query)\r\n",
                "    \r\n",
                "    def close_connection(self):\r\n",
                "        self.connection.close()"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from datetime import date, timedelta\r\n",
                "from dateutil.relativedelta import relativedelta\r\n",
                "import pandas as pd\r\n",
                "from pyspark.sql import SparkSession\r\n",
                "\r\n",
                "class SparkTableGenerator:\r\n",
                "    def __init__(self, spark_session, base_table_db_name, current_school_year, current_execution_datetime):\r\n",
                "        self.spark = spark_session\r\n",
                "        self.base_table_db_name = base_table_db_name\r\n",
                "        self.current_school_year = int(current_school_year)\r\n",
                "        self.current_execution_datetime = current_execution_datetime\r\n",
                "\r\n",
                "    def generate_fiscal_month(self):\r\n",
                "        # result_df = self.spark.sql(f\"SELECT * FROM {self.base_table_db_name}.config_parameter\")\r\n",
                "        result_df = self.spark.sql(f\"SELECT * FROM config_vw_parameter\")\r\n",
                "        if result_df is not None and not result_df.filter(result_df['ParameterValue'].isNotNull()).isEmpty():\r\n",
                "            fiscal_month_df = result_df.filter(result_df['ParameterName'].contains(\"FiscalMonth\"))\r\n",
                "\r\n",
                "            if not fiscal_month_df.isEmpty():\r\n",
                "                fiscal_month_value = fiscal_month_df.select('ParameterValue').first()[0]\r\n",
                "                fiscal_month = int(fiscal_month_value)\r\n",
                "            else:\r\n",
                "                fiscal_month = 7\r\n",
                "        else:\r\n",
                "            fiscal_month = 7\r\n",
                "\r\n",
                "        print(\"Fiscal Month:\", fiscal_month)\r\n",
                "        return fiscal_month\r\n",
                "\r\n",
                "    def generate_first_day_of_week(self):\r\n",
                "        # result_df = self.spark.sql(f\"SELECT * FROM {self.base_table_db_name}.config_parameter\")\r\n",
                "        result_df = self.spark.sql(f\"SELECT * FROM config_vw_parameter\")\r\n",
                "        if result_df is not None and not result_df.filter(result_df['ParameterValue'].isNotNull()).isEmpty():\r\n",
                "            first_day_df = result_df.filter(result_df['ParameterName'] == 'FirstDayOfWeek')\r\n",
                "\r\n",
                "            if not first_day_df.isEmpty():\r\n",
                "                first_day_value = first_day_df.select('ParameterValue').first()[0]\r\n",
                "                first_day_of_week = int(first_day_value)\r\n",
                "            else:\r\n",
                "                first_day_of_week = 1\r\n",
                "        else:\r\n",
                "            first_day_of_week = 1\r\n",
                "\r\n",
                "        print(\"First Day of Week:\", first_day_of_week)\r\n",
                "        return first_day_of_week\r\n",
                "\r\n",
                "    def generate_dim_date(self, fiscal_month, num_years = 4):\r\n",
                "        current_execution_datetime = self.current_execution_datetime\r\n",
                "        start_year = self.current_school_year - num_years\r\n",
                "        dim_date_list = []\r\n",
                "\r\n",
                "        while start_year <= self.current_school_year:\r\n",
                "            tmp_fiscal_date = date(start_year, fiscal_month, 1)\r\n",
                "            end_date = tmp_fiscal_date - timedelta(days=1)\r\n",
                "            start_date = tmp_fiscal_date - relativedelta(years=1)\r\n",
                "\r\n",
                "            while start_date <= end_date:\r\n",
                "                # Your existing date generation logic here\r\n",
                "                date_key = start_date.year * 10000 + start_date.month * 100 + start_date.day\r\n",
                "                day = start_date.day\r\n",
                "\r\n",
                "                suffix = 'th'\r\n",
                "                if 4 <= day % 100 <= 20 or day % 10 == 0:\r\n",
                "                    suffix = 'th'\r\n",
                "                elif day % 10 == 1:\r\n",
                "                    suffix = 'st'\r\n",
                "                elif day % 10 == 2:\r\n",
                "                    suffix = 'nd'\r\n",
                "                elif day % 10 == 3:\r\n",
                "                    suffix = 'rd'\r\n",
                "\r\n",
                "                dim_date_dict = {\r\n",
                "                    'DateKey': date_key,\r\n",
                "                    'Date': start_date,\r\n",
                "                    'DayOfMonth': day,\r\n",
                "                    'DayOfMonthWithSuffix': f\"{day}{suffix}\",\r\n",
                "                    'DayOfWeek': start_date.weekday() + 1,\r\n",
                "                    'DayOfYear': start_date.timetuple().tm_yday,\r\n",
                "                    'WeekDayName': start_date.strftime('%A'),\r\n",
                "                    'WeekdayNameShort': start_date.strftime('%a').upper(),\r\n",
                "                    'WeekOfMonth': (start_date.day - 1) // 7 + 1,\r\n",
                "                    'WeekOfYear': start_date.isocalendar()[1],\r\n",
                "                    'Month': start_date.month,\r\n",
                "                    'MonthName': start_date.strftime('%B'),\r\n",
                "                    'MonthNameShort': start_date.strftime('%b').upper(),\r\n",
                "                    'SchoolYear': f\"{start_year - 1}-{start_year}\",\r\n",
                "                    'SchoolYearShort': str(start_year),\r\n",
                "                    'CalendarYear': start_date.year,\r\n",
                "                    'DW_CreatedDateTime': str(current_execution_datetime),\r\n",
                "                    'DW_ModifiedDateTime': str(current_execution_datetime),\r\n",
                "                }\r\n",
                "\r\n",
                "                dim_date_list.append(dim_date_dict)\r\n",
                "\r\n",
                "                start_date += timedelta(days=1)\r\n",
                "\r\n",
                "            start_year += 1\r\n",
                "\r\n",
                "        dim_date_df = self.spark.createDataFrame(dim_date_list)\r\n",
                "        dim_date_df.withColumn(\"SchoolYearShort\", dim_date_df[\"SchoolYearShort\"].cast(IntegerType()))\r\n",
                "        dim_date_df.withColumn(\"DatSkey\", lit(None).cast(IntegerType()))\r\n",
                "        return dim_date_df#self.spark.createDataFrame(dim_date_df)"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                ""
            ],
            "outputs": []
        }
    ]
}