{
    "nbformat": 4,
    "nbformat_minor": 2,
    "metadata": {
        "save_output": true,
        "kernelspec": {
            "name": "synapse_pyspark",
            "display_name": "Synapse PySpark"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "execution_count": 73,
            "cell_type": "code",
            "source": [
                "from notebookutils import mssparkutils\r\n",
                "from datetime import datetime"
            ],
            "outputs": []
        },
        {
            "execution_count": 75,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "start_time = datetime.now()\r\n",
                "# FIXME: 2024-03-18 Frequency Based ETL deprecated in v0.6\r\n",
                "highFrequentDelta = lowFrequentDelta = moderateFrequentDelta = descriptorsDelta = 0.00002"
            ],
            "outputs": []
        },
        {
            "execution_count": 76,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "import random\r\n",
                "import string\r\n",
                "\r\n",
                "characters = string.ascii_letters + string.digits\r\n",
                "ten_digit_alphanumeric = ''.join(random.choice(characters) for _ in range(10))\r\n",
                "\r\n",
                "pipelineExecutionId = executionId = f'TEST_{ten_digit_alphanumeric}'"
            ],
            "outputs": []
        },
        {
            "execution_count": 77,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# NOTE: DO NOT REMOVE THESE VARIABLES\r\n",
                "schoolYear_varParam = \"SchoolYear\"\r\n",
                "districtId_varParam = \"DistrictId\""
            ],
            "outputs": []
        },
        {
            "execution_count": 78,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "input_params = {\r\n",
                "    'create_s2r_descriptor_views': create_s2r_descriptor_views,\r\n",
                "    'migrate_s2r_to_s3': migrate_s2r_to_s3,\r\n",
                "    'create_s3_sql_db_views': create_s3_sql_db_views,\r\n",
                "    'create_s3_semantic_views': create_s3_semantic_views,\r\n",
                "    'workspace': workspace,\r\n",
                "    'stage2_db_name': stage2_db_name,\r\n",
                "    'stage3_db_name': stage3_db_name,\r\n",
                "    'base_table_db_name': stage3_db_name,\r\n",
                "    'schoolYear_varParam': schoolYear_varParam,\r\n",
                "    'current_school_year': schoolYear,\r\n",
                "    'schoolYear': schoolYear,\r\n",
                "    'districtId_varParam': districtId_varParam,\r\n",
                "    'districtId': districtId,\r\n",
                "    'sql_db_secret_name': secret_name,\r\n",
                "    'sql_db_user_name': user_name,\r\n",
                "    'sql_db_name': database_name,\r\n",
                "    'sql_db_data_source': data_source,\r\n",
                "    'sql_db_server_name': server_name,\r\n",
                "    'secret_name': secret_name,\r\n",
                "    'database_name':database_name,\r\n",
                "    'user_name':user_name,\r\n",
                "    'data_source':data_source,\r\n",
                "    'semantic_schema_name':semantic_schema_name,\r\n",
                "    'driver':driver,\r\n",
                "    'server_name':server_name,\r\n",
                "    'apiVersion': apiVersion,\r\n",
                "    'highFrequentDelta': highFrequentDelta,\r\n",
                "    'moderateFrequentDelta': moderateFrequentDelta,\r\n",
                "    'lowFrequentDelta': lowFrequentDelta,\r\n",
                "    'descriptorsDelta': descriptorsDelta,\r\n",
                "    'prepare_edgraph_metadata': prepare_edgraph_metadata}\r\n",
                ""
            ],
            "outputs": []
        },
        {
            "execution_count": 79,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "%run OEA/modules/Ed-Fi/v0.7/src/utilities/edfi_v0_7_edfi_py"
            ],
            "outputs": []
        },
        {
            "execution_count": 80,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from datetime import datetime\r\n",
                "oea = EdFiOEAChild()   \r\n",
                "oea.set_workspace(workspace)"
            ],
            "outputs": []
        },
        {
            "execution_count": 81,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "def execute_etl_step(condition,\r\n",
                "                     nb_path,\r\n",
                "                     nb_timeout,\r\n",
                "                     nb_params,\r\n",
                "                     nb_error,\r\n",
                "                     etl_stage):\r\n",
                "    global etl_status\r\n",
                "    # FIXME: Parameterize etl_status \r\n",
                "    stage_start_time = datetime.now()\r\n",
                "    if nb_error is None:\r\n",
                "        etl_status = f\"{etl_stage} - RUNNING\"\r\n",
                "        try:\r\n",
                "            if condition:\r\n",
                "                mssparkutils.notebook.run(path = nb_path,\r\n",
                "                                          timeout_seconds = nb_timeout,\r\n",
                "                                          arguments = nb_params)\r\n",
                "                etl_status = f\"{etl_stage} - SUCCEEDED\"\r\n",
                "            else:\r\n",
                "                etl_status = f\"{etl_stage} - SKIPPED\"\r\n",
                "        except Exception as error_msg:\r\n",
                "            etl_status = f\"{etl_stage} - FAILED\"\r\n",
                "            logger.info(f\"{error_msg}\")\r\n",
                "            nb_error = error_msg\r\n",
                "    else:\r\n",
                "        pass\r\n",
                "    \r\n",
                "    stage_end_time = datetime.now()\r\n",
                "    log_data = pipeline_error_logger.create_log_dict(uniqueId = pipeline_error_logger.generate_random_alphanumeric(10), # Generate a random 10-character alphanumeric value\r\n",
                "                                            pipelineExecutionId = pipelineExecutionId,#'TEST_1234',#executionId,\r\n",
                "                                            sparkSessionId = spark.sparkContext.applicationId,\r\n",
                "                                            stageName = etl_stage,\r\n",
                "                                            start_time = stage_start_time,\r\n",
                "                                            end_time = stage_end_time,\r\n",
                "                                            run_status = 'SUCCEEDED' if nb_error is None else 'FAILED',\r\n",
                "                                            etl_status = etl_status,\r\n",
                "                                            error_description = str(nb_error))\r\n",
                "    pipeline_error_logger.consolidate_logs(log_data,'stage')\r\n",
                "    if ' - SUCCEEDED' in etl_status or ' - SKIPPED' in etl_status:\r\n",
                "        return etl_status, None\r\n",
                "    else:\r\n",
                "        return etl_status, nb_error\r\n",
                "\r\n",
                "def get_entity_logs_status(df, executionId, stageName):\r\n",
                "    df = df.filter((F.col('pipelineExecutionId') == executionId) & (F.col('stageName') == stageName))\r\n",
                "    df = df.withColumn('numRecordsFailed', F.col('totalNumOutputRows') - F.col('numInputRows'))\r\n",
                "\r\n",
                "    if df.filter(F.col(\"numRecordsFailed\") == 0).count() == df.count():\r\n",
                "        return 'success'\r\n",
                "    \r\n",
                "    if df.filter(F.col(\"numRecordsFailed\") != 0).count() == df.count():\r\n",
                "        return 'failure'\r\n",
                "    \r\n",
                "    if df.agg(F.sum(\"numRecordsFailed\")).collect()[0][0] != 0:\r\n",
                "        return 'partial-success'\r\n",
                "    \r\n",
                "    return 'unknown'"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Main Code"
            ],
            "outputs": []
        },
        {
            "execution_count": 82,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "nb_root = \"EdGraph/modules/EdGraph_DW/v0.6/src\"\r\n",
                "nb_version = \"edgraph_dw_v0_6\"\r\n",
                "nb_error = None\r\n",
                "\r\n",
                "run_status = \"INITIATED\"\r\n",
                "etl_status = \"\""
            ],
            "outputs": []
        },
        {
            "execution_count": 83,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "pipeline_error_logger = ErrorLogging(spark = spark, \r\n",
                "                                     oea = oea, \r\n",
                "                                     logger = logger)\r\n",
                "\r\n",
                "try:\r\n",
                "    run_status = \"RUNNING\"\r\n",
                "    # NOTE: Prepare Meta Data \r\n",
                "    etl_status, nb_error = execute_etl_step(condition = prepare_edgraph_metadata,\r\n",
                "                                            nb_path = f\"{nb_root}/main/{nb_version}_freq_etl_metadata\",\r\n",
                "                                            nb_timeout = 1200,\r\n",
                "                                            nb_params = input_params,\r\n",
                "                                            nb_error = nb_error,\r\n",
                "                                            etl_stage = \"edgraph-dwh: Metadata Preparation\")\r\n",
                "    \r\n",
                "    # NOTE: Prepare Meta Data \r\n",
                "    etl_status, nb_error = execute_etl_step(condition = create_s2r_descriptor_views,\r\n",
                "                                            nb_path = f\"{nb_root}/main/{nb_version}_s2r_descriptor_views\",\r\n",
                "                                            nb_timeout = 1200,\r\n",
                "                                            nb_params = input_params,\r\n",
                "                                            nb_error = nb_error,\r\n",
                "                                            etl_stage = \"edgraph-dwh: Metadata Preparation\")\r\n",
                "    # NOTE: Migrate To Stage 3 (Edgraph DWH)\r\n",
                "    etl_status, nb_error = execute_etl_step(condition = migrate_s2r_to_s3,\r\n",
                "                                            nb_path = f\"{nb_root}/main/{nb_version}_migrate_to_stage3\",\r\n",
                "                                            nb_timeout = 3600,\r\n",
                "                                            nb_params = input_params,\r\n",
                "                                            nb_error = nb_error,\r\n",
                "                                            etl_stage = \"edgraph-dwh: Migrate From S2R To Edgraph DWH\")\r\n",
                "    # NOTE: Create SQL DB Views\r\n",
                "    etl_status, nb_error = execute_etl_step(condition = create_s3_sql_db_views,\r\n",
                "                                            nb_path = f\"{nb_root}/main/{nb_version}_populate_sql_db\",\r\n",
                "                                            nb_timeout = 1200,\r\n",
                "                                            nb_params = input_params,\r\n",
                "                                            nb_error = nb_error,\r\n",
                "                                            etl_stage = \"edgraph-dwh: Create SQL DB Views\")\r\n",
                "    \r\n",
                "    # NOTE: Create Semantic Views\r\n",
                "    etl_status, nb_error = execute_etl_step(condition = create_s3_semantic_views,\r\n",
                "                                            nb_path = f\"{nb_root}/main/{nb_version}_semantic_views\",\r\n",
                "                                            nb_timeout = 1200,\r\n",
                "                                            nb_params = input_params,\r\n",
                "                                            nb_error = nb_error,\r\n",
                "                                            etl_stage = \"edgraph-dwh: Create Semantic Views\")\r\n",
                "    if nb_error is not None:\r\n",
                "        raise Exception(f\"{nb_error}\")\r\n",
                "    run_status = \"SUCCEEDED\"\r\n",
                "except Exception as e:\r\n",
                "    run_status = \"FAILED\"\r\n",
                "    print(f'Pipeline Executed with Error {e}')"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Pipeline Level Logs"
            ],
            "outputs": []
        },
        {
            "execution_count": 70,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "log_type = 'pipeline'\r\n",
                "\r\n",
                "end_time = datetime.now()\r\n",
                "log_data = pipeline_error_logger.create_log_dict(uniqueId = pipeline_error_logger.generate_random_alphanumeric(10), # Generate a random 10-character alphanumeric value\r\n",
                "                                                pipelineExecutionId = pipelineExecutionId,#'TEST_1234',#executionId,\r\n",
                "                                                sparkSessionId = spark.sparkContext.applicationId,\r\n",
                "                                                start_time = start_time,\r\n",
                "                                                end_time = end_time,\r\n",
                "                                                run_status = run_status,\r\n",
                "                                                etl_status = str(etl_status))\r\n",
                "pipeline_error_logger.consolidate_logs(log_data,'pipeline')\r\n",
                "df = pipeline_error_logger.create_spark_df('pipeline')\r\n",
                "\r\n",
                "\r\n",
                "pipeline_error_logger.write_logs_to_delta_lake(df = df, \r\n",
                "                             log_type = 'pipeline',\r\n",
                "                             destination_url = pipeline_error_logger.to_logs_url('etl-logs/log_type=pipeline'))\r\n",
                "pipeline_error_logger.add_etl_logs_to_lake_db(db_name = f'ldb_{workspace}_edgraph_etl_logs',\r\n",
                "                                     logs_base_path = 'etl-logs',\r\n",
                "                                     log_type = 'pipeline',\r\n",
                "                                     overwrite = False)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Stage Level Logs"
            ],
            "outputs": []
        },
        {
            "execution_count": 71,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# log_type = 'entity'\r\n",
                "# entity_logs_url = pipeline_error_logger.to_logs_url('etl-logs/log_type=entity')\r\n",
                "# entity_logs_df = oea.load(entity_logs_url)"
            ],
            "outputs": []
        },
        {
            "execution_count": 72,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "df = pipeline_error_logger.create_spark_df('stage')\r\n",
                "df = df.withColumn('stage_status', F.lit('Unknown'))\r\n",
                "\r\n",
                "log_type = 'stage'\r\n",
                "destination_url = pipeline_error_logger.to_logs_url('etl-logs/log_type=stage')\r\n",
                "\r\n",
                "pipeline_error_logger.write_logs_to_delta_lake(df = df, \r\n",
                "                                               log_type = log_type,\r\n",
                "                                               destination_url = destination_url)\r\n",
                "pipeline_error_logger.add_etl_logs_to_lake_db(db_name = f'ldb_{workspace}_edgraph_etl_logs',\r\n",
                "                                     logs_base_path = 'etl-logs',\r\n",
                "                                     log_type = 'stage',\r\n",
                "                                     overwrite = False)"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                ""
            ],
            "outputs": []
        }
    ]
}