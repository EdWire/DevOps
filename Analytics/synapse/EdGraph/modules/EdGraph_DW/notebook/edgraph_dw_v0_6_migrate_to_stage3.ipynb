{
    "nbformat": 4,
    "nbformat_minor": 2,
    "metadata": {
        "save_output": true,
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "execution_count": 2,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "essential_columns = ['DistrictId','SchoolYear','SKey','row_hash']"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Initializations"
            ],
            "outputs": []
        },
        {
            "execution_count": 3,
            "cell_type": "code",
            "source": [
                "%run OEA/modules/Ed-Fi/v0.7/src/utilities/edfi_v0_7_edfi_py"
            ],
            "outputs": []
        },
        {
            "execution_count": 4,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "%run EdGraph/modules/EdGraph_DW/v0.6/src/utilities/edgraph_dw_v0_6_dw_builder"
            ],
            "outputs": []
        },
        {
            "execution_count": 5,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from datetime import datetime\r\n",
                "oea = EdFiOEAChild()   \r\n",
                "error_logger = ErrorLogging(spark = spark,\r\n",
                "                            oea = oea,\r\n",
                "                            logger = logger)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Global Parameters"
            ],
            "outputs": []
        },
        {
            "execution_count": 6,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "def assign_default_variable(variable_name, default_value):\r\n",
                "    if variable_name not in globals():\r\n",
                "        globals()[variable_name] = default_value\r\n",
                "        logger.info(f'{variable_name} not found - using system default')"
            ],
            "outputs": []
        },
        {
            "execution_count": 7,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "\r\n",
                "oea.set_workspace(workspace)"
            ],
            "outputs": []
        },
        {
            "execution_count": 8,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "metadata_path = \"stage3/EdGraph_DW/build-metadata/stage3-query-assets.json\"\r\n",
                "metadata_url = oea.to_url(metadata_path)"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from datetime import datetime, timedelta\r\n",
                "\r\n",
                "incremental_prefix = ''\r\n",
                "current_datetime = datetime.now()\r\n",
                "last_datetime = current_datetime - timedelta(days=30) # TODO: Example Offset\r\n",
                "\r\n",
                "current_execution_datetime_str = current_datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f0\")\r\n",
                "last_execution_datetime_str = last_datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f0\")\r\n",
                "\r\n",
                "assign_default_variable(variable_name = 'current_school_year', \r\n",
                "                        default_value = 'SchoolYear')\r\n",
                "\r\n",
                "assign_default_variable(variable_name = 'schoolYear_varParam', \r\n",
                "                        default_value = 'SchoolYear')\r\n",
                "\r\n",
                "assign_default_variable(variable_name = 'districtId_varParam', \r\n",
                "                        default_value = 'DistrictId')\r\n",
                "\r\n",
                ""
            ],
            "outputs": []
        },
        {
            "execution_count": 9,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from pyspark.sql import SparkSession\r\n",
                "from pyspark.sql.functions import lit\r\n",
                "from pyspark.sql.types import IntegerType"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### Global Query Parameters"
            ],
            "outputs": []
        },
        {
            "execution_count": 10,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from datetime import datetime, timedelta\r\n",
                "\r\n",
                "incremental_prefix = ''\r\n",
                "current_datetime = datetime.now()\r\n",
                "#last_datetime = current_datetime - timedelta(days=30) # TODO: Example Offset\r\n",
                "#last_datetime = '1999-03-19 12:22:10.417323' #For Overwrite\r\n",
                "current_execution_datetime_str = current_datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f0\")\r\n",
                "#last_execution_datetime_str = last_datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f0\")\r\n",
                "last_execution_datetime_str = '1999-01-01T12:25:26.1061490' # For Overwrite\r\n",
                "present_year = current_datetime.year"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Main Code"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### DimDate"
            ],
            "outputs": []
        },
        {
            "execution_count": 11,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "def dump_dim_date(table_name, table_generator, metadata_processor):\r\n",
                "    fiscal_month = table_generator.generate_fiscal_month()\r\n",
                "    first_day_of_week = table_generator.generate_first_day_of_week()\r\n",
                "        \r\n",
                "    dimDate_staging = table_generator.generate_dim_date(fiscal_month,num_years=20)\r\n",
                "    dimDate_staging = dimDate_staging.withColumn(\"SchoolYearShort\", dimDate_staging[\"SchoolYearShort\"].cast(IntegerType()))\r\n",
                "    dimDate_staging = dimDate_staging.withColumn(\"DatSkey\", lit(None).cast(IntegerType()))\r\n",
                "    dimDate_staging.createOrReplaceTempView(\"staging_vw_DimDate\")\r\n",
                "    query=\"\"\" SELECT * from staging_vw_dimDate UNION ALL\r\n",
                "    SELECT\r\n",
                "    1900 AS CalendarYear,'2023-10-17 15:06:01.1438416 +05:30' AS DW_CreatedDateTime,'2023-10-17 15:06:01.1438416 +05:30' AS DW_ModifiedDateTime,CAST('1900-01-01' AS DATE) AS Date,\r\n",
                "    -1 AS DateKey,-1 AS DayOfMonth,'-1' AS DayOfMonthWithSuffix,-1 AS DayOfWeek,-1 AS DayOfYear,\r\n",
                "    -1 AS Month,'Not Specified' AS MonthName,'Not Specified' AS MonthNameShort,'Not Specified' AS SchoolYear,1900 AS SchoolYearShort,\r\n",
                "    'Not Specified' AS WeekdayName,-1 AS WeekOfMonth,-1 AS WeekOfYear,'Not Specified' AS WeekdayNameShort,'-1' as DatSkey\r\n",
                "    \"\"\"\r\n",
                "    dimDate = spark.sql(query)\r\n",
                "    dimDate.createOrReplaceTempView(\"dbo_vw_DimDate\")\r\n",
                "    # print(spark.sql(\"SELECT count(*) from dbo_vw_DimDate\").collect())\r\n",
                "    metadata_processor.dump_to_stage3_delta_lake(step_prefix = 'dbo_vw', \r\n",
                "                                                            table_name = table_name,\r\n",
                "                                                            essential_columns=essential_columns,surrogate_key = True)\r\n",
                "    metadata_processor.add_to_lake_db_stage3(step_prefix = 'dbo_vw', \r\n",
                "                                                        table_name = table_name, \r\n",
                "                                                        overwrite = True)\r\n",
                "\r\n",
                "def common_elements_preserve_order(list1, list2):\r\n",
                "    # NOTE: Returns the list of entities to etl that are common in the order of list1\r\n",
                "    set_list1 = set(list1)\r\n",
                "    common_elements = [elem for elem in list1 if elem in set_list1.intersection(list2)]\r\n",
                "    return common_elements\r\n",
                "\r\n",
                "def non_common_elements_preserve_order(list1, list2):\r\n",
                "    set_list1 = set(list1)\r\n",
                "    non_common_elements = [elem for elem in list1 if elem not in set_list1.intersection(list2)]\r\n",
                "    return non_common_elements"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### Automated (via MetadataProcessor)"
            ],
            "outputs": []
        },
        {
            "execution_count": 12,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# NOTE: the df is cached so if the source JSON file changes during the same session\r\n",
                "#       then, the df should be unpersisted before executing this cell again\r\n",
                "jsonDF = spark.read.option(\"multiline\", \"true\").json(metadata_url).cache()\r\n",
                "\r\n",
                "json_string = jsonDF.toJSON().collect()[0]\r\n",
                "original_metadata = json.loads(json_string)\r\n",
                "\r\n",
                ""
            ],
            "outputs": []
        },
        {
            "execution_count": 13,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# NOTE: Keep the var test_version = '' (empty string) for prod\r\n",
                "#       Set the value to a string like \"/v1.0\" if you wish to data to the \r\n",
                "test_version = ''\r\n",
                "metadata_processor = EdgraphDWHBuilder(original_metadata = original_metadata, \r\n",
                "                                       stage3_db_name = stage3_db_name,\r\n",
                "                                       stage_3_path = f'stage3/EdGraph_DW{test_version}', \r\n",
                "                                       partitioning = False, \r\n",
                "                                       spark = spark, \r\n",
                "                                       oea = oea, \r\n",
                "                                       logger = logger,\r\n",
                "                                       error_logger = error_logger,\r\n",
                "                                       lakeTableOverwrite = False)\r\n",
                "metadata_processor.process_metadata()\r\n",
                "table_generator = SparkTableGenerator(spark, base_table_db_name, present_year, current_datetime)"
            ],
            "outputs": []
        },
        {
            "execution_count": 14,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from datetime import datetime\r\n",
                "import math\r\n",
                "source_path = f'stage1/Transactional/Ed-Fi/{apiVersion}/DistrictId={districtId}/SchoolYear={schoolYear}/metadata-assets/edgraph_frequency_etl.csv'  \r\n",
                "destination_path = source_path #f'stage1/Transactional/Ed-Fi/{apiVersion}/DistrictId={districtId}/SchoolYear={schoolYear}/metadata-assets/frequency_based_etl.csv'  \r\n",
                "logs_path = f\"stage1/Transactional/Ed-Fi/{apiVersion}/DistrictId={districtId}/SchoolYear={schoolYear}/metadata-assets/_edgraph_frequency_etl_logs/run_logs_{datetime.today().strftime('%Y-%m-%d')}.csv\""
            ],
            "outputs": []
        },
        {
            "execution_count": 15,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "processor = EntityFrequencyProcessor(oea = oea, \r\n",
                "                                     filepath = source_path, \r\n",
                "                                     highFrequentDelta = highFrequentDelta,#0.005, \r\n",
                "                                     moderateFrequentDelta = moderateFrequentDelta, #5, \r\n",
                "                                     lowFrequentDelta = lowFrequentDelta, #10, \r\n",
                "                                     descriptorsDelta = descriptorsDelta) #360)"
            ],
            "outputs": []
        },
        {
            "execution_count": 16,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# NOTE: entities_to_etl consist of the list of tables to be ETL based on their frequency profiles\r\n",
                "\r\n",
                "processor.load_lookup_df()\r\n",
                "processor.write_lookup_df(logs_path)\r\n",
                "entities_to_etl, _ = processor.edgraph_return_entities_to_etl()"
            ],
            "outputs": []
        },
        {
            "execution_count": 17,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from datetime import datetime\r\n",
                "from concurrent.futures import ThreadPoolExecutor,as_completed\r\n",
                "queries_in_order = dict()\r\n",
                "queries_params_in_order = dict()\r\n",
                "parameterized_queries = dict()\r\n",
                "schema_names = ['dbo','config','auth']\r\n",
                "explain = False\r\n",
                "\r\n",
                "for schema_name in schema_names:\r\n",
                "    queries_in_order[schema_name], queries_params_in_order[schema_name] = metadata_processor.return_schema_queries_in_order(schema_name)\r\n",
                "\r\n",
                "for schema_name in schema_names:\r\n",
                "    non_surrogate_tables = ['DimSchoolYear']\r\n",
                "    schema_queries_in_order = queries_in_order[schema_name]\r\n",
                "    schema_queries_params_in_order = queries_params_in_order[schema_name]\r\n",
                "    \r\n",
                "    table_names = list(schema_queries_in_order.keys())\r\n",
                "    table_names = common_elements_preserve_order(table_names, entities_to_etl)\r\n",
                "    #table_names = ['FactEnrollment']\r\n",
                "    for table_name in table_names:\r\n",
                "        try:\r\n",
                "            if table_name == 'DimDate':\r\n",
                "                logger.info(f'Dependent TABLE CREATION - config.Parameter')\r\n",
                "                logger.info(f'TABLE CREATION - {table_name}')\r\n",
                "                parameterized_queries = metadata_processor.parameterize_table_queries(\r\n",
                "                                                    schema_queries = queries_in_order['config'],\r\n",
                "                                                    schema_queries_params = queries_params_in_order['config'],\r\n",
                "                                                    table_name = 'Parameter',\r\n",
                "                                                    stage2_db_name = stage2_db_name,\r\n",
                "                                                    stage3_db_name = stage3_db_name,\r\n",
                "                                                    base_table_db_name = base_table_db_name,\r\n",
                "                                                    current_execution_datetime_str = current_execution_datetime_str,\r\n",
                "                                                    last_execution_datetime_str = last_execution_datetime_str,\r\n",
                "                                                    current_school_year = current_school_year,\r\n",
                "                                                    schoolYear_varParam = schoolYear_varParam,\r\n",
                "                                                    districtId_varParam = districtId_varParam,\r\n",
                "                                                    incremental_prefix = incremental_prefix,\r\n",
                "                                                    # CurrentSchoolYear = current_school_year,\r\n",
                "                                                    query_params={})\r\n",
                "                \r\n",
                "                metadata_processor.execute_table_queries('config',\r\n",
                "                                                        'Parameter', \r\n",
                "                                                        parameterized_queries,\r\n",
                "                                                        stage3_db_name,\r\n",
                "                                                        surrogate_key = False,\r\n",
                "                                                        insertion_type = 'append',\r\n",
                "                                                        explain = explain)\r\n",
                "                \r\n",
                "                dump_dim_date(table_name, table_generator, metadata_processor)\r\n",
                "            \r\n",
                "            elif table_name == 'Parameter' and schema_name == 'config':\r\n",
                "                pass\r\n",
                "            else:\r\n",
                "                logger.info(f'TABLE CREATION - {table_name}')\r\n",
                "                parameterized_queries = metadata_processor.parameterize_table_queries(\r\n",
                "                                                    schema_queries=schema_queries_in_order,\r\n",
                "                                                    schema_queries_params=schema_queries_params_in_order,\r\n",
                "                                                    table_name=table_name,\r\n",
                "                                                    stage2_db_name = stage2_db_name,\r\n",
                "                                                    stage3_db_name = stage3_db_name,\r\n",
                "                                                    base_table_db_name = base_table_db_name,\r\n",
                "                                                    current_execution_datetime_str = current_execution_datetime_str,\r\n",
                "                                                    last_execution_datetime_str = last_execution_datetime_str,\r\n",
                "                                                    current_school_year = current_school_year,\r\n",
                "                                                    schoolYear_varParam = schoolYear_varParam,\r\n",
                "                                                    districtId_varParam = districtId_varParam,\r\n",
                "                                                    incremental_prefix = incremental_prefix,\r\n",
                "                                                    # CurrentSchoolYear = current_school_year,\r\n",
                "                                                    query_params={})\r\n",
                "                \r\n",
                "                if table_name in non_surrogate_tables:\r\n",
                "                    # FIXME: 2024-02-27 Need to assess and change the param insertion_type\r\n",
                "                    logger.info(f\"[SURROGATE TABLE] {table_name}\")\r\n",
                "                    metadata_processor.execute_table_queries(schema_name,\r\n",
                "                                                            table_name, \r\n",
                "                                                            parameterized_queries,\r\n",
                "                                                            stage3_db_name,\r\n",
                "                                                            surrogate_key = False,\r\n",
                "                                                            insertion_type = 'append',\r\n",
                "                                                            explain = explain)\r\n",
                "                else:\r\n",
                "                    metadata_processor.execute_table_queries(schema_name,\r\n",
                "                                                        table_name, \r\n",
                "                                                        parameterized_queries,\r\n",
                "                                                        stage3_db_name,\r\n",
                "                                                        surrogate_key = True,\r\n",
                "                                                        insertion_type = 'append',\r\n",
                "                                                        explain = explain)\r\n",
                "            print()\r\n",
                "        except Exception as error:\r\n",
                "            logger.exception(f\"[MIGRATION ERROR FLAG] {table_name} - {error}\")\r\n",
                "            #logger.exception(error)\r\n",
                "            #logger.error(f\"An Error Occurred while creating: {table_name}\")\r\n",
                "\r\n",
                "print()\r\n",
                "print()\r\n",
                "print()\r\n",
                "\r\n",
                "for schema_name in schema_names:\r\n",
                "    non_surrogate_tables = ['DimSchoolYear']\r\n",
                "    schema_queries_in_order = queries_in_order[schema_name]\r\n",
                "    schema_queries_params_in_order = queries_params_in_order[schema_name]\r\n",
                "    table_names = list(schema_queries_in_order.keys())\r\n",
                "    table_names = common_elements_preserve_order(table_names, entities_to_etl)\r\n",
                "    table_names = non_common_elements_preserve_order(table_names, metadata_processor.base_tables) \r\n",
                "    if schema_name == 'dbo':\r\n",
                "        # NOTE: ThreadPool for base tables\r\n",
                "        with ThreadPoolExecutor(max_workers=4) as tpe:\r\n",
                "            futures=[]\r\n",
                "            for table_name in metadata_processor.base_tables:\r\n",
                "                try:\r\n",
                "                    staging_list = metadata_processor.generate_staging_list(len(schema_queries_in_order), schema_name)\r\n",
                "                    for step_prefix, query in zip(staging_list,schema_queries_in_order):\r\n",
                "                        if table_name in non_surrogate_tables:\r\n",
                "                            args = (table_name,step_prefix,schema_queries_in_order,essential_columns,False)\r\n",
                "                            futures.append(tpe.submit(metadata_processor.threaded_dump_to_stage3_delta,args))\r\n",
                "                        else:\r\n",
                "                            args = (table_name,step_prefix,schema_queries_in_order,essential_columns,True)\r\n",
                "                            futures.append(tpe.submit(metadata_processor.threaded_dump_to_stage3_delta,args))\r\n",
                "                except Exception as error:\r\n",
                "                    logger.exception(f\"[MIGRATION ERROR FLAG] {table_name} - {error}\")\r\n",
                "        for future in as_completed(futures):\r\n",
                "                try:\r\n",
                "                    future.result()  # This will raise an exception if the function call inside the future raised one\r\n",
                "                except Exception as e:\r\n",
                "                    logger.exception(f\"[MIGRATION ERROR FLAG] {table_name} - {e}\")    \r\n",
                "  \r\n",
                "    # NOTE: Main ThreadPool for non-base tables\r\n",
                "    with ThreadPoolExecutor(max_workers=24) as tpe:\r\n",
                "        futures=[]\r\n",
                "        for table_name in table_names:\r\n",
                "            try:\r\n",
                "                staging_list = metadata_processor.generate_staging_list(len(schema_queries_in_order), schema_name)\r\n",
                "                for step_prefix, query in zip(staging_list,schema_queries_in_order):\r\n",
                "                    if table_name in non_surrogate_tables:\r\n",
                "                        args = (table_name,step_prefix,schema_queries_in_order,essential_columns,False)\r\n",
                "                        futures.append(tpe.submit(metadata_processor.threaded_dump_to_stage3_delta,args))\r\n",
                "                    else:\r\n",
                "                        args = (table_name,step_prefix,schema_queries_in_order,essential_columns,True)\r\n",
                "                        futures.append(tpe.submit(metadata_processor.threaded_dump_to_stage3_delta,args))\r\n",
                "            except Exception as error:\r\n",
                "                logger.exception(f\"[MIGRATION ERROR FLAG] {table_name} - {error}\")\r\n",
                "    for future in as_completed(futures):\r\n",
                "            try:\r\n",
                "                future.result()  # This will raise an exception if the function call inside the future raised one\r\n",
                "            except Exception as error:\r\n",
                "                logger.exception(f\"[MIGRATION ERROR FLAG] {table_name} - {error}\")"
            ],
            "outputs": []
        },
        {
            "execution_count": 50,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# NOTE: Update the frequency profile\r\n",
                "processor.update_lookup_df()\r\n",
                "processor.write_lookup_df(destination_path)\r\n",
                ""
            ],
            "outputs": []
        },
        {
            "execution_count": 39,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "if error_logger.entity_logs != []:\r\n",
                "    df_logs = error_logger.create_spark_df('entity')\r\n",
                "    error_logger.write_logs_to_delta_lake(df = df_logs, \r\n",
                "                                        log_type = 'entity',\r\n",
                "                                        destination_url = error_logger.to_logs_url('etl-logs/log_type=entity'))\r\n",
                "\r\n",
                "    error_logger.add_etl_logs_to_lake_db(db_name = f'ldb_{workspace}_edgraph_etl_logs',\r\n",
                "                                        logs_base_path = 'etl-logs',\r\n",
                "                                        log_type = 'entity',\r\n",
                "                                        overwrite = False)"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "jsonDF.unpersist()\r\n",
                "logger.info('Cached Data Removed From Memory')"
            ],
            "outputs": []
        }
    ]
}