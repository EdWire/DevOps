{
    "nbformat": 4,
    "nbformat_minor": 2,
    "metadata": {
        "save_output": true,
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "execution_count": 3,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "instance = InstanceId = instanceId\r\n",
                "ApiUrl = apiUrl\r\n",
                "SchoolYear = schoolYear\r\n",
                "districtPath = DistrictId = DistrictID  = districtID = districtId\r\n",
                "apiLimit = batchLimit\r\n",
                "\r\n",
                "prepareSAPMetaData = prepareSAPMetadata\r\n",
                "zone = submissions_type = submissionsType = sap_pipeline"
            ],
            "outputs": []
        },
        {
            "execution_count": 4,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "def assign_default_variable(variable_name, default_value):\r\n",
                "    if variable_name not in globals():\r\n",
                "        globals()[variable_name] = default_value\r\n",
                "        logger.info(f'{variable_name} not found - using system default')"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Pre-Requisites (Dev)"
            ],
            "outputs": []
        },
        {
            "execution_count": 5,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from notebookutils import mssparkutils\r\n",
                "import configparser\r\n",
                "import random\r\n",
                "\r\n",
                "config_path = \"/tmp/conf.ini\"\r\n",
                "def copy_config_to_temp():\r\n",
                "    mssparkutils.fs.cp(oea.to_url(\"stage1/Transactional/SAP/metadata-assets/edfi-configs.ini\"),\"file:/tmp/conf.ini\")\r\n",
                "\r\n",
                "def read_edfi_credentials(config_path):\r\n",
                "    config = configparser.ConfigParser()\r\n",
                "    config.read(config_path)\r\n",
                "\r\n",
                "    edfi_credentials = {}\r\n",
                "\r\n",
                "    if 'EdFi' in config:\r\n",
                "        edfi_credentials['client_id'] = config['EdFi'].get('client_id', '')\r\n",
                "        edfi_credentials['client_secret'] = config['EdFi'].get('client_secret', '')\r\n",
                "    \r\n",
                "    return edfi_credentials"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Actual Code"
            ],
            "outputs": []
        },
        {
            "execution_count": 6,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "import pyspark\r\n",
                "from pyspark.sql import SparkSession\r\n",
                "from pyspark.sql import Column\r\n",
                "from pyspark.sql.utils import AnalysisException\r\n",
                "from pyspark.sql.types import * #StringType, StructType, StructField, IntegerType, DateType\r\n",
                "\r\n",
                "import pyspark.sql.functions as F\r\n",
                "from pyspark.sql.functions import col, struct, concat, lit, round, concat, array\r\n",
                "from pyspark.sql.functions import regexp_replace, expr, when, date_format, to_date\r\n",
                "\r\n",
                "import uuid\r\n",
                "from datetime import datetime\r\n",
                "import logging\r\n",
                "import json\r\n",
                "import csv\r\n",
                "import copy\r\n",
                "\r\n",
                "import threading\r\n",
                "import requests\r\n",
                "from requests.auth import HTTPBasicAuth\r\n",
                "\r\n",
                "import random\r\n",
                "import string"
            ],
            "outputs": []
        },
        {
            "execution_count": 7,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "print('Submissions - TESTING PARAMETERIZATION')\r\n",
                "try:\r\n",
                "    print(kVName)\r\n",
                "    print(workspace)\r\n",
                "    print(apiUrl)\r\n",
                "    print(instanceId)\r\n",
                "    print(moduleName)\r\n",
                "    print(apiLimit)\r\n",
                "    print(minChangeVer)\r\n",
                "    print(maxChangeVer)\r\n",
                "    print(sapVersion)\r\n",
                "    print(prepareSAPMetaData)\r\n",
                "    print(submissions)\r\n",
                "    print(sap_pipeline)\r\n",
                "    print(sap_pipelineType)\r\n",
                "    print(schoolYear)\r\n",
                "    print(districtID)\r\n",
                "    print(pipelineExecutionId)\r\n",
                "\r\n",
                "    kvName = kVName\r\n",
                "    districtId = districtID\r\n",
                "    districtPath = districtId\r\n",
                "except Exception as params_error:\r\n",
                "    print('CATCHING ERROR!!!')\r\n",
                "    print(params_error)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### URL Initializations"
            ],
            "outputs": []
        },
        {
            "execution_count": 8,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "%run OEA/modules/Ed-Fi/v0.7/src/utilities/edfi_v0_7_fetch_urls"
            ],
            "outputs": []
        },
        {
            "execution_count": 9,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "instance_id = instanceId\r\n",
                "school_year = schoolYear\r\n",
                "api_year = school_year\r\n",
                "api_url = apiUrl\r\n",
                "\r\n",
                "# FIXME: 2024-01-31 TEMP FIX FOR FY\r\n",
                "try:\r\n",
                "    edfi_api_manager = EdFiApiManager(api_url, instance_id, api_year)\r\n",
                "    edfi_api_manager.update_urls()\r\n",
                "    edfi_api_manager.set_other_metadata()\r\n",
                "\r\n",
                "    dependenciesUrl = edfi_api_manager.dependencies_url\r\n",
                "    openApiMetadataUrl = edfi_api_manager.openapi_metadata_url\r\n",
                "    dataManagementUrl = edfi_api_manager.data_management_url\r\n",
                "    authUrl = edfi_api_manager.auth_url\r\n",
                "\r\n",
                "    changeQueriesUrl = edfi_api_manager.get_referenced_url('Change-Queries')\r\n",
                "    changeQueriesUrl = changeQueriesUrl[:-13].replace('/metadata/', '/')\r\n",
                "    swagger_url = swaggerUrl = edfi_api_manager.get_referenced_url('Resources')\r\n",
                "\r\n",
                "    apiVersion = edfi_api_manager.api_version\r\n",
                "    apiVersion = apiVersion[1:] if apiVersion.startswith('v') else apiVersion\r\n",
                "except Exception as error:\r\n",
                "    edfi_api_manager = EdFiApiManager(api_url, instance_id, '')\r\n",
                "    edfi_api_manager.update_urls()\r\n",
                "    edfi_api_manager.set_other_metadata()\r\n",
                "\r\n",
                "    dependenciesUrl = edfi_api_manager.dependencies_url\r\n",
                "    openApiMetadataUrl = edfi_api_manager.openapi_metadata_url\r\n",
                "    dataManagementUrl = edfi_api_manager.data_management_url\r\n",
                "    authUrl = edfi_api_manager.auth_url\r\n",
                "\r\n",
                "    changeQueriesUrl = edfi_api_manager.get_referenced_url('Change-Queries')\r\n",
                "    changeQueriesUrl = changeQueriesUrl[:-13].replace('/metadata/', '/')\r\n",
                "    swagger_url = swaggerUrl = edfi_api_manager.get_referenced_url('Resources')\r\n",
                "\r\n",
                "    apiVersion = edfi_api_manager.api_version\r\n",
                "    apiVersion = apiVersion[1:] if apiVersion.startswith('v') else apiVersion"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### OEA Initializations"
            ],
            "outputs": []
        },
        {
            "execution_count": 10,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "%run EdGraph/modules/SAP_PEIMS/v0.6/src/utilities/sap_peim_v0_6_sap_py"
            ],
            "outputs": []
        },
        {
            "execution_count": 11,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from datetime import datetime\r\n",
                "oea = SAPEdFiOEAChild(workspace='dev', \r\n",
                "                      logging_level=logging.INFO, \r\n",
                "                      storage_account=None, \r\n",
                "                      keyvault=None, \r\n",
                "                      timezone=None,\r\n",
                "                      sap_pipeline = sap_pipeline,\r\n",
                "                      sap_pipelineType = sap_pipelineType)   \r\n",
                "oea.set_workspace(workspace)\r\n",
                "oea.ingestionHistoryMode = ingestionHistoryMode"
            ],
            "outputs": []
        },
        {
            "execution_count": 12,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# swagger_url = swaggerUrl = edfi_api_manager.get_referenced_url('Descriptors')\r\n",
                "oea_utils = schema_gen = SAPOpenAPIUtilChild(swagger_url)\r\n",
                "oea_utils.create_definitions()\r\n",
                "schemas = schema_gen.create_spark_schemas()"
            ],
            "outputs": []
        },
        {
            "execution_count": 13,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# Set Ed-Fi Credentials\r\n",
                "# copy_config_to_temp()\r\n",
                "\r\n",
                "# credentials = read_edfi_credentials(config_path)\r\n",
                "# client_id = credentials.get('client_id')\r\n",
                "# client_secret_id = credentials.get('client_secret')"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Metadata Processing"
            ],
            "outputs": []
        },
        {
            "execution_count": 14,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "metadata_path = \"stage1/Transactional/SAP/metadata-assets/sap-to-edfi.json\"\r\n",
                "metadata_url = oea.to_url(metadata_path)"
            ],
            "outputs": []
        },
        {
            "execution_count": 15,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "jsonDF = spark.read.option(\"multiline\", \"true\").json(metadata_url).cache()\r\n",
                "\r\n",
                "json_string = jsonDF.toJSON().collect()[0]\r\n",
                "config_data = json.loads(json_string)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### SAP & Error Logging Initiliazations"
            ],
            "outputs": []
        },
        {
            "execution_count": 16,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "error_logger = ErrorLogging(spark = spark, \r\n",
                "                            oea = oea, \r\n",
                "                            logger = logger)"
            ],
            "outputs": []
        },
        {
            "execution_count": 17,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "sap_utilities = SAPUtilities(spark = spark, \r\n",
                "                             oea = oea)"
            ],
            "outputs": []
        },
        {
            "execution_count": 18,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "sap_to_edfi_complex = config_data.get('sap_to_edfi_complex', {})\r\n",
                "final_columns = config_data.get('final_columns', {})\r\n",
                "final_columns = {key: [col for col in columns if col not in ['rundate']] for key, columns in final_columns.items()}\r\n",
                "\r\n",
                "_ext_TX_cols = config_data.get('_ext_TX_cols', {})\r\n",
                "descriptorsDFRef = config_data.get('descriptorsDFRef', {})\r\n",
                "descriptors = config_data.get('descriptors', [])"
            ],
            "outputs": []
        },
        {
            "execution_count": 19,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "nine_digit_number = random.randint(100000000, 999999999)\r\n",
                "assign_default_variable(variable_name = 'pipeline_execution_id', \r\n",
                "                        default_value = 'Test_1234')\r\n",
                "pipeline_execution_id = pipelineExecutionId\r\n",
                "\r\n",
                "characters = string.ascii_letters + string.digits\r\n",
                "random_word = ''.join(random.choice(characters) for _ in range(15))\r\n",
                "run_id = f\"runid_{random_word}\""
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### EdFi Client Initializations"
            ],
            "outputs": []
        },
        {
            "execution_count": 20,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "logger = logging.getLogger('EdFiAPIClient')\r\n",
                "exception = None\r\n",
                "edfiAPIClient = edfiLandClient = None\r\n",
                "\r\n",
                "from datetime import datetime\r\n",
                "minChangeVer = None\r\n",
                "maxChangeVer = None\r\n",
                "\r\n",
                "# kvSecret_clientId = None # oea._get_secret(\"oea-edfi-api-client-id\")\r\n",
                "# kvSecret_clientSecret = None # oea._get_secret(\"oea-edfi-api-client-secret\")\r\n",
                "\r\n",
                "try:\r\n",
                "    global edfiAPIClient, edfiLandClient\r\n",
                "    edfiAPIClient = SAPEdFiClient(workspace = workspace, \r\n",
                "                                    kvName = kvName, #NOTE: Default to None \r\n",
                "                                    moduleName = moduleName, \r\n",
                "                                    authUrl = authUrl, \r\n",
                "                                    dataManagementUrl = dataManagementUrl, \r\n",
                "                                    changeQueriesUrl = changeQueriesUrl, \r\n",
                "                                    dependenciesUrl = dependenciesUrl, \r\n",
                "                                    apiVersion = apiVersion, \r\n",
                "                                    batchLimit = batchLimit, \r\n",
                "                                    minChangeVer = minChangeVer, \r\n",
                "                                    maxChangeVer = maxChangeVer,\r\n",
                "                                    oea = oea,\r\n",
                "                                    schoolYear = schoolYear,\r\n",
                "                                    districtId = districtId,\r\n",
                "                                    final_columns  = final_columns,\r\n",
                "                                    lookup_table_name = 'submissions_lookup_table',\r\n",
                "                                    lookup_table_base_path = 'stage1/Transactional/SAP/metadata-assets',\r\n",
                "                                    lookup_db_name = f'ldb_{workspace}_sap_etl_logs',\r\n",
                "                                    kvSecret_clientId = kvSecret_clientId,\r\n",
                "                                    kvSecret_clientSecret = kvSecret_clientSecret)\r\n",
                "except Exception as exception:\r\n",
                "    logger.exception(exception)\r\n",
                "\r\n",
                "# FIXME: Temporary workaround to deal with REDACTED values\r\n",
                "# edfiAPIClient.clientId = client_id#oea._get_secret('oea-edfi-api-client-id')\r\n",
                "# edfiAPIClient.clientSecret = client_secret_id#oea._get_secret('oea-edfi-api-client-secret')"
            ],
            "outputs": []
        },
        {
            "execution_count": 21,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "entities_info = edfiAPIClient.getEntities()"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Main Code"
            ],
            "outputs": []
        },
        {
            "execution_count": 22,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from datetime import datetime\r\n",
                "from pyspark.sql.functions import lit\r\n",
                "\r\n",
                "class EdfiSubmissionProcessor:\r\n",
                "    def __init__(self, oea, edfiAPIClient, error_logger, logger, spark, pipelineExecutionId, test_mode=False):\r\n",
                "        self.oea = oea\r\n",
                "        self.edfiAPIClient = edfiAPIClient\r\n",
                "        self.error_logger = error_logger\r\n",
                "        self.logger = logger\r\n",
                "        self.spark = spark\r\n",
                "        self.test_mode = test_mode\r\n",
                "        self.pipelineExecutionId = pipelineExecutionId\r\n",
                "    \r\n",
                "    def set_submission_type(self, sap_pipeline, sap_pipelineType):\r\n",
                "        self.sap_pipeline = sap_pipeline\r\n",
                "        self.sap_pipelineType = sap_pipelineType\r\n",
                "\r\n",
                "    def extract_resources_dict(self, file_path):\r\n",
                "        items = self.oea.get_folders(file_path)\r\n",
                "        resource_names = [item for item in items if not(item == 'data-submission-logs' or item.lower().endswith('descriptors'))]\r\n",
                "\r\n",
                "        resource_json_dict = self.edfiAPIClient.getDataForEdFiPosts(\r\n",
                "            resource_names = resource_names,\r\n",
                "            file_path = file_path,\r\n",
                "            resource_json_dict = dict())\r\n",
                "        resource_names = {resource: resource for resource in resource_names}\r\n",
                "        return resource_json_dict, resource_names\r\n",
                "\r\n",
                "    def post_to_edfi_resources(self, \r\n",
                "                               run_id = None,\r\n",
                "                               resource_json_dict = None, \r\n",
                "                               resource_names = None,\r\n",
                "                               data_sample = None, \r\n",
                "                               test_mode = True):\r\n",
                "        # global pipelineExecutionId\r\n",
                "        for stage3Name, edfiName in resource_names.items():\r\n",
                "            start_time = datetime.now()\r\n",
                "\r\n",
                "            if test_mode:\r\n",
                "                data_slice = data_sample if data_sample else resource_json_dict[edfiName]\r\n",
                "            else:\r\n",
                "                data_slice = resource_json_dict[edfiName]\r\n",
                "\r\n",
                "            try:\r\n",
                "                self.logger.info(f\"POST Requests for the Resource: {stage3Name}\")\r\n",
                "                if stage3Name == 'staffs' or stage3Name == 'staffEducationOrganizationEmploymentAssociations' or stage3Name == 'staffEducationOrganizationAssignmentAssociations':\r\n",
                "                    self.edfiAPIClient.upsert_records(\r\n",
                "                        pipeline_execution_id = self.pipelineExecutionId,\r\n",
                "                        run_id = run_id,\r\n",
                "                        resource = f'/ed-fi/{edfiName}',\r\n",
                "                        resource_name = stage3Name,\r\n",
                "                        records = data_slice,\r\n",
                "                        chunk_size = 500,\r\n",
                "                        num_threads = 10,\r\n",
                "                        function_name = 'post',\r\n",
                "                        success_logging = True,\r\n",
                "                        error_logging = True\r\n",
                "                    )\r\n",
                "\r\n",
                "                    end_time = datetime.now()\r\n",
                "                    log_data = self.error_logger.create_log_dict(\r\n",
                "                        uniqueId = self.error_logger.generate_random_alphanumeric(10),\r\n",
                "                        pipelineExecutionId = self.pipelineExecutionId,\r\n",
                "                        sparkSessionId = self.spark.sparkContext.applicationId,\r\n",
                "                        sap_pipeline = self.sap_pipeline,\r\n",
                "                        sap_pipelineType = self.sap_pipelineType,\r\n",
                "                        stageName = \"Submission\",\r\n",
                "                        schemaFormat = 'ed-fi',\r\n",
                "                        entityType = 'ed-fi',\r\n",
                "                        entityName = edfiName,\r\n",
                "                        numInputRows = len(data_slice),\r\n",
                "                        totalNumOutputRows = len(data_slice),\r\n",
                "                        numTargetRowsInserted = 0,\r\n",
                "                        numTargetRowsUpdated = 0,\r\n",
                "                        numRecordsSkipped = 0,\r\n",
                "                        numRecordsDeleted = 0,\r\n",
                "                        start_time = start_time,\r\n",
                "                        end_time = end_time,                                                \r\n",
                "                        insertionType = 'upsert',\r\n",
                "                        emptySchemaMetadata = False\r\n",
                "                    )\r\n",
                "                else:\r\n",
                "                    self.edfiAPIClient.upsert_records(\r\n",
                "                        pipeline_execution_id = self.pipelineExecutionId,\r\n",
                "                        run_id = run_id,\r\n",
                "                        resource = f'/TX/{edfiName}',\r\n",
                "                        resource_name = stage3Name,\r\n",
                "                        records = data_slice,\r\n",
                "                        chunk_size = 500,\r\n",
                "                        num_threads = 10,\r\n",
                "                        function_name = 'post',\r\n",
                "                        success_logging = True,\r\n",
                "                        error_logging = True\r\n",
                "                    )\r\n",
                "\r\n",
                "                    end_time = datetime.now()\r\n",
                "                    log_data = self.error_logger.create_log_dict(\r\n",
                "                        uniqueId = self.error_logger.generate_random_alphanumeric(10),\r\n",
                "                        pipelineExecutionId = self.pipelineExecutionId,\r\n",
                "                        sparkSessionId = self.spark.sparkContext.applicationId,\r\n",
                "                        sap_pipeline = self.sap_pipeline,\r\n",
                "                        sap_pipelineType = self.sap_pipelineType,\r\n",
                "                        stageName = \"Submission\",\r\n",
                "                        schemaFormat = 'ed-fi',\r\n",
                "                        entityType = 'tx',\r\n",
                "                        entityName = edfiName,\r\n",
                "                        numInputRows = len(data_slice),\r\n",
                "                        totalNumOutputRows = len(data_slice),\r\n",
                "                        numTargetRowsInserted = 0,\r\n",
                "                        numTargetRowsUpdated = 0,\r\n",
                "                        numRecordsSkipped = 0,\r\n",
                "                        numRecordsDeleted = 0,\r\n",
                "                        start_time = start_time,\r\n",
                "                        end_time = end_time,\r\n",
                "                        insertionType = 'upsert',\r\n",
                "                        emptySchemaMetadata = False\r\n",
                "                    )\r\n",
                "\r\n",
                "                self.error_logger.consolidate_logs(log_data, 'entity')\r\n",
                "            except Exception as e:\r\n",
                "                self.logger.info(f\"Exception {e}\")\r\n",
                "\r\n",
                "    def process_logs(self, df, sap_pipeline, sap_pipelineType, is_post_success, run_date):\r\n",
                "        df = df.withColumn('sap_pipeline', lit(sap_pipeline))\r\n",
                "        df = df.withColumn('sap_pipelineType', lit(sap_pipelineType))\r\n",
                "        df = df.withColumn('is_post_success', lit(is_post_success))\r\n",
                "        df = df.withColumn('run_date', lit(run_date))\r\n",
                "        df = df.withColumn('entityType', F.split(col('resource'), '/')[1])\r\n",
                "        df = df.withColumn('entityName', F.split(col('resource'), '/')[2])\r\n",
                "        df = df.withColumn('log_type', lit('submission'))\r\n",
                "        return df\r\n",
                "\r\n",
                "    def write_logs(self, df, log_file_url):\r\n",
                "        if self.test_mode:\r\n",
                "            self.logger.info('TEST MODE - LOGS NOT WRITTEN')\r\n",
                "            return df, log_file_url\r\n",
                "        else:\r\n",
                "            # self.spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\r\n",
                "            df.write.format('delta').partitionBy('sap_pipeline', 'sap_pipelineType', 'entityName').mode('append').save(log_file_url)\r\n",
                "            return None, None\r\n",
                "\r\n",
                "    def return_resource_json_dict(self, apiVersion, ext_type):\r\n",
                "        file_path = f'stage3/pipeline={self.sap_pipeline}/pipelineType={self.sap_pipelineType}/{apiVersion}/general/{ext_type}'\r\n",
                "        try:\r\n",
                "            resource_json_dict, resource_names = self.extract_resources_dict(file_path)\r\n",
                "        except:\r\n",
                "            self.logger.exception(f'[POST TO ED-FI] Loading Data From Stage 3 {file_path}')\r\n",
                "            resource_json_dict = dict()\r\n",
                "            resource_names = list()\r\n",
                "        return resource_json_dict, resource_names"
            ],
            "outputs": []
        },
        {
            "execution_count": 23,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "test_mode = False\r\n",
                "testEntity = 'contractedInstructionalStaffFTEExts'\r\n",
                "sap_pipeline = zone = submissions_type #= 'peims-submissions' \r\n",
                "\r\n",
                "edfiSubmissionProcessor = EdfiSubmissionProcessor(oea = oea, \r\n",
                "                                                  edfiAPIClient = edfiAPIClient, \r\n",
                "                                                  error_logger = error_logger, \r\n",
                "                                                  logger = logger, \r\n",
                "                                                  spark = spark, \r\n",
                "                                                  pipelineExecutionId = pipelineExecutionId, \r\n",
                "                                                  test_mode = False)"
            ],
            "outputs": []
        },
        {
            "execution_count": 24,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "edfiSubmissionProcessor.set_submission_type(sap_pipeline = sap_pipeline, \r\n",
                "                                            sap_pipelineType = sap_pipelineType) \r\n",
                "tx_resource_json_dict, tx_resource_names = edfiSubmissionProcessor.return_resource_json_dict(apiVersion = apiVersion,\r\n",
                "                                                                        ext_type = 'tx')\r\n",
                "edfi_resource_json_dict, edfi_resource_names = edfiSubmissionProcessor.return_resource_json_dict(apiVersion = apiVersion,\r\n",
                "                                                                        ext_type = 'ed-fi')\r\n",
                "if test_mode:  \r\n",
                "    if testEntity in edfi_resource_json_dict.keys():\r\n",
                "        data_sample = edfi_resource_json_dict[testEntity]    \r\n",
                "    elif testEntity in tx_resource_json_dict.keys():\r\n",
                "        data_sample = tx_resource_json_dict[testEntity]    \r\n",
                "else:\r\n",
                "    data_sample = None\r\n",
                "\r\n",
                "for ext_type in ['ed-fi', 'tx']:\r\n",
                "    if ext_type == 'ed-fi':\r\n",
                "        copy_resource_json_dict = copy.deepcopy(edfi_resource_json_dict)\r\n",
                "        resource_names = edfi_resource_names\r\n",
                "    elif ext_type == 'tx':\r\n",
                "        copy_resource_json_dict = copy.deepcopy(tx_resource_json_dict)\r\n",
                "        resource_names = tx_resource_names\r\n",
                "    if test_mode and testEntity in copy_resource_json_dict.keys():\r\n",
                "        edfiSubmissionProcessor.post_to_edfi_resources(run_id = run_id,\r\n",
                "                            resource_json_dict = copy_resource_json_dict,\r\n",
                "                            resource_names = {testEntity: testEntity}, \r\n",
                "                            data_sample = data_sample)\r\n",
                "    elif test_mode and testEntity not in copy_resource_json_dict.keys():\r\n",
                "        pass\r\n",
                "    elif copy_resource_json_dict != dict():\r\n",
                "        edfiSubmissionProcessor.post_to_edfi_resources(run_id = run_id,\r\n",
                "                            resource_json_dict = copy_resource_json_dict,\r\n",
                "                            resource_names =  resource_names, \r\n",
                "                            data_sample = None)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Ed-Fi IDs - Lookup Table"
            ],
            "outputs": []
        },
        {
            "execution_count": 25,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# TODO: Partitioning by ['entity_name']\r\n",
                "# NOTE: current version of upsert has bug\r\n",
                "# UPSERT is only valid for partitionigs DI and SY; otherwise dynamic overwrites \r\n",
                "# For Partitions\r\n",
                "edfi_ids_lookup_table_path = 'stage1/Transactional/SAP/metadata-assets/edfi_ids_lookup_table'\r\n",
                "if edfiAPIClient.edfi_id_records != [] and sap_pipeline != 'analytics':\r\n",
                "    edfi_id_records_df = spark.createDataFrame(edfiAPIClient.edfi_id_records, schema = edfiAPIClient.edfi_id_record_schema) if len(edfiAPIClient.edfi_id_records) != 0 else spark.createDataFrame([], schema = edfiAPIClient.edfi_id_record_schema)\r\n",
                "    oea.upsert(df = edfi_id_records_df,\r\n",
                "               destination_path = edfi_ids_lookup_table_path,\r\n",
                "               primary_key = 'edfi_location',\r\n",
                "               partitioning = False,\r\n",
                "               partitioning_cols = [],\r\n",
                "               surrogate_key = False)\r\n",
                "    spark.sql(f'CREATE DATABASE IF NOT EXISTS ldb_{workspace}_sap_etl_logs')\r\n",
                "    spark.sql(f\"drop table if exists ldb_{workspace}_sap_etl_logs.edfi_ids_lookup_table\")\r\n",
                "    spark.sql(f\"create table if not exists ldb_{workspace}_sap_etl_logs.edfi_ids_lookup_table using DELTA location '{oea.to_url(edfi_ids_lookup_table_path)}'\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Submissions - Lookup Table"
            ],
            "outputs": []
        },
        {
            "execution_count": 26,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "if edfiAPIClient.max_rundates != []:\r\n",
                "    edfiAPIClient.dump_lookup_table()\r\n",
                "    edfiAPIClient.add_lookup_table_to_lake_db(overwrite = True)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Error Logging"
            ],
            "outputs": []
        },
        {
            "execution_count": 27,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "if edfiSubmissionProcessor.error_logger.entity_logs != [] and sap_pipeline != 'analytics':\r\n",
                "    df_logs = edfiSubmissionProcessor.error_logger.create_spark_df('entity')\r\n",
                "    edfiSubmissionProcessor.error_logger.write_logs_to_delta_lake(df = df_logs, \r\n",
                "                                log_type = 'entity',\r\n",
                "                                destination_url = edfiSubmissionProcessor.error_logger.to_logs_url('etl-logs/log_type=entity'))\r\n",
                "    edfiSubmissionProcessor.error_logger.add_etl_logs_to_lake_db(db_name = f'ldb_{workspace}_sap_etl_logs',\r\n",
                "                                        logs_base_path = 'etl-logs',\r\n",
                "                                        log_type = 'entity',\r\n",
                "                                        overwrite = False)"
            ],
            "outputs": []
        },
        {
            "execution_count": 28,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "if sap_pipeline != 'analytics':\r\n",
                "    error_df = spark.createDataFrame(edfiAPIClient.error_logs, schema = edfiAPIClient.log_schema) if len(edfiAPIClient.error_logs) != 0 else spark.createDataFrame([], schema = edfiAPIClient.log_schema)\r\n",
                "    success_df = spark.createDataFrame(edfiAPIClient.success_logs, schema = edfiAPIClient.log_schema) if len(edfiAPIClient.success_logs) != 0 else spark.createDataFrame([], schema = edfiAPIClient.log_schema)\r\n",
                "\r\n",
                "    current_datetime = datetime.today()\r\n",
                "    run_date = current_datetime.strftime(\"%Y-%m-%d\")\r\n",
                "    test_mode = False"
            ],
            "outputs": []
        },
        {
            "execution_count": 29,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "if sap_pipeline != 'analytics':\r\n",
                "    # NOTE: If all is passed all will be the submission_type\r\n",
                "    success_df = edfiSubmissionProcessor.process_logs(df = success_df, \r\n",
                "                                sap_pipeline = sap_pipeline,\r\n",
                "                                sap_pipelineType = sap_pipelineType,\r\n",
                "                                is_post_success = True, \r\n",
                "                                run_date = run_date)\r\n",
                "    # NOTE: If all is passed all will be the submission_type\r\n",
                "    error_df = edfiSubmissionProcessor.process_logs(df = error_df, \r\n",
                "                                sap_pipeline = sap_pipeline,\r\n",
                "                                sap_pipelineType = sap_pipelineType,\r\n",
                "                                is_post_success = False, \r\n",
                "                                run_date = run_date)\r\n",
                "    logs_df = success_df.union(error_df)"
            ],
            "outputs": []
        },
        {
            "execution_count": 30,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "if sap_pipeline != 'analytics':\r\n",
                "   log_file_url = edfiSubmissionProcessor.error_logger.to_logs_url('etl-logs/log_type=submissions')\r\n",
                "   df_delta, _ = edfiSubmissionProcessor.write_logs(df = logs_df, \r\n",
                "                           log_file_url = log_file_url)\r\n",
                "   edfiSubmissionProcessor.error_logger.add_etl_logs_to_lake_db(db_name = f'ldb_{workspace}_sap_etl_logs',\r\n",
                "                                       logs_base_path = 'etl-logs',\r\n",
                "                                       log_type = 'submissions',\r\n",
                "                                          overwrite = True)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Update Stage 3"
            ],
            "outputs": []
        },
        {
            "execution_count": 31,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "logs_df.createOrReplaceTempView('temp_edfi_logs_table')\r\n",
                "ext_types = ['ed-fi', 'tx']\r\n",
                "primary_key = ['NATURAL_KEY_HASH', 'sap_pipeline','sap_pipelineType']\r\n",
                "update_cols = {\"sink.edfi_location\": \"updates.edfi_location\", \r\n",
                "               \"sink.edfi_id\": \"updates.edfi_id\", \r\n",
                "               \"sink.edfi_id_modified\": \"updates.edfi_id_modified\"}\r\n",
                "\r\n",
                "if sap_pipeline != 'analytics':\r\n",
                "    for ext_type in ext_types:\r\n",
                "        file_path = f'stage3/pipeline={sap_pipeline}/pipelineType={sap_pipelineType}/{apiVersion}/general/{ext_type}'\r\n",
                "        items = oea.get_folders(file_path)\r\n",
                "        for item in items:\r\n",
                "            logger.info(f'Updating stage 3 delta lake for the item - {item}')\r\n",
                "            query = f\"\"\"SELECT NATURAL_KEY_HASH, \r\n",
                "                                         sap_pipeline as sap_pipeline,\r\n",
                "                                         sap_pipelineType as sap_pipelineType,\r\n",
                "                                         edfi_location, \r\n",
                "                                         edfi_id, \r\n",
                "                                         edfi_id_modified \r\n",
                "                      FROM temp_edfi_logs_table\r\n",
                "                      WHERE is_post_success = True\r\n",
                "                        AND entityName = '{item}'\r\n",
                "                        \"\"\"\r\n",
                "            temp_df = spark.sql(query)\r\n",
                "            entity_path = f\"{file_path}/{item}\"\r\n",
                "            destination_url = oea.to_url(entity_path)\r\n",
                "\r\n",
                "            pk_statement = oea.return_pk_statement(primary_key)\r\n",
                "            if DeltaTable.isDeltaTable(spark, destination_url):\r\n",
                "                logger.info('TRUE UPSERT')\r\n",
                "                delta_table_sink = DeltaTable.forPath(spark, destination_url)\r\n",
                "                delta_table_sink.alias('sink').merge(temp_df.alias('updates'), pk_statement).whenMatchedUpdate(set = update_cols).execute()# .whenNotMatchedInsert(values = insert_cols).execute()\r\n",
                "            else:\r\n",
                "                logger.error(f'Invalid stage 3 delta location for the item - {item}')"
            ],
            "outputs": []
        }
    ]
}