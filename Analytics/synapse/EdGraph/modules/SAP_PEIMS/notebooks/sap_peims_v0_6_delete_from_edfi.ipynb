{
    "nbformat": 4,
    "nbformat_minor": 2,
    "metadata": {
        "save_output": true,
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "execution_count": 40,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "instance = InstanceId = instanceId\n",
                "ApiUrl = apiUrl\n",
                "SchoolYear = schoolYear\n",
                "\n",
                "districtPath = DistrictId = DistrictID  = districtID = districtId\n",
                "apiLimit = batchLimit\n",
                "\n",
                "prepareSAPMetaData = prepareSAPMetadata\n",
                "zone = submissionsType = sap_pipeline"
            ],
            "outputs": []
        },
        {
            "execution_count": 41,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "def assign_default_variable(variable_name, default_value):\n",
                "    if variable_name not in globals():\n",
                "        globals()[variable_name] = default_value\n",
                "        logger.info(f'{variable_name} not found - using system default')"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Pre-Requisites (Dev)"
            ],
            "outputs": []
        },
        {
            "execution_count": 42,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from notebookutils import mssparkutils\n",
                "import configparser\n",
                "import random\n",
                "\n",
                "config_path = \"/tmp/conf.ini\"\n",
                "def copy_config_to_temp():\n",
                "    mssparkutils.fs.cp(oea.to_url(\"stage1/Transactional/SAP/metadata-assets/edfi-configs.ini\"),\"file:/tmp/conf.ini\")\n",
                "\n",
                "def read_edfi_credentials(config_path):\n",
                "    config = configparser.ConfigParser()\n",
                "    config.read(config_path)\n",
                "\n",
                "    edfi_credentials = {}\n",
                "\n",
                "    if 'EdFi' in config:\n",
                "        edfi_credentials['client_id'] = config['EdFi'].get('client_id', '')\n",
                "        edfi_credentials['client_secret'] = config['EdFi'].get('client_secret', '')\n",
                "    \n",
                "    return edfi_credentials"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Actual Code"
            ],
            "outputs": []
        },
        {
            "execution_count": 43,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "import pyspark\n",
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql import Column\n",
                "from pyspark.sql.utils import AnalysisException\n",
                "from pyspark.sql.types import * #StringType, StructType, StructField, IntegerType, DateType\n",
                "\n",
                "import pyspark.sql.functions as F\n",
                "from pyspark.sql.functions import col, struct, concat, lit, round, concat, array\n",
                "from pyspark.sql.functions import regexp_replace, expr, when, date_format, to_date\n",
                "\n",
                "import uuid\n",
                "from datetime import datetime\n",
                "import logging\n",
                "import json\n",
                "import csv\n",
                "import copy\n",
                "\n",
                "import threading\n",
                "import requests\n",
                "from requests.auth import HTTPBasicAuth\n",
                "\n",
                "import random\n",
                "import string"
            ],
            "outputs": []
        },
        {
            "execution_count": 44,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "print('Submissions - TESTING PARAMETERIZATION')\n",
                "try:\n",
                "    print(kVName)\n",
                "    print(workspace)\n",
                "    print(apiUrl)\n",
                "    print(instanceId)\n",
                "    print(moduleName)\n",
                "    print(apiLimit)\n",
                "    print(minChangeVer)\n",
                "    print(maxChangeVer)\n",
                "    print(sapVersion)\n",
                "    print(prepareSAPMetaData)\n",
                "    print(submissions)\n",
                "    print(sap_pipeline)\n",
                "    print(sap_pipelineType)\n",
                "    print(schoolYear)\n",
                "    print(districtID)\n",
                "    print(pipelineExecutionId)\n",
                "\n",
                "    kvName = kVName\n",
                "    districtId = districtID\n",
                "    districtPath = districtId\n",
                "except Exception as params_error:\n",
                "    print('CATCHING ERROR!!!')\n",
                "    print(params_error)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### URL Initializations"
            ],
            "outputs": []
        },
        {
            "execution_count": 45,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "%run OEA/modules/Ed-Fi/v0.7/src/utilities/edfi_v0_7_fetch_urls"
            ],
            "outputs": []
        },
        {
            "execution_count": 46,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "instance_id = instanceId\n",
                "school_year = schoolYear\n",
                "api_year = school_year\n",
                "api_url = apiUrl\n",
                "\n",
                "# FIXME: 2024-01-31 TEMP FIX FOR FY\n",
                "try:\n",
                "    edfi_api_manager = EdFiApiManager(api_url, instance_id, api_year)\n",
                "    edfi_api_manager.update_urls()\n",
                "    edfi_api_manager.set_other_metadata()\n",
                "\n",
                "    dependenciesUrl = edfi_api_manager.dependencies_url\n",
                "    openApiMetadataUrl = edfi_api_manager.openapi_metadata_url\n",
                "    dataManagementUrl = edfi_api_manager.data_management_url\n",
                "    authUrl = edfi_api_manager.auth_url\n",
                "\n",
                "    changeQueriesUrl = edfi_api_manager.get_referenced_url('Change-Queries')\n",
                "    changeQueriesUrl = changeQueriesUrl[:-13].replace('/metadata/', '/')\n",
                "    swagger_url = swaggerUrl = edfi_api_manager.get_referenced_url('Resources')\n",
                "\n",
                "    apiVersion = edfi_api_manager.api_version\n",
                "    apiVersion = apiVersion[1:] if apiVersion.startswith('v') else apiVersion\n",
                "except Exception as error:\n",
                "    edfi_api_manager = EdFiApiManager(api_url, instance_id, '')\n",
                "    edfi_api_manager.update_urls()\n",
                "    edfi_api_manager.set_other_metadata()\n",
                "\n",
                "    dependenciesUrl = edfi_api_manager.dependencies_url\n",
                "    openApiMetadataUrl = edfi_api_manager.openapi_metadata_url\n",
                "    dataManagementUrl = edfi_api_manager.data_management_url\n",
                "    authUrl = edfi_api_manager.auth_url\n",
                "\n",
                "    changeQueriesUrl = edfi_api_manager.get_referenced_url('Change-Queries')\n",
                "    changeQueriesUrl = changeQueriesUrl[:-13].replace('/metadata/', '/')\n",
                "    swagger_url = swaggerUrl = edfi_api_manager.get_referenced_url('Resources')\n",
                "\n",
                "    apiVersion = edfi_api_manager.api_version\n",
                "    apiVersion = apiVersion[1:] if apiVersion.startswith('v') else apiVersion"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### OEA Initializations"
            ],
            "outputs": []
        },
        {
            "execution_count": 47,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "%run EdGraph/modules/SAP_PEIMS/v0.6/src/utilities/sap_peim_v0_6_sap_py"
            ],
            "outputs": []
        },
        {
            "execution_count": 48,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# TODO: Rename Relevant Child Class to follow a more intuitive naming convention\n",
                "from datetime import datetime\n",
                "oea = SAPEdFiOEAChild()   \n",
                "oea.set_workspace(workspace)"
            ],
            "outputs": []
        },
        {
            "execution_count": 49,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# swagger_url = swaggerUrl = edfi_api_manager.get_referenced_url('Descriptors')\n",
                "oea_utils = schema_gen = SAPOpenAPIUtilChild(swagger_url)\n",
                "oea_utils.create_definitions()\n",
                "schemas = schema_gen.create_spark_schemas()"
            ],
            "outputs": []
        },
        {
            "execution_count": 50,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# Set Ed-Fi Credentials\n",
                "# copy_config_to_temp()\n",
                "\n",
                "# credentials = read_edfi_credentials(config_path)\n",
                "# client_id = credentials.get('client_id')\n",
                "# client_secret_id = credentials.get('client_secret')"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Metadata Processing"
            ],
            "outputs": []
        },
        {
            "execution_count": 51,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "metadata_path = \"stage1/Transactional/SAP/metadata-assets/sap-to-edfi.json\"\n",
                "metadata_url = oea.to_url(metadata_path)"
            ],
            "outputs": []
        },
        {
            "execution_count": 52,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "jsonDF = spark.read.option(\"multiline\", \"true\").json(metadata_url).cache()\n",
                "\n",
                "json_string = jsonDF.toJSON().collect()[0]\n",
                "config_data = json.loads(json_string)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### SAP & Error Logging Initiliazations"
            ],
            "outputs": []
        },
        {
            "execution_count": 53,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "error_logger = ErrorLogging(spark = spark, \n",
                "                            oea = oea, \n",
                "                            logger = logger)"
            ],
            "outputs": []
        },
        {
            "execution_count": 54,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "sap_utilities = SAPUtilities(spark = spark, \n",
                "                             oea = oea)"
            ],
            "outputs": []
        },
        {
            "execution_count": 55,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "sap_to_edfi_complex = config_data.get('sap_to_edfi_complex', {})\n",
                "final_columns = config_data.get('final_columns', {})\n",
                "final_columns = {key: [col for col in columns if col not in ['rundate']] for key, columns in final_columns.items()}\n",
                "\n",
                "_ext_TX_cols = config_data.get('_ext_TX_cols', {})\n",
                "descriptorsDFRef = config_data.get('descriptorsDFRef', {})\n",
                "descriptors = config_data.get('descriptors', [])"
            ],
            "outputs": []
        },
        {
            "execution_count": 56,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "nine_digit_number = random.randint(100000000, 999999999)\n",
                "assign_default_variable(variable_name = 'pipeline_execution_id', \n",
                "                        default_value = 'Test_1234')\n",
                "pipeline_execution_id = pipelineExecutionId\n",
                "\n",
                "characters = string.ascii_letters + string.digits\n",
                "random_word = ''.join(random.choice(characters) for _ in range(15))\n",
                "run_id = f\"runid_{random_word}\""
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### EdFi Client Initializations"
            ],
            "outputs": []
        },
        {
            "execution_count": 57,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "logger = logging.getLogger('EdFiAPIClient')\n",
                "exception = None\n",
                "edfiAPIClient = edfiLandClient = None\n",
                "\n",
                "from datetime import datetime\n",
                "minChangeVer = None\n",
                "maxChangeVer = None\n",
                "\n",
                "# kvSecret_clientId = None # oea._get_secret(\"oea-edfi-api-client-id\")\n",
                "# kvSecret_clientSecret = None # oea._get_secret(\"oea-edfi-api-client-secret\")\n",
                "\n",
                "try:\n",
                "    global edfiAPIClient, edfiLandClient\n",
                "    edfiAPIClient = SAPEdFiClient(workspace = workspace, \n",
                "                                    kvName = kvName, #NOTE: Default to None \n",
                "                                    moduleName = moduleName, \n",
                "                                    authUrl = authUrl, \n",
                "                                    dataManagementUrl = dataManagementUrl, \n",
                "                                    changeQueriesUrl = changeQueriesUrl, \n",
                "                                    dependenciesUrl = dependenciesUrl, \n",
                "                                    apiVersion = apiVersion, \n",
                "                                    batchLimit = batchLimit, \n",
                "                                    minChangeVer = minChangeVer, \n",
                "                                    maxChangeVer = maxChangeVer,\n",
                "                                    oea = oea,\n",
                "                                    schoolYear = schoolYear,\n",
                "                                    districtId = districtId,\n",
                "                                    final_columns  = final_columns,\n",
                "                                    lookup_table_name = 'submissions_lookup_table',\n",
                "                                    lookup_table_base_path = 'stage1/Transactional/SAP/metadata-assets',\n",
                "                                    lookup_db_name = f'ldb_{workspace}_sap_etl_logs',\n",
                "                                    kvSecret_clientId = kvSecret_clientId,\n",
                "                                    kvSecret_clientSecret = kvSecret_clientSecret)\n",
                "except Exception as exception:\n",
                "    logger.exception(exception)"
            ],
            "outputs": []
        },
        {
            "execution_count": 58,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "entities_info = edfiAPIClient.getEntities()"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Utility Functions\n",
                "1. EdFiSubmissionProcessor Class\n",
                "2. Utilities To Selectively Delete\n",
                "3. Utilities To Return only the relevant ed-fi entities"
            ],
            "outputs": []
        },
        {
            "execution_count": 59,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from datetime import datetime\n",
                "from pyspark.sql.functions import lit\n",
                "\n",
                "class EdfiSubmissionProcessor:\n",
                "    def __init__(self, oea, edfiAPIClient, error_logger, logger, spark, pipelineExecutionId, test_mode=False):\n",
                "        self.oea = oea\n",
                "        self.edfiAPIClient = edfiAPIClient\n",
                "        self.error_logger = error_logger\n",
                "        self.logger = logger\n",
                "        self.spark = spark\n",
                "        self.test_mode = test_mode\n",
                "        self.pipelineExecutionId = pipelineExecutionId\n",
                "    \n",
                "    def set_submission_type(self, sap_pipeline, sap_pipelineType):\n",
                "        self.sap_pipeline = sap_pipeline\n",
                "        self.sap_pipelineType = sap_pipelineType\n",
                "\n",
                "    def extract_resources_dict(self, file_path):\n",
                "        items = self.oea.get_folders(file_path)\n",
                "        resource_names = [item for item in items if not(item == 'data-submission-logs' or item.lower().endswith('descriptors'))]\n",
                "\n",
                "        resource_json_dict = self.edfiAPIClient.getDataForEdFiPosts(\n",
                "            resource_names = resource_names,\n",
                "            file_path = file_path,\n",
                "            resource_json_dict = dict())\n",
                "        resource_names = {resource: resource for resource in resource_names}\n",
                "        return resource_json_dict, resource_names\n",
                "\n",
                "    def post_to_edfi_resources(self, \n",
                "                               run_id = None,\n",
                "                               resource_json_dict = None, \n",
                "                               resource_names = None,\n",
                "                               data_sample = None, \n",
                "                               test_mode = True):\n",
                "        # global pipelineExecutionId\n",
                "        for stage3Name, edfiName in resource_names.items():\n",
                "            start_time = datetime.now()\n",
                "\n",
                "            if test_mode:\n",
                "                data_slice = data_sample if data_sample else resource_json_dict[edfiName]\n",
                "            else:\n",
                "                data_slice = resource_json_dict[edfiName]\n",
                "\n",
                "            try:\n",
                "                self.logger.info(f\"POST Requests for the Resource: {stage3Name}\")\n",
                "                if stage3Name == 'staffs' or stage3Name == 'staffEducationOrganizationEmploymentAssociations' or stage3Name == 'staffEducationOrganizationAssignmentAssociations':\n",
                "                    self.edfiAPIClient.upsert_records(\n",
                "                        pipeline_execution_id = self.pipelineExecutionId,\n",
                "                        run_id = run_id,\n",
                "                        resource = f'/ed-fi/{edfiName}',\n",
                "                        resource_name = stage3Name,\n",
                "                        records = data_slice,\n",
                "                        chunk_size = 500,\n",
                "                        num_threads = 10,\n",
                "                        function_name = 'post',\n",
                "                        success_logging = True,\n",
                "                        error_logging = True\n",
                "                    )\n",
                "\n",
                "                    end_time = datetime.now()\n",
                "                    log_data = self.error_logger.create_log_dict(\n",
                "                        uniqueId = self.error_logger.generate_random_alphanumeric(10),\n",
                "                        pipelineExecutionId = self.pipelineExecutionId,\n",
                "                        sparkSessionId = self.spark.sparkContext.applicationId,\n",
                "                        etlType = self.sap_pipeline,\n",
                "                        stageName = \"Submission\",\n",
                "                        schemaFormat = 'ed-fi',\n",
                "                        entityType = 'ed-fi',\n",
                "                        entityName = edfiName,\n",
                "                        numInputRows = len(data_slice),\n",
                "                        totalNumOutputRows = len(data_slice),\n",
                "                        numTargetRowsInserted = 0,\n",
                "                        numTargetRowsUpdated = 0,\n",
                "                        numRecordsSkipped = 0,\n",
                "                        # numRecordsDeleted = 0,\n",
                "                        start_time = start_time,\n",
                "                        end_time = end_time,                                                \n",
                "                        insertionType = 'upsert',\n",
                "                        emptySchemaMetadata = False\n",
                "                    )\n",
                "                else:\n",
                "                    self.edfiAPIClient.upsert_records(\n",
                "                        pipeline_execution_id = self.pipelineExecutionId,\n",
                "                        run_id = run_id,\n",
                "                        resource = f'/TX/{edfiName}',\n",
                "                        resource_name = stage3Name,\n",
                "                        records = data_slice,\n",
                "                        chunk_size = 500,\n",
                "                        num_threads = 10,\n",
                "                        function_name = 'post',\n",
                "                        success_logging = True,\n",
                "                        error_logging = True\n",
                "                    )\n",
                "\n",
                "                    end_time = datetime.now()\n",
                "                    log_data = self.error_logger.create_log_dict(\n",
                "                        uniqueId = self.error_logger.generate_random_alphanumeric(10),\n",
                "                        pipelineExecutionId = self.pipelineExecutionId,\n",
                "                        sparkSessionId = self.spark.sparkContext.applicationId,\n",
                "                        etlType = self.sap_pipeline,\n",
                "                        stageName = \"Submission\",\n",
                "                        schemaFormat = 'ed-fi',\n",
                "                        entityType = 'tx',\n",
                "                        entityName = edfiName,\n",
                "                        numInputRows = len(data_slice),\n",
                "                        totalNumOutputRows = len(data_slice),\n",
                "                        numTargetRowsInserted = 0,\n",
                "                        numTargetRowsUpdated = 0,\n",
                "                        numRecordsSkipped = 0,\n",
                "                        # numRecordsDeleted = 0,\n",
                "                        start_time = start_time,\n",
                "                        end_time = end_time,\n",
                "                        insertionType = 'upsert',\n",
                "                        emptySchemaMetadata = False\n",
                "                    )\n",
                "\n",
                "                self.error_logger.consolidate_logs(log_data, 'entity')\n",
                "            except Exception as e:\n",
                "                self.logger.info(f\"Exception {e}\")\n",
                "\n",
                "    def process_logs(self, df, sap_pipeline, sap_pipelineType, is_post_success, run_date):\n",
                "        df = df.withColumn('sap_pipeline', lit(sap_pipeline))\n",
                "        df = df.withColumn('sap_pipelineType', lit(sap_pipelineType))\n",
                "        df = df.withColumn('is_post_success', lit(is_post_success))\n",
                "        df = df.withColumn('run_date', lit(run_date))\n",
                "        df = df.withColumn('entityType', F.split(col('resource'), '/')[1])\n",
                "        df = df.withColumn('entityName', F.split(col('resource'), '/')[2])\n",
                "        df = df.withColumn('log_type', lit('submission'))\n",
                "        return df\n",
                "\n",
                "    def write_logs(self, df, log_file_url):\n",
                "        if self.test_mode:\n",
                "            self.logger.info('TEST MODE - LOGS NOT WRITTEN')\n",
                "            return df, log_file_url\n",
                "        else:\n",
                "            # self.spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
                "            df.write.format('delta').partitionBy('sap_pipeline', 'sap_pipelineType', 'entityName').mode('append').save(log_file_url)\n",
                "            return None, None\n",
                "\n",
                "    def return_resource_json_dict(self, apiVersion, ext_type):\n",
                "        file_path = f'stage3/pipeline={self.sap_pipeline}/pipelineType={self.sap_pipelineType}/{apiVersion}/general/{ext_type}'\n",
                "        try:\n",
                "            resource_json_dict, resource_names = self.extract_resources_dict(file_path)\n",
                "        except:\n",
                "            self.logger.exception(f'[POST TO ED-FI] Loading Data From Stage 3 {file_path}')\n",
                "            resource_json_dict = dict()\n",
                "            resource_names = list()\n",
                "        return resource_json_dict, resource_names"
            ],
            "outputs": []
        },
        {
            "execution_count": 60,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "def return_last_submission_logs(entity_name):\n",
                "    query = f\"\"\"WITH maxPipelineExecutionIdCTE AS\n",
                "                (\n",
                "                    SELECT pipelineExecutionId\n",
                "                    FROM ldb_{workspace}_sap_etl_logs.etlsubmissionslogs\n",
                "                    WHERE entity_name = '{entity_name}'\n",
                "                      AND operation_type != 'delete'\n",
                "                    ORDER BY start_time desc, end_time desc\n",
                "                    LIMIT 1\n",
                "                )\n",
                "                SELECT DISTINCT edfi_id,\n",
                "                                edfi_location,\n",
                "                                NATURAL_KEY_HASH \n",
                "                FROM ldb_{workspace}_sap_etl_logs.etlsubmissionslogs\n",
                "                INNER JOIN maxPipelineExecutionIdCTE \n",
                "                    ON etlsubmissionslogs.pipelineExecutionId = maxPipelineExecutionIdCTE.pipelineExecutionId\n",
                "                WHERE entity_name = '{entity_name}'\n",
                "                  AND response_status_code LIKE '2%' \n",
                "                  AND response_status_code NOT LIKE '204'\n",
                "                \"\"\"\n",
                "    df = spark.sql(query)\n",
                "    return df\n",
                "\n",
                "def return_records_to_be_deleted(entity_name):\n",
                "    query = f\"\"\"SELECT DISTINCT temp_vw_last_logs_df.edfi_id,\n",
                "                                temp_vw_last_logs_df.edfi_location\n",
                "                FROM temp_vw_last_logs_df\n",
                "                LEFT JOIN temp_vw_recent_df\n",
                "                    ON temp_vw_last_logs_df.NATURAL_KEY_HASH = temp_vw_recent_df.NATURAL_KEY_HASH\n",
                "                ---WHERE temp_vw_last_logs_df.NATURAL_KEY_HASH IS NOT NULL\n",
                "                WHERE temp_vw_recent_df.NATURAL_KEY_HASH IS NULL\n",
                "            \"\"\"\n",
                "    df = spark.sql(query)\n",
                "    return df"
            ],
            "outputs": []
        },
        {
            "execution_count": 61,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "def return_sap_entities(sap_pipeline, sap_pipelineType):\n",
                "    if sap_pipeline == 'TEA':\n",
                "        if sap_pipelineType == 'PEIMS_FALL':\n",
                "            return ['YHROHPM04', 'YHROHPM07', 'YHROHPM08', 'YHROHPM09', 'YHROHPM10', 'YFMOHPEIM']\n",
                "        elif sap_pipelineType == 'PEIMS_MIDYR':\n",
                "            return ['YFIOHPEIM']\n",
                "        elif sap_pipelineType == 'PEIMS_EXYR':\n",
                "            return ['YHROHPM04']\n",
                "        elif sap_pipelineType == 'TSDS_ECDS_KG':\n",
                "            return ['YHROHPM03']\n",
                "        elif sap_pipelineType == 'TSDS_CLASS_ROSTER_FALL':\n",
                "            return ['YHROHPM05']\n",
                "        elif sap_pipelineType == 'TSDS_ECDS_PK':\n",
                "            return ['YHROHPM02']\n",
                "    return None\n",
                "    \n",
                "def return_entities_to_delete(tables_source, \n",
                "                              items):\n",
                "    edfi_items = list()\n",
                "    entity_base_path = '/'.join(tables_source.split('/')[2:])\n",
                "    if items is None:\n",
                "        items = oea.get_folders(tables_source)\n",
                "    else:\n",
                "        temp_items = set(oea.get_folders(tables_source))\n",
                "        common_items = list(temp_items.intersection(items))\n",
                "        items = [item for item in items if item in common_items]\n",
                "    for item in items:\n",
                "        edfi_items.append(sap_to_edfi_complex.get(item))\n",
                "    return edfi_items"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Entities To Delete\n",
                "1. Return Only Submission Type and Period Specific Entities \n",
                "2. Return Only Entities to Delete from the above ones"
            ],
            "outputs": []
        },
        {
            "execution_count": 62,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "items = return_sap_entities(sap_pipeline, sap_pipelineType)\n",
                "if sap_pipeline != 'analytics':\n",
                "    entities_to_delete = return_entities_to_delete(tables_source = f'stage1/Transactional/SAP/pipeline={sap_pipeline}/pipelineType={sap_pipelineType}/{sapVersion}/DistrictId={districtId}/SchoolYear={schoolYear}', \n",
                "                                                   items = items)\n",
                "else:\n",
                "    entities_to_delete = list()"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Main Code"
            ],
            "outputs": []
        },
        {
            "execution_count": 64,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "test_mode = False\n",
                "testEntity = 'contractedInstructionalStaffFTEExts'\n",
                "\n",
                "edfiSubmissionProcessor = EdfiSubmissionProcessor(oea = oea, \n",
                "                                                  edfiAPIClient = edfiAPIClient, \n",
                "                                                  error_logger = error_logger, \n",
                "                                                  logger = logger, \n",
                "                                                  spark = spark, \n",
                "                                                  pipelineExecutionId = pipelineExecutionId, \n",
                "                                                  test_mode = False)"
            ],
            "outputs": []
        },
        {
            "execution_count": 69,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "tables_with_dependencies = {'staffs': ['payrollExts', \n",
                "                                       'staffEducationOrganizationEmploymentAssociations',\n",
                "                                       'staffEducationOrganizationAssignmentAssociations']}\n",
                "\n",
                "query = f\"\"\"SELECT edfi_id, \n",
                "                   edfi_id_modified,\n",
                "                   edfi_location,\n",
                "                   entity_name,\n",
                "                   staffUniqueId\n",
                "                FROM ldb_{workspace}_sap_etl_logs.edfi_ids_lookup_table\n",
                "                WHERE isDeleted = False\n",
                "            \"\"\"\n",
                "entity_ids_df = spark.sql(query).cache()\n",
                "entity_ids_df.createOrReplaceTempView('temp_edfi_ids_lookup_table')\n",
                "\n",
                "for entity_name in entities_to_delete:\n",
                "    # TODO: Review or optimize?\n",
                "    if entity_name.lower().endswith('exts'):\n",
                "        ext_type = 'tx'\n",
                "    else:\n",
                "        ext_type = 'ed-fi'\n",
                "    stage3_path = f'stage3/pipeline={sap_pipeline}/pipelineType={sap_pipelineType}/{apiVersion}/general/{ext_type}/{entity_name}'\n",
                "    \n",
                "    # NOTE: FETCH RECORDS BY SY AND DI\n",
                "    df = edfiAPIClient.load_by_SY_DI(stage3_path)\n",
                "    submission_type = edfiAPIClient.return_submission_type(df)\n",
                "    \n",
                "    # NOTE: FETCH THE LATEST RECORDS FROM STAGE 3 I.E. HAVING RUNDATE >= LASTSUBMISSIONDATE\n",
                "    df = edfiAPIClient.get_latest_submission_records(df = df, \n",
                "                                                    lookup_table_name = edfiAPIClient.lookup_table_name,#'submissions_lookup_table', \n",
                "                                                    filtering_date = 'rundate',\n",
                "                                                    resource_name = entity_name,\n",
                "                                                    sap_pipeline = sap_pipeline,\n",
                "                                                    sap_pipelineType = sap_pipelineType,\n",
                "                                                    operationType = 'delete',\n",
                "                                                    debugMode = False)\n",
                "    df = df.filter(df['SUBMISSION_RECORD_IS_ACTIVE'] == False)\n",
                "    if entity_name == 'staffs':\n",
                "        df = df.select('NATURAL_KEY_HASH', 'lakeId', 'edfi_id', 'edfi_id_modified', 'staffUniqueId', 'edfi_location')\n",
                "    else:\n",
                "        df = df.select('NATURAL_KEY_HASH', 'lakeId', 'edfi_id', 'edfi_id_modified', 'edfi_location')\n",
                "    \n",
                "    # NOTE: Temp Session Views\n",
                "    df.createOrReplaceTempView('temp_vw_recent_df')\n",
                "    df.createOrReplaceTempView('temp_vw_record_deletes')\n",
                "    \n",
                "    # TODO: Assess if the logging output is semantically correct or not\n",
                "    logger.info(f'{entity_name} - NUMBER OF RECORDS AS INCOMING RECORDS FOR DELETES    : {df.count()}')\n",
                "    if entity_name in tables_with_dependencies.keys():\n",
                "        # FIXME: 2024-03-15: Need to check the side effects of v0.6 delete codes on dependent entities\n",
                "        logger.info(f'Removing the dependent entities')\n",
                "        for dependent_entity_name in tables_with_dependencies[entity_name]:\n",
                "            if entity_name == 'staffs':\n",
                "                temp_df = spark.sql(f\"\"\"SELECT temp_edfi_ids_lookup_table.edfi_id,\n",
                "                                                     temp_edfi_ids_lookup_table.edfi_location\n",
                "                                FROM temp_edfi_ids_lookup_table\n",
                "                                INNER JOIN temp_vw_recent_df\n",
                "                                    ON temp_edfi_ids_lookup_table.staffUniqueId = temp_vw_recent_df.staffUniqueId\n",
                "                                WHERE LOWER(entity_name) = '{dependent_entity_name.lower()}'\n",
                "                                \"\"\")\n",
                "                temp_df.createOrReplaceTempView('temp_vw_dependent_record_deletes')\n",
                "                logger.info(f'{dependent_entity_name} - NUMBER OF RECORDS TO BE DELETED: {temp_df.count()}')\n",
                "            temp_records = temp_df.toJSON().map(lambda x: json.loads(x)).collect()\n",
                "            ext_type = 'TX' if dependent_entity_name.lower().endswith('exts') else 'ed-fi'\n",
                "            edfiAPIClient.upsert_records(pipeline_execution_id = pipelineExecutionId,\n",
                "                                         run_id = run_id,\n",
                "                                         resource = f\"/{ext_type}/{dependent_entity_name}\", \n",
                "                                         resource_name = dependent_entity_name, \n",
                "                                         records = temp_records, \n",
                "                                         chunk_size = 500, \n",
                "                                         num_threads = 10, \n",
                "                                         function_name = 'delete',\n",
                "                                         success_logging = True, \n",
                "                                         error_logging = True)\n",
                "    # NOTE: FILTER OUT THE RECORDS THAT HAVE ALREADY BEEN DELETED\n",
                "    temp_df = spark.sql(f\"\"\"SELECT DISTINCT temp_vw_record_deletes.edfi_id,\n",
                "                                            temp_vw_record_deletes.edfi_location\n",
                "                            FROM temp_vw_record_deletes \n",
                "                            INNER JOIN temp_edfi_ids_lookup_table\n",
                "                                ON temp_vw_record_deletes.edfi_id = temp_edfi_ids_lookup_table.edfi_id\n",
                "                               AND temp_vw_record_deletes.edfi_location = temp_edfi_ids_lookup_table.edfi_location\n",
                "                        \"\"\")\n",
                "    temp_df.createOrReplaceTempView('temp_vw_record_deletes')\n",
                "    logger.info(f'{entity_name} - NUMBER OF RECORDS TO BE DELETED: {temp_df.count()}')\n",
                "    temp_records = temp_df.toJSON().map(lambda x: json.loads(x)).collect()\n",
                "    ext_type = 'TX' if entity_name.lower().endswith('exts') else 'ed-fi'\n",
                "    edfiAPIClient.upsert_records(pipeline_execution_id = pipelineExecutionId,\n",
                "                                     run_id = run_id,\n",
                "                                     resource = f\"/{ext_type}/{entity_name}\", \n",
                "                                     resource_name = entity_name, \n",
                "                                     records = temp_records, \n",
                "                                     chunk_size = 500, \n",
                "                                     num_threads = 10, \n",
                "                                     function_name = 'delete',\n",
                "                                     success_logging = True, \n",
                "                                     error_logging = True)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Ed-Fi IDs - Lookup Table"
            ],
            "outputs": []
        },
        {
            "execution_count": 31,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "edfi_ids_lookup_table_path = 'stage1/Transactional/SAP/metadata-assets/edfi_ids_lookup_table'\n",
                "if edfiAPIClient.edfi_id_records != [] and sap_pipeline != 'analytics':\n",
                "    edfi_id_records_df = spark.createDataFrame(edfiAPIClient.edfi_id_records, schema = edfiAPIClient.edfi_id_record_schema) if len(edfiAPIClient.edfi_id_records) != 0 else spark.createDataFrame([], schema = edfiAPIClient.edfi_id_record_schema)\n",
                "    primary_key = ['edfi_id', 'resource']\n",
                "    update_cols = {\"sink.isDeleted\": \"updates.isDeleted\"}\n",
                "\n",
                "    destination_url = oea.to_url(edfi_ids_lookup_table_path)\n",
                "    pk_statement = oea.return_pk_statement(primary_key)\n",
                "    if DeltaTable.isDeltaTable(spark, destination_url):\n",
                "        logger.info('TRUE UPSERT')\n",
                "        delta_table_sink = DeltaTable.forPath(spark, destination_url)\n",
                "        delta_table_sink.alias('sink').merge(edfi_id_records_df.alias('updates'), pk_statement).whenMatchedUpdate(set = update_cols).execute()# .whenNotMatchedInsert(values = insert_cols).execute()\n",
                "    else:\n",
                "        logger.error(f'Invalid Path - {edfi_ids_lookup_table_path}')\n",
                "\n",
                "    spark.sql(f'CREATE DATABASE IF NOT EXISTS ldb_{workspace}_sap_etl_logs')\n",
                "    spark.sql(f\"drop table if exists ldb_{workspace}_sap_etl_logs.edfi_ids_lookup_table\")\n",
                "    spark.sql(f\"create table if not exists ldb_{workspace}_sap_etl_logs.edfi_ids_lookup_table using DELTA location '{oea.to_url(edfi_ids_lookup_table_path)}'\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Submissions - Lookup Table"
            ],
            "outputs": []
        },
        {
            "execution_count": 32,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "if edfiAPIClient.max_rundates != []:\n",
                "    edfiAPIClient.dump_lookup_table()\n",
                "    edfiAPIClient.add_lookup_table_to_lake_db(overwrite = True)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Error Logging"
            ],
            "outputs": []
        },
        {
            "execution_count": 33,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "if sap_pipeline != 'analytics':\n",
                "    error_df = spark.createDataFrame(edfiAPIClient.error_logs, schema = edfiAPIClient.log_schema) if len(edfiAPIClient.error_logs) != 0 else spark.createDataFrame([], schema = edfiAPIClient.log_schema)\n",
                "    success_df = spark.createDataFrame(edfiAPIClient.success_logs, schema = edfiAPIClient.log_schema) if len(edfiAPIClient.success_logs) != 0 else spark.createDataFrame([], schema = edfiAPIClient.log_schema)\n",
                "\n",
                "    current_datetime = datetime.today()\n",
                "    run_date = current_datetime.strftime(\"%Y-%m-%d\")\n",
                "    test_mode = False"
            ],
            "outputs": []
        },
        {
            "execution_count": 37,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "if sap_pipeline != 'analytics':\n",
                "    # NOTE: If all is passed all will be the submission_type\n",
                "    success_df = edfiSubmissionProcessor.process_logs(df = success_df, \n",
                "                                sap_pipeline = sap_pipeline,\n",
                "                                sap_pipelineType =  sap_pipelineType,\n",
                "                                is_post_success = True, \n",
                "                                run_date = run_date)\n",
                "    # NOTE: If all is passed all will be the submission_type\n",
                "    error_df = edfiSubmissionProcessor.process_logs(df = error_df, \n",
                "                                sap_pipeline = sap_pipeline,\n",
                "                                sap_pipelineType = sap_pipelineType,\n",
                "                                is_post_success = False, \n",
                "                                run_date = run_date)\n",
                "    logs_df = success_df.union(error_df)"
            ],
            "outputs": []
        },
        {
            "execution_count": 38,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "if sap_pipeline != 'analytics':\n",
                "   log_file_url = error_logger.to_logs_url('etl-logs/log_type=submissions')\n",
                "   df_delta, _ = edfiSubmissionProcessor.write_logs(df = logs_df, \n",
                "                           log_file_url = log_file_url)\n",
                "   edfiSubmissionProcessor.error_logger.add_etl_logs_to_lake_db(db_name = f'ldb_{workspace}_sap_etl_logs',\n",
                "                                       logs_base_path = 'etl-logs',\n",
                "                                       log_type = 'submissions',\n",
                "                                       overwrite = True)"
            ],
            "outputs": []
        },
        {
            "execution_count": 44,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# TODO: Create entity level logs for deletes as well\n",
                "# TODO: Test this with some actual data\n",
                "# NOTE: Current implementation assumes that numInputRows is for only those rows that go to the API\n",
                "#       That is, if the code filters out records before delete api calls => they are NOT counted\n",
                "#       as part of input rows\n",
                "from pyspark.sql import Window\n",
                "if logs_df.count() >= 0:\n",
                "    logs_df.createOrReplaceTempView('temp_df_logs_df')\n",
                "    query = f\"\"\"\n",
                "            WITH entity_logs AS (\n",
                "                SELECT * FROM temp_df_logs_df\n",
                "                WHERE operation_type = 'delete'\n",
                "            )\n",
                "            SELECT  max(end_time) as end_time,\n",
                "                    entityName as entityName,\n",
                "                    entityType as entityType,\n",
                "                    sap_pipeline as sap_pipeline,\n",
                "                    sap_pipelineType as sap_pipelineType,\n",
                "                    'entity' as log_type,\n",
                "                    count(*) as numInputRows,\n",
                "                    count(CASE WHEN response_status_code like '204' THEN 1 END) AS numRecordsDeleted,\n",
                "                    0 as numRecordsSkipped,\n",
                "                    0 as numTargetRowsInserted,\n",
                "                    0 as numTargetRowsUpdated,\n",
                "                    pipelineExecutionId,\n",
                "                    'ed-fi' as schemaFormat,\n",
                "                    '{spark.sparkContext.applicationId}' as sparkSessionId,\n",
                "                    'Deletion' as stageName,\n",
                "                    min(start_time) as start_time,\n",
                "                    0 as totalNumOutputRows,\n",
                "                    'NA' as insertionType,\n",
                "                    False as emptySchemaMetadata\n",
                "            FROM entity_logs\n",
                "            GROUP BY entityName,\n",
                "                    entityType,\n",
                "                    sap_pipeline,\n",
                "                    sap_pipelineType,\n",
                "                    pipelineExecutionId\n",
                "    \"\"\"\n",
                "    entity_logs = spark.sql(query)\n",
                "    entity_logs = entity_logs.withColumn(\"row_idx\", F.row_number().over(Window.orderBy(F.monotonically_increasing_id())))\n",
                "\n",
                "    temp_uniqueIDs = [{'uniqueId': error_logger.generate_random_alphanumeric(10)} for i in range(entity_logs.count())]\n",
                "    temp_uniqueIDsDF = spark.createDataFrame(data = temp_uniqueIDs, \n",
                "                                             schema = StructType([\n",
                "                                                                StructField(\"uniqueId\", StringType(), True)\n",
                "                                                                ]))\n",
                "    temp_uniqueIDsDF = temp_uniqueIDsDF.withColumn(\"row_idx\", f.row_number().over(Window.orderBy(F.monotonically_increasing_id())))\n",
                "    entity_logs = entity_logs.join(temp_uniqueIDsDF, entity_logs.row_idx == temp_uniqueIDsDF.row_idx).drop(\"row_idx\")\n",
                "else:\n",
                "    # TODO: Log for zero records being posted but how?\n",
                "    pass\n",
                "error_logger.write_logs_to_delta_lake(df = entity_logs, \n",
                "                                      log_type = 'entity',\n",
                "                                      destination_url = error_logger.to_logs_url('etl-logs/log_type=entity'))\n",
                "error_logger.add_etl_logs_to_lake_db(db_name = f'ldb_{workspace}_sap_etl_logs',\n",
                "                                     logs_base_path = 'etl-logs',\n",
                "                                     log_type = 'entity',\n",
                "                                     overwrite = False)"
            ],
            "outputs": []
        }
    ]
}