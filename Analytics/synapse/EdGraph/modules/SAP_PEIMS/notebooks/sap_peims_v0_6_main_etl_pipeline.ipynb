{
    "nbformat": 4,
    "nbformat_minor": 2,
    "metadata": {
        "save_output": true,
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "execution_count": 38,
            "cell_type": "code",
            "source": [
                "from notebookutils import mssparkutils\r\n",
                "from datetime import datetime"
            ],
            "outputs": []
        },
        {
            "execution_count": 39,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "start_time = datetime.now()"
            ],
            "outputs": []
        },
        {
            "execution_count": 40,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "instance = InstanceId = instanceId\r\n",
                "ApiUrl = apiUrl\r\n",
                "SchoolYear = schoolYear\r\n",
                "DistrictId = DistrictID = districtID = districtId\r\n",
                "apiLimit = batchLimit\r\n",
                "\r\n",
                "prepareSAPMetaData = prepareSAPMetadata\r\n",
                "zone = submissionsType = sap_pipeline\r\n",
                "submissionPeriod = sap_pipelineType\r\n",
                "\r\n",
                "etl_processing = etlProcessing #= True"
            ],
            "outputs": []
        },
        {
            "execution_count": 41,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "import random\r\n",
                "import string\r\n",
                "\r\n",
                "characters = string.ascii_letters + string.digits\r\n",
                "ten_digit_alphanumeric = ''.join(random.choice(characters) for _ in range(10))\r\n",
                "\r\n",
                "pipelineExecutionId = executionId = f'TEST_{ten_digit_alphanumeric}'\r\n",
                "pipelineExecutionId"
            ],
            "outputs": []
        },
        {
            "execution_count": 43,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# FIXME: Do not pass uncessary params\r\n",
                "input_params = {\r\n",
                "        \"kvName\": kvName,\r\n",
                "        \"kVName\": kvName,\r\n",
                "        \"workspace\": workspace,\r\n",
                "        \"apiUrl\": apiUrl,\r\n",
                "        \"ApiUrl\": apiUrl,\r\n",
                "        \"instanceId\": instanceId,\r\n",
                "        \"InstanceId\": instanceId,\r\n",
                "        \"moduleName\": moduleName,\r\n",
                "        \"apiLimit\": apiLimit,\r\n",
                "        \"minChangeVer\": minChangeVer,\r\n",
                "        \"maxChangeVer\": maxChangeVer,\r\n",
                "        \"sapVersion\": sapVersion,\r\n",
                "        \"prepareSAPMetaData\": prepareSAPMetaData,\r\n",
                "        \"prepareSAPMetadata\": prepareSAPMetaData,\r\n",
                "        \"submissions\": submissions,\r\n",
                "        \"submissionsType\": submissionsType,\r\n",
                "        \"schoolYear\": schoolYear,\r\n",
                "        \"SchoolYear\": schoolYear,\r\n",
                "        \"districtId\": districtId,\r\n",
                "        \"districtID\": districtId,\r\n",
                "        \"DistrictId\": districtId,\r\n",
                "        \"edfi_url\": apiUrl,\r\n",
                "        \"pipelineExecutionId\" : pipelineExecutionId,\r\n",
                "        \"batchLimit\": apiLimit,\r\n",
                "        \"etlProcessing\": etlProcessing,\r\n",
                "        \"submissionPeriod\": submissionPeriod,\r\n",
                "        \"kvSecret_clientId\": kvSecret_clientId,\r\n",
                "        \"kvSecret_clientSecret\": kvSecret_clientSecret,\r\n",
                "        \"deletePrevSubmissions\": deletePrevSubmissions,\r\n",
                "\r\n",
                "        \"sap_pipeline\": sap_pipeline,\r\n",
                "        \"sap_pipelineType\": sap_pipelineType,\r\n",
                "        \"ingestionHistoryMode\": ingestionHistoryMode,\r\n",
                "        \"landingDateTimeFormat\": landingDateTimeFormat \r\n",
                "\r\n",
                "    }"
            ],
            "outputs": []
        },
        {
            "execution_count": 1,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "print(input_params)"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "%run EdGraph/modules/SAP_PEIMS/v0.6/src/utilities/sap_peim_v0_6_sap_py"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from datetime import datetime\r\n",
                "oea = EdFiOEAChild()   "
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "oea.set_workspace(workspace)"
            ],
            "outputs": []
        },
        {
            "execution_count": 48,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "def execute_etl_step(condition,\r\n",
                "                     nb_path,\r\n",
                "                     nb_timeout,\r\n",
                "                     nb_params,\r\n",
                "                     nb_error,\r\n",
                "                     etl_stage):\r\n",
                "    global etl_status\r\n",
                "    # FIXME: Parameterize etl_status \r\n",
                "    stage_start_time = datetime.now()\r\n",
                "    if nb_error is None:\r\n",
                "        etl_status = f\"{etl_stage} - RUNNING\"\r\n",
                "        try:\r\n",
                "            if condition:\r\n",
                "                mssparkutils.notebook.run(path = nb_path,\r\n",
                "                                          timeout_seconds = nb_timeout,\r\n",
                "                                          arguments = nb_params)\r\n",
                "                etl_status = f\"{etl_stage} - SUCCEEDED\"\r\n",
                "            else:\r\n",
                "                etl_status = f\"{etl_stage} - SKIPPED\"\r\n",
                "        except Exception as error_msg:\r\n",
                "            etl_status = f\"{etl_stage} - FAILED\"\r\n",
                "            logger.info(f\"{error_msg}\")\r\n",
                "            nb_error = error_msg\r\n",
                "    else:\r\n",
                "        pass\r\n",
                "    \r\n",
                "    stage_end_time = datetime.now()\r\n",
                "    log_data = pipeline_error_logger.create_log_dict(uniqueId = pipeline_error_logger.generate_random_alphanumeric(10), # Generate a random 10-character alphanumeric value\r\n",
                "                                            pipelineExecutionId = pipelineExecutionId,#'TEST_1234',#executionId,\r\n",
                "                                            sparkSessionId = spark.sparkContext.applicationId,\r\n",
                "                                            stageName = etl_stage,\r\n",
                "                                            start_time = stage_start_time,\r\n",
                "                                            end_time = stage_end_time,\r\n",
                "                                            run_status = 'SUCCEEDED' if nb_error is None else 'FAILED',\r\n",
                "                                            etl_status = etl_status,\r\n",
                "                                            error_description = str(nb_error))\r\n",
                "    pipeline_error_logger.consolidate_logs(log_data,'stage')\r\n",
                "    if ' - SUCCEEDED' in etl_status or ' - SKIPPED' in etl_status:\r\n",
                "        return etl_status, None\r\n",
                "    else:\r\n",
                "        return etl_status, nb_error\r\n",
                "\r\n",
                "def get_entity_logs_status(df, executionId, stageName):\r\n",
                "    df = df.filter((F.col('pipelineExecutionId') == executionId) & (F.col('stageName') == stageName))\r\n",
                "    df = df.withColumn('numRecordsFailed', F.col('totalNumOutputRows') - F.col('numInputRows'))\r\n",
                "\r\n",
                "    if df.filter(F.col(\"numRecordsFailed\") == 0).count() == df.count():\r\n",
                "        return 'success'\r\n",
                "    \r\n",
                "    if df.filter(F.col(\"numRecordsFailed\") != 0).count() == df.count():\r\n",
                "        return 'failure'\r\n",
                "    \r\n",
                "    if df.agg(F.sum(\"numRecordsFailed\")).collect()[0][0] != 0:\r\n",
                "        return 'partial-success'\r\n",
                "    \r\n",
                "    return 'unknown'"
            ],
            "outputs": []
        },
        {
            "execution_count": 49,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "nb_root = \"EdGraph/modules/SAP_PEIMS/v0.6/src\"\r\n",
                "nb_version = \"sap_peim_v0_6\"\r\n",
                "nb_error = None\r\n",
                "\r\n",
                "run_status = \"INITIATED\"\r\n",
                "etl_status = \"\""
            ],
            "outputs": []
        },
        {
            "execution_count": 50,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "pipeline_error_logger = ErrorLogging(spark = spark, \r\n",
                "                                     oea = oea, \r\n",
                "                                     logger = logger)\r\n",
                "\r\n",
                "try:\r\n",
                "    run_status = \"RUNNING\"\r\n",
                "    # NOTE: Prepare SAP Meta Data \r\n",
                "    etl_status, nb_error = execute_etl_step(condition = prepareSAPMetaData,\r\n",
                "                                            nb_path = f\"{nb_root}/utilities/{nb_version}_descriptors_dumping\",\r\n",
                "                                            nb_timeout = 1800,\r\n",
                "                                            nb_params = input_params,\r\n",
                "                                            nb_error = nb_error,\r\n",
                "                                            etl_stage = \"Metadata-Preparation\")\r\n",
                "    # NOTE: Ingestion\r\n",
                "    etl_status, nb_error = execute_etl_step(condition = etl_processing,\r\n",
                "                                            nb_path = f\"{nb_root}/main/{nb_version}_ingest\",\r\n",
                "                                            nb_timeout = 900,\r\n",
                "                                            nb_params = input_params,\r\n",
                "                                            nb_error = nb_error,\r\n",
                "                                            etl_stage = \"Ingestion\")\r\n",
                "    # NOTE: Refinement\r\n",
                "    etl_status, nb_error = execute_etl_step(condition = etl_processing or prepareSAPMetaData,\r\n",
                "                                            nb_path = f\"{nb_root}/main/{nb_version}_refine\",\r\n",
                "                                            nb_timeout = 7200 if prepareSAPMetaData else 1800,\r\n",
                "                                            nb_params = input_params,\r\n",
                "                                            nb_error = nb_error,\r\n",
                "                                            etl_stage = \"Refinement\")\r\n",
                "    \r\n",
                "    # NOTE: Move to Stage 3 (Submissions)\r\n",
                "    etl_status, nb_error = execute_etl_step(condition = submissions,\r\n",
                "                                            nb_path = f\"{nb_root}/main/{nb_version}_migrate_to_s3\",\r\n",
                "                                            nb_timeout = 900,\r\n",
                "                                            nb_params = input_params,\r\n",
                "                                            nb_error = nb_error,\r\n",
                "                                            etl_stage = \"Stage 3 Transfer (Submission)\")\r\n",
                "\r\n",
                "    # NOTE: Data Deletions\r\n",
                "    etl_status, nb_error = execute_etl_step(condition = deletePrevSubmissions,\r\n",
                "                                            nb_path = f\"{nb_root}/main/{nb_version}_delete_from_edfi\",\r\n",
                "                                            nb_timeout = 2500,\r\n",
                "                                            nb_params = input_params,\r\n",
                "                                            nb_error = nb_error,\r\n",
                "                                            etl_stage = \"API-Deletion\")\r\n",
                "\r\n",
                "    # NOTE: Data Validation\r\n",
                "    etl_status, nb_error = execute_etl_step(condition = validation,\r\n",
                "                                            nb_path = f\"{nb_root}/main/{nb_version}_create_views_for_validations\",\r\n",
                "                                            nb_timeout = 2500,\r\n",
                "                                            nb_params = input_params,\r\n",
                "                                            nb_error = nb_error,\r\n",
                "                                            etl_stage = \"Submission\")\r\n",
                "\r\n",
                "    # NOTE: Data Submissions\r\n",
                "    etl_status, nb_error = execute_etl_step(condition = submissions,\r\n",
                "                                            nb_path = f\"{nb_root}/main/{nb_version}_submit_to_edfi\",\r\n",
                "                                            nb_timeout = 3600,\r\n",
                "                                            nb_params = input_params,\r\n",
                "                                            nb_error = nb_error,\r\n",
                "                                            etl_stage = \"Submission\")\r\n",
                "    if nb_error is not None:\r\n",
                "        raise Exception(f\"{nb_error}\")\r\n",
                "    run_status = \"SUCCEEDED\"\r\n",
                "except Exception as e:\r\n",
                "    run_status = \"FAILED\"\r\n",
                "    print(f'Pipeline Executed with Error {e}')"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Pipeline Level Logs"
            ],
            "outputs": []
        },
        {
            "execution_count": 51,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "log_type = 'pipeline'\r\n",
                "\r\n",
                "end_time = datetime.now()\r\n",
                "log_data = pipeline_error_logger.create_log_dict(uniqueId = pipeline_error_logger.generate_random_alphanumeric(10), # Generate a random 10-character alphanumeric value\r\n",
                "                                                pipelineExecutionId = pipelineExecutionId,#'TEST_1234',#executionId,\r\n",
                "                                                sparkSessionId = spark.sparkContext.applicationId,\r\n",
                "                                                start_time = start_time,\r\n",
                "                                                end_time = end_time,\r\n",
                "                                                run_status = run_status,\r\n",
                "                                                etl_status = str(etl_status))\r\n",
                "pipeline_error_logger.consolidate_logs(log_data,'pipeline')\r\n",
                "df = pipeline_error_logger.create_spark_df('pipeline')\r\n",
                "\r\n",
                "\r\n",
                "pipeline_error_logger.write_logs_to_delta_lake(df = df, \r\n",
                "                             log_type = 'pipeline',\r\n",
                "                             destination_url = pipeline_error_logger.to_logs_url('etl-logs/log_type=pipeline'))\r\n",
                "pipeline_error_logger.add_etl_logs_to_lake_db(db_name = f'ldb_{workspace}_sap_etl_logs',\r\n",
                "                                     logs_base_path = 'etl-logs',\r\n",
                "                                     log_type = 'pipeline',\r\n",
                "                                     overwrite = True)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Stage Level Logs"
            ],
            "outputs": []
        },
        {
            "execution_count": 52,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "log_type = 'entity'\r\n",
                "entity_logs_url = pipeline_error_logger.to_logs_url('etl-logs/log_type=entity')\r\n",
                "entity_logs_df = oea.load(entity_logs_url)"
            ],
            "outputs": []
        },
        {
            "execution_count": 53,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "df = pipeline_error_logger.create_spark_df('stage')\r\n",
                "df = df.withColumn('stage_status', F.lit('Unknown'))\r\n",
                "\r\n",
                "log_type = 'stage'\r\n",
                "destination_url = pipeline_error_logger.to_logs_url('etl-logs/log_type=stage')\r\n",
                "\r\n",
                "for stageName in ['Ingestion', 'Refinement']:\r\n",
                "    stage_status = get_entity_logs_status(df = entity_logs_df, \r\n",
                "                                          executionId = pipelineExecutionId, \r\n",
                "                                          stageName = stageName)\r\n",
                "                    \r\n",
                "    df = df.withColumn('stage_status', \r\n",
                "        F.when((F.col('stageName') == stageName) & (F.col('pipelineExecutionId') == executionId), stage_status)\r\n",
                "        .otherwise(F.col('stage_status'))\r\n",
                "    )\r\n",
                "\r\n",
                "pipeline_error_logger.write_logs_to_delta_lake(df = df, \r\n",
                "                                               log_type = log_type,\r\n",
                "                                               destination_url = destination_url)\r\n",
                "pipeline_error_logger.add_etl_logs_to_lake_db(db_name = f'ldb_{workspace}_sap_etl_logs',\r\n",
                "                                     logs_base_path = 'etl-logs',\r\n",
                "                                     log_type = 'stage',\r\n",
                "                                     overwrite = True)"
            ],
            "outputs": []
        }
    ]
}