{
    "nbformat": 4,
    "nbformat_minor": 2,
    "metadata": {
        "save_output": true,
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "execution_count": 39,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "instance = instanceId = InstanceId\r\n",
                "apiUrl = ApiUrl\r\n",
                "schoolYear = SchoolYear\r\n",
                "DistrictId = DistrictID = districtId = districtID\r\n",
                "apiLimit = batchLimit\r\n",
                "\r\n",
                "prepareSAPMetaData = prepareSAPMetadata\r\n",
                "zone = submissionsType = sap_pipeline"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "azdata_cell_guid": "fbd392d9-afa3-4158-8555-1a87898307f4",
                "language": "python"
            },
            "source": [
                "import pyspark\r\n",
                "from pyspark.sql import SparkSession\r\n",
                "from pyspark.sql.utils import AnalysisException\r\n",
                "\r\n",
                "from pyspark.sql.types import StructField, StructType\r\n",
                "from pyspark.sql.functions import col, expr\r\n",
                "\r\n",
                "from pyspark.sql.functions import col\r\n",
                "from pyspark.sql.functions import col, substring, regexp_extract, split, lit, lower\r\n",
                "\r\n",
                "import pyspark\r\n",
                "from pyspark.sql import SparkSession\r\n",
                "from pyspark.sql.utils import AnalysisException\r\n",
                "from pyspark.sql.functions import input_file_name, lit, expr"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "print('Stage 3 Submission - TESTING PARAMETERIZATION')\r\n",
                "try:\r\n",
                "    print(kVName)\r\n",
                "    print(workspace)\r\n",
                "    print(apiUrl)\r\n",
                "    print(instanceId)\r\n",
                "    print(moduleName)\r\n",
                "    print(apiLimit)\r\n",
                "    print(minChangeVer)\r\n",
                "    print(maxChangeVer)\r\n",
                "    print(sapVersion)\r\n",
                "    print(prepareSAPMetaData)\r\n",
                "    print(submissions)\r\n",
                "    print(submissionsType)\r\n",
                "    print(schoolYear)\r\n",
                "    print(districtID)\r\n",
                "    print(pipelineExecutionId)\r\n",
                "\r\n",
                "    kvName = kVName\r\n",
                "    districtId = districtID\r\n",
                "    districtPath = districtId\r\n",
                "    \r\n",
                "except Exception as params_error:\r\n",
                "    print('CATCHING ERROR!!!')\r\n",
                "    print(params_error)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### URL Initializations"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "141e868b-7606-49a2-8cbe-3c614a224622"
            },
            "source": [
                "%run OEA/modules/Ed-Fi/v0.7/src/utilities/edfi_v0_7_fetch_urls"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "instance_id = instanceId\r\n",
                "school_year = schoolYear\r\n",
                "api_year = school_year\r\n",
                "api_url = apiUrl\r\n",
                "\r\n",
                "# FIXME: 2024-01-31 TEMP FIX FOR FY\r\n",
                "try:\r\n",
                "    edfi_api_manager = EdFiApiManager(api_url, instance_id, api_year)\r\n",
                "    edfi_api_manager.update_urls()\r\n",
                "    edfi_api_manager.set_other_metadata()\r\n",
                "\r\n",
                "    dependenciesUrl = edfi_api_manager.dependencies_url\r\n",
                "    openApiMetadataUrl = edfi_api_manager.openapi_metadata_url\r\n",
                "    dataManagementUrl = edfi_api_manager.data_management_url\r\n",
                "    authUrl = edfi_api_manager.auth_url\r\n",
                "\r\n",
                "    changeQueriesUrl = edfi_api_manager.get_referenced_url('Change-Queries')\r\n",
                "    changeQueriesUrl = changeQueriesUrl[:-13].replace('/metadata/', '/')\r\n",
                "    swagger_url = swaggerUrl = edfi_api_manager.get_referenced_url('Resources')\r\n",
                "\r\n",
                "    apiVersion = edfi_api_manager.api_version\r\n",
                "    apiVersion = apiVersion[1:] if apiVersion.startswith('v') else apiVersion\r\n",
                "except Exception as error:\r\n",
                "    edfi_api_manager = EdFiApiManager(api_url, instance_id, '')\r\n",
                "    edfi_api_manager.update_urls()\r\n",
                "    edfi_api_manager.set_other_metadata()\r\n",
                "\r\n",
                "    dependenciesUrl = edfi_api_manager.dependencies_url\r\n",
                "    openApiMetadataUrl = edfi_api_manager.openapi_metadata_url\r\n",
                "    dataManagementUrl = edfi_api_manager.data_management_url\r\n",
                "    authUrl = edfi_api_manager.auth_url\r\n",
                "\r\n",
                "    changeQueriesUrl = edfi_api_manager.get_referenced_url('Change-Queries')\r\n",
                "    changeQueriesUrl = changeQueriesUrl[:-13].replace('/metadata/', '/')\r\n",
                "    swagger_url = swaggerUrl = edfi_api_manager.get_referenced_url('Resources')\r\n",
                "\r\n",
                "    apiVersion = edfi_api_manager.api_version\r\n",
                "    apiVersion = apiVersion[1:] if apiVersion.startswith('v') else apiVersion"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### OEA Initializations"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "%run EdGraph/modules/SAP_PEIMS/v0.6/src/utilities/sap_peim_v0_6_sap_py"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from datetime import datetime\r\n",
                "oea = SAPEdFiOEAChild()   \r\n",
                "oea.set_workspace(workspace)"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "oea_utils = schema_gen = SAPOpenAPIUtilChild(swagger_url)\r\n",
                "oea_utils.create_definitions()\r\n",
                "schemas = schema_gen.create_spark_schemas()\r\n",
                "\r\n",
                "primitive_datatypes = ['timestamp', 'date', 'decimal', 'boolean', 'integer', 'string', 'long']"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Metadata For Processing"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "metadata_path = \"stage1/Transactional/SAP/metadata-assets/sap-to-edfi.json\"\r\n",
                "metadata_url = oea.to_url(metadata_path)"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "jsonDF = spark.read.option(\"multiline\", \"true\").json(metadata_url).cache()\r\n",
                "\r\n",
                "json_string = jsonDF.toJSON().collect()[0]\r\n",
                "config_data = json.loads(json_string)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### SAP & Error Logging Initializations"
            ],
            "outputs": []
        },
        {
            "execution_count": 25,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "error_logger = ErrorLogging(spark = spark, \r\n",
                "                            oea = oea, \r\n",
                "                            logger = logger)"
            ],
            "outputs": []
        },
        {
            "execution_count": 26,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "sap_utilities = SAPUtilities(spark = spark, \r\n",
                "                             oea = oea)"
            ],
            "outputs": []
        },
        {
            "execution_count": 27,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# sap_pipeline = \"peims-submissions\"\r\n",
                "\r\n",
                "sap_to_edfi_complex = config_data.get('sap_to_edfi_complex', {})\r\n",
                "final_columns = config_data.get('final_columns', {})\r\n",
                "_ext_TX_cols = config_data.get('_ext_TX_cols', {})\r\n",
                "descriptorsDFRef = config_data.get('descriptorsDFRef', {})\r\n",
                "descriptors = config_data.get('descriptors', [])\r\n",
                "\r\n",
                "edfi_to_sap_complex = {value: key for key, value in sap_to_edfi_complex.items()}"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Main Code"
            ],
            "outputs": []
        },
        {
            "execution_count": 34,
            "cell_type": "code",
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "6ba4d88f-27ea-47ea-850b-84d97c128d76"
            },
            "source": [
                "def add_metadata_columns(df, **kwargs):\r\n",
                "    for column_name, constant_value in kwargs.items():\r\n",
                "        if column_name not in df.columns:\r\n",
                "            df = df.withColumn(column_name, F.lit(constant_value))\r\n",
                "    return df\r\n",
                "\r\n",
                "def get_latest_s2r_changes(source_path, sink_path, filtering_date = 'rundate',debugMode = False, sap_pipeline = 'PLACEHOLDER', sap_pipelineType = 'PLACEHOLDER', districtId = '101912', schoolYear = 'PLACEHOLDER'):\r\n",
                "        maxdatetime = None\r\n",
                "        try:\r\n",
                "            sink_df = oea.query(source_path = sink_path, \r\n",
                "                                query_str = f\"\"\"select max({filtering_date}) maxdatetime \r\n",
                "                                                \"\"\",\r\n",
                "                                criteria_str = f\"\"\"DistrictId = '{districtId}'\r\n",
                "                                                   AND SchoolYear = '{schoolYear}' \r\n",
                "                                                   AND sap_pipeline = '{sap_pipeline}'\r\n",
                "                                                   AND sap_pipelineType = '{sap_pipelineType}'\r\n",
                "                                                \"\"\")\r\n",
                "            maxdatetime = sink_df.first()['maxdatetime']\r\n",
                "        except AnalysisException as e:\r\n",
                "            pass\r\n",
                "\r\n",
                "        changes_df = oea.query(source_path = source_path, \r\n",
                "                               query_str = f\"\"\"select * \"\"\",\r\n",
                "                               criteria_str = f\"\"\"DistrictId = '{districtId}'\r\n",
                "                                                   AND SchoolYear = '{schoolYear}' \r\n",
                "                                                   AND sap_pipeline = '{sap_pipeline}'\r\n",
                "                                                   AND sap_pipelineType = '{sap_pipelineType}'\r\n",
                "                                                \"\"\")\r\n",
                "        if maxdatetime and not(debugMode):\r\n",
                "            changes_df = changes_df.where(f\"{filtering_date} > '{maxdatetime}'\")        \r\n",
                "        return changes_df\r\n",
                "\r\n",
                "def upsert_with_logging(df, \r\n",
                "                        sap_pipeline,\r\n",
                "                        destination_path, \r\n",
                "                        primary_key, \r\n",
                "                        partitioning, \r\n",
                "                        partitioning_cols,\r\n",
                "                        table_name,\r\n",
                "                        ext_entity):\r\n",
                "        start_time = datetime.now()\r\n",
                "        numInputRows, numOutputRows, numTargetRowsInserted, numTargetRowsUpdated = oea.upsert(df = df, \r\n",
                "                                                                    destination_path = destination_path,\r\n",
                "                                                                    primary_key = primary_key,#['RECORD', 'DistrictId', 'SchoolYear'],\r\n",
                "                                                                    partitioning = True,\r\n",
                "                                                                    partitioning_cols = partitioning_cols,\r\n",
                "                                                                    surrogate_key = False)   \r\n",
                "        end_time = datetime.now()\r\n",
                "        log_data = error_logger.create_log_dict(uniqueId = error_logger.generate_random_alphanumeric(10), # Generate a random 10-character alphanumeric value\r\n",
                "                                                pipelineExecutionId = pipelineExecutionId,#'TEST_1234',#executionId,\r\n",
                "                                                sparkSessionId = spark.sparkContext.applicationId,\r\n",
                "                                                sap_pipeline = sap_pipeline,\r\n",
                "                                                sap_pipelineType = sap_pipelineType,\r\n",
                "                                                stageName = \"S2R-To-Stage3\",\r\n",
                "                                                schemaFormat = 'ed-fi',\r\n",
                "                                                entityType = ext_entity,\r\n",
                "                                                entityName = table_name,\r\n",
                "                                                numInputRows = numInputRows,\r\n",
                "                                                totalNumOutputRows = numOutputRows,\r\n",
                "                                                numTargetRowsInserted = numTargetRowsInserted,\r\n",
                "                                                numTargetRowsUpdated = numTargetRowsUpdated,\r\n",
                "                                                numRecordsSkipped = 0,\r\n",
                "                                                numRecordsDeleted = 0,\r\n",
                "                                                start_time = start_time,\r\n",
                "                                                end_time = end_time,\r\n",
                "                                                insertionType = 'upsert',\r\n",
                "                                                emptySchemaMetadata = False)\r\n",
                "        error_logger.consolidate_logs(log_data,'entity')\r\n",
                "\r\n",
                "\r\n",
                "def dump_to_stage3(source_path, \r\n",
                "                   sap_pipeline,\r\n",
                "                   sap_pipelineType,\r\n",
                "                   destination_path,\r\n",
                "                   primary_key,\r\n",
                "                   partitioning,\r\n",
                "                   partitioning_cols,\r\n",
                "                   extension,\r\n",
                "                   schoolYear,\r\n",
                "                   districtId):\r\n",
                "    items = oea.get_folders(source_path)\r\n",
                "    for item in items: \r\n",
                "        table_path = source_path +'/'+ item\r\n",
                "        sink_path = f'{destination_path}/{item}'\r\n",
                "        if item == 'metadata.csv':\r\n",
                "            logger.info('ignore metadata processing, since this is not a table to be ingested')\r\n",
                "        # NOTE: The commented code below makes no impact in the loop. Hence it's under review\r\n",
                "        # elif metadata_pipeline_type is not None and not(metadata_pipeline_type.lower().startswith('peims_')) and sap_pipeline == 'peims-submissions':\r\n",
                "        #     pass\r\n",
                "        # elif metadata_pipeline_type is not None and not(metadata_pipeline_type.lower().startswith('tsds_')) and sap_pipeline == 'tsds_crf-submissions':\r\n",
                "        #     # FIXME: Temporary fix for current dir. structure\r\n",
                "        #     sap_pipeline_arg = sap_pipeline = 'peims-submissions'\r\n",
                "        else:\r\n",
                "            try:\r\n",
                "                logger.info(f'Transfering {item} ({sap_pipeline}, {sap_pipelineType}) to stage 3 for submissions pipeline')\r\n",
                "                # logger.info(f\"SOURCE PATH - {table_path}\")\r\n",
                "                # logger.info(f\"SINK PATH - {sink_path}\")\r\n",
                "                if '/general/' in source_path:\r\n",
                "                    df_changes = get_latest_s2r_changes(source_path = table_path, \r\n",
                "                                                        sink_path = sink_path,\r\n",
                "                                                        filtering_date = 'rundate',\r\n",
                "                                                        debugMode = False,\r\n",
                "                                                        sap_pipeline = sap_pipeline,\r\n",
                "                                                        sap_pipelineType = sap_pipelineType,\r\n",
                "                                                        schoolYear = schoolYear,\r\n",
                "                                                        districtId = districtId)\r\n",
                "                    # TODO: UNDER REVIEW\r\n",
                "                    df_changes = add_metadata_columns(df = df_changes,\r\n",
                "                                                      edfi_location = 'INVALID_VALUE_PLACEHOLDER',\r\n",
                "                                                      edfi_id = 'INVALID_VALUE_PLACEHOLDER',\r\n",
                "                                                      edfi_id_modified = 'INVALID_VALUE_PLACEHOLDER'\r\n",
                "                                                      )\r\n",
                "                    if df_changes.count() > 0:\r\n",
                "                        upsert_with_logging(df = df_changes, \r\n",
                "                                sap_pipeline = sap_pipeline,\r\n",
                "                                destination_path = sink_path, \r\n",
                "                                primary_key = primary_key, \r\n",
                "                                partitioning = partitioning, \r\n",
                "                                partitioning_cols = partitioning_cols,\r\n",
                "                                table_name = item,\r\n",
                "                                ext_entity = 'ed-fi' if extension is None else extension)\r\n",
                "                    else:\r\n",
                "                        logger.info(f'No Ingress Records - {item}')\r\n",
                "            except AnalysisException as e:\r\n",
                "                # This means the table may have not been properly refined due to errors with the primary key not aligning with columns expected in the lookup table.\r\n",
                "                logger.info(e)\r\n",
                "            \r\n",
                "            logger.info('Refined table: ' + item + ' from: ' + table_path)"
            ],
            "outputs": []
        },
        {
            "execution_count": 35,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "primary_key = ['NATURAL_KEY_HASH','DistrictId', 'SchoolYear']\r\n",
                "partitioning = True\r\n",
                "partitioning_cols = ['DistrictId', 'SchoolYear']\r\n",
                "\r\n",
                "# submission_periods = {sap_pipeline : submissionPeriod}\r\n",
                "# # elif sap_pipeline == 'All':\r\n",
                "# #    submission_periods = {'peims-submissions': submissionPeriod , \r\n",
                "# #                         'tsds_crf-submissions':'winter'}"
            ],
            "outputs": []
        },
        {
            "execution_count": 36,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "schema = 'ed-fi'\r\n",
                "extension = None\r\n",
                "\r\n",
                "source_path = f'stage2/Refined/SAP/data-submissions/pipeline={sap_pipeline}/pipelineType={sap_pipelineType}/{sapVersion}/general/{schema}'\r\n",
                "destination_path = f'stage3/pipeline={sap_pipeline}/pipelineType={sap_pipelineType}/{apiVersion}/general/{schema}'\r\n",
                "\r\n",
                "dump_to_stage3(source_path = source_path, \r\n",
                "                sap_pipeline = sap_pipeline,\r\n",
                "                sap_pipelineType = sap_pipelineType,\r\n",
                "                destination_path = destination_path,\r\n",
                "                primary_key = primary_key,\r\n",
                "                partitioning = partitioning,\r\n",
                "                partitioning_cols = partitioning_cols,\r\n",
                "                extension = extension,\r\n",
                "                schoolYear = schoolYear,\r\n",
                "                districtId = districtId)"
            ],
            "outputs": []
        },
        {
            "execution_count": 37,
            "cell_type": "code",
            "metadata": {
                "language": "python",
                "azdata_cell_guid": "7b9a7eb7-79ea-4994-9412-7bebadf63bb9"
            },
            "source": [
                "schema = 'tx'\r\n",
                "extension = 'tx'\r\n",
                "\r\n",
                "source_path = f'stage2/Refined/SAP/data-submissions/pipeline={sap_pipeline}/pipelineType={sap_pipelineType}/{sapVersion}/general/{schema}'\r\n",
                "destination_path = f'stage3/pipeline={sap_pipeline}/pipelineType={sap_pipelineType}/{apiVersion}/general/{schema}'\r\n",
                "\r\n",
                "dump_to_stage3(source_path = source_path, \r\n",
                "                sap_pipeline = sap_pipeline,\r\n",
                "                sap_pipelineType = sap_pipelineType,\r\n",
                "                destination_path = destination_path,\r\n",
                "                primary_key = primary_key,\r\n",
                "                partitioning = partitioning,\r\n",
                "                partitioning_cols = partitioning_cols,\r\n",
                "                extension = extension,\r\n",
                "                schoolYear = schoolYear,\r\n",
                "                districtId = districtId)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Error Logging"
            ],
            "outputs": []
        },
        {
            "execution_count": 38,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "if error_logger.entity_logs != []:\r\n",
                "    df_logs = error_logger.create_spark_df('entity')\r\n",
                "    error_logger.write_logs_to_delta_lake(df = df_logs, \r\n",
                "                                log_type = 'entity',\r\n",
                "                                destination_url = error_logger.to_logs_url('etl-logs/log_type=entity'))\r\n",
                "    error_logger.add_etl_logs_to_lake_db(db_name = f'ldb_{workspace}_sap_etl_logs',\r\n",
                "                                        logs_base_path = 'etl-logs',\r\n",
                "                                        log_type = 'entity',\r\n",
                "                                        overwrite = True)"
            ],
            "outputs": []
        }
    ]
}