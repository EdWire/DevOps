{
    "nbformat": 4,
    "nbformat_minor": 2,
    "metadata": {
        "save_output": true,
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "execution_count": 50,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# NOTE: Under Experimentation\n",
                "pipeline_suffix_mappings = {'PEIMS_FALL': '_peims',\n",
                "                            'PEIMS_MIDYR': '_peims',\n",
                "                            'TSDS_CLASS_ROSTER_FALL': '_tsds',\n",
                "                            'ANALYTICS': '_analytics'}"
            ],
            "outputs": []
        },
        {
            "execution_count": 51,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "instance = instanceId = InstanceId\n",
                "apiUrl = ApiUrl\n",
                "schoolYear = SchoolYear\n",
                "DistrictId = DistrictID = districtId = districtID\n",
                "apiLimit = batchLimit\n",
                "\n",
                "prepareSAPMetaData = prepareSAPMetadata\n",
                "zone = submissionsType = sap_pipeline"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Pre-Requisites (Dev)"
            ],
            "outputs": []
        },
        {
            "execution_count": 52,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from notebookutils import mssparkutils\n",
                "import configparser\n",
                "\n",
                "config_path = \"/tmp/conf.ini\"\n",
                "def copy_config_to_temp():\n",
                "    mssparkutils.fs.cp(oea.to_url(\"stage1/Transactional/SAP/metadata-assets/edfi-configs.ini\"),\"file:/tmp/conf.ini\")\n",
                "\n",
                "def read_edfi_credentials(config_path):\n",
                "    config = configparser.ConfigParser()\n",
                "    config.read(config_path)\n",
                "\n",
                "    edfi_credentials = {}\n",
                "\n",
                "    if 'EdFi' in config:\n",
                "        edfi_credentials['client_id'] = config['EdFi'].get('client_id', '')\n",
                "        edfi_credentials['client_secret'] = config['EdFi'].get('client_secret', '')\n",
                "\n",
                "    return edfi_credentials"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Actual Code"
            ],
            "outputs": []
        },
        {
            "execution_count": 53,
            "cell_type": "code",
            "metadata": {
                "azdata_cell_guid": "fbd392d9-afa3-4158-8555-1a87898307f4",
                "language": "python"
            },
            "source": [
                "import pyspark\n",
                "\n",
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql import DataFrame\n",
                "from pyspark.sql.utils import AnalysisException\n",
                "from pyspark.sql.types import StringType, StructType, StructField, IntegerType\n",
                "\n",
                "from pyspark.sql.functions import col, substring, regexp_extract, split, lit, struct, to_date, from_unixtime, date_format\n",
                "from pyspark.sql.functions import create_map, lit, when, array, coalesce, concat_ws\n",
                "from pyspark.sql.functions import collect_list, create_map, lit, struct, array, concat\n",
                "from pyspark.sql.functions import expr\n",
                "\n",
                "from pyspark.sql import functions as F\n",
                "import pyspark.sql.functions as f\n",
                "from pyspark.sql.functions import udf\n",
                "\n",
                "import json\n",
                "import os\n",
                "import pandas as pd\n",
                "import re\n",
                "\n",
                "import copy\n",
                "from itertools import chain"
            ],
            "outputs": []
        },
        {
            "execution_count": 54,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "print('REFINEMENT - TESTING PARAMETERIZATION')\n",
                "try:\n",
                "    print(kVName)\n",
                "    print(workspace)\n",
                "    print(apiUrl)\n",
                "    print(instanceId)\n",
                "    print(moduleName)\n",
                "    print(apiLimit)\n",
                "    print(minChangeVer)\n",
                "    print(maxChangeVer)\n",
                "    print(sapVersion)\n",
                "    print(prepareSAPMetaData)\n",
                "    print(submissions)\n",
                "    print(submissionsType)\n",
                "    print(schoolYear)\n",
                "    print(districtID)\n",
                "    print(pipelineExecutionId)\n",
                "\n",
                "    kvName = kVName\n",
                "    districtId = districtID\n",
                "    districtId = districtId\n",
                "except Exception as params_error:\n",
                "    print('CATCHING ERROR!!!')\n",
                "    print(params_error)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### URLs Initializations"
            ],
            "outputs": []
        },
        {
            "execution_count": 55,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "%run OEA/modules/Ed-Fi/v0.7/src/utilities/edfi_v0_7_fetch_urls"
            ],
            "outputs": []
        },
        {
            "execution_count": 56,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "instance_id = instanceId\n",
                "school_year = schoolYear\n",
                "api_year = school_year\n",
                "api_url = apiUrl\n",
                "\n",
                "# FIXME: 2024-01-31 TEMP FIX FOR FY\n",
                "try:\n",
                "    edfi_api_manager = EdFiApiManager(api_url, instance_id, api_year)\n",
                "    edfi_api_manager.update_urls()\n",
                "    edfi_api_manager.set_other_metadata()\n",
                "\n",
                "    dependenciesUrl = edfi_api_manager.dependencies_url\n",
                "    openApiMetadataUrl = edfi_api_manager.openapi_metadata_url\n",
                "    dataManagementUrl = edfi_api_manager.data_management_url\n",
                "    authUrl = edfi_api_manager.auth_url\n",
                "\n",
                "    changeQueriesUrl = edfi_api_manager.get_referenced_url('Change-Queries')\n",
                "    changeQueriesUrl = changeQueriesUrl[:-13].replace('/metadata/', '/')\n",
                "    swagger_url = swaggerUrl = edfi_api_manager.get_referenced_url('Resources')\n",
                "\n",
                "    apiVersion = edfi_api_manager.api_version\n",
                "    apiVersion = apiVersion[1:] if apiVersion.startswith('v') else apiVersion\n",
                "except Exception as error:\n",
                "    edfi_api_manager = EdFiApiManager(api_url, instance_id, '')\n",
                "    edfi_api_manager.update_urls()\n",
                "    edfi_api_manager.set_other_metadata()\n",
                "\n",
                "    dependenciesUrl = edfi_api_manager.dependencies_url\n",
                "    openApiMetadataUrl = edfi_api_manager.openapi_metadata_url\n",
                "    dataManagementUrl = edfi_api_manager.data_management_url\n",
                "    authUrl = edfi_api_manager.auth_url\n",
                "\n",
                "    changeQueriesUrl = edfi_api_manager.get_referenced_url('Change-Queries')\n",
                "    changeQueriesUrl = changeQueriesUrl[:-13].replace('/metadata/', '/')\n",
                "    swagger_url = swaggerUrl = edfi_api_manager.get_referenced_url('Resources')\n",
                "\n",
                "    apiVersion = edfi_api_manager.api_version\n",
                "    apiVersion = apiVersion[1:] if apiVersion.startswith('v') else apiVersion"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### OEA Initiliazations"
            ],
            "outputs": []
        },
        {
            "execution_count": 57,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "%run EdGraph/modules/SAP_PEIMS/v0.6/src/utilities/sap_peim_v0_6_sap_py"
            ],
            "outputs": []
        },
        {
            "execution_count": 58,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from datetime import datetime\n",
                "oea = SAPEdFiOEAChild(workspace='dev', \n",
                "                      logging_level=logging.INFO, \n",
                "                      storage_account=None, \n",
                "                      keyvault=None, \n",
                "                      timezone=None,\n",
                "                      sap_pipeline = sap_pipeline,\n",
                "                      sap_pipelineType = sap_pipelineType)   \n",
                "oea.set_workspace(workspace)\n",
                "oea.ingestionHistoryMode = ingestionHistoryMode"
            ],
            "outputs": []
        },
        {
            "execution_count": 59,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# swagger_url = swaggerUrl = edfi_api_manager.get_referenced_url('Descriptors')\n",
                "oea_utils = schema_gen = SAPOpenAPIUtilChild(swagger_url)\n",
                "oea_utils.create_definitions()\n",
                "schemas = schema_gen.create_spark_schemas()"
            ],
            "outputs": []
        },
        {
            "execution_count": 60,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# Set Ed-Fi Credentials\n",
                "# copy_config_to_temp()\n",
                "\n",
                "# credentials = read_edfi_credentials(config_path)\n",
                "# client_id = credentials.get('client_id')\n",
                "# client_secret_id = credentials.get('client_secret')"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Metadata for Processing"
            ],
            "outputs": []
        },
        {
            "execution_count": 61,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "metadata_path = \"stage1/Transactional/SAP/metadata-assets/sap-to-edfi.json\"\n",
                "metadata_url = oea.to_url(metadata_path)"
            ],
            "outputs": []
        },
        {
            "execution_count": 62,
            "cell_type": "code",
            "metadata": {
                "azdata_cell_guid": "c436cf92-c8ca-4e43-989b-d941044521fa",
                "language": "python"
            },
            "source": [
                "jsonDF = spark.read.option(\"multiline\", \"true\").json(metadata_url).cache()\n",
                "\n",
                "json_string = jsonDF.toJSON().collect()[0]\n",
                "config_data = json.loads(json_string)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### SAP & Error Logging Initiliazations"
            ],
            "outputs": []
        },
        {
            "execution_count": 63,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "error_logger = ErrorLogging(spark = spark, \n",
                "                            oea = oea, \n",
                "                            logger = logger)"
            ],
            "outputs": []
        },
        {
            "execution_count": 64,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# oea_utils = schema_gen = SAPOpenAPIUtilChild(swagger_url)\n",
                "# oea_utils.create_definitions()\n",
                "# schemas = schema_gen.create_spark_schemas()\n",
                "\n",
                "primitive_datatypes = ['timestamp', 'date', 'decimal', 'boolean', 'integer', 'string', 'long']\n",
                "sap_to_edfi_complex = config_data.get('sap_to_edfi_complex', {})\n",
                "final_columns = config_data.get('final_columns', {})\n",
                "_ext_TX_cols = config_data.get('_ext_TX_cols', {})\n",
                "descriptorsDFRef = config_data.get('descriptorsDFRef', {})\n",
                "descriptors = config_data.get('descriptors', [])\n",
                "sap_essential_columns = config_data.get('sap_essential_columns', {}) # ['DistrictId', 'SchoolYear', 'metadata_pipeline_type', 'lakeId', 'validationRecordId', 'LastModifiedDate', 'rundate', 'stage1_source_url', 'RECORD', 'NATURAL_KEY_HASH', 'RECORD_HASH']"
            ],
            "outputs": []
        },
        {
            "execution_count": 65,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "sap_utilities = SAPUtilities(spark = spark, \n",
                "                             oea = oea,\n",
                "                             sap_essential_columns = sap_essential_columns)"
            ],
            "outputs": []
        },
        {
            "execution_count": 66,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "sap_process_client = SAPProcessClient(spark = spark, \n",
                "                                      oea = oea,\n",
                "                                      sap_utilities = sap_utilities,\n",
                "                                      sap_to_edfi_complex = sap_to_edfi_complex,\n",
                "                                      final_columns = final_columns,\n",
                "                                      _ext_TX_cols = _ext_TX_cols, \n",
                "                                      descriptorsDFRef = descriptorsDFRef,\n",
                "                                      descriptors = descriptors) "
            ],
            "outputs": []
        },
        {
            "execution_count": 67,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "def initialize_sap_pipeline_vars(sapVersion = '1.0',\n",
                "                                 districtId = '101912',\n",
                "                                 schoolYear = '2023',\n",
                "                                 base_path = '',\n",
                "                                 column_mapping_file_path = ''):\n",
                "    for descriptor in sap_process_client.descriptors:\n",
                "        descriptor_path = f\"{base_path}/ed-fi/{descriptor}\"\n",
                "        # FIXME: Temp Fix\n",
                "        try:\n",
                "            sap_process_client.descriptorsDFRef[descriptor] = sap_utilities.loadDescriptors(descriptor_path).cache()\n",
                "        except Exception as error:\n",
                "            sap_process_client.descriptorsDFRef[descriptor] = sap_utilities.loadDescriptors(descriptor_path.replace('/ed-fi', '/TX')).cache()\n",
                "\n",
                "    column_mapping_file_url = oea.to_url(column_mapping_file_path) \n",
                "    column_mappings = sap_utilities.extract_refined_cols_mapping(column_mapping_file_url)\n",
                "\n",
                "    return column_mappings"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### REFINEMENT - UTILITIES"
            ],
            "outputs": []
        },
        {
            "execution_count": 68,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "def add_metadata_columns(df, overwrite,**kwargs):\n",
                "    for column_name, constant_value in kwargs.items():\n",
                "        if overwrite:\n",
                "            df = df.withColumn(column_name, F.lit(constant_value))\n",
                "        elif column_name not in df.columns:\n",
                "            df = df.withColumn(column_name, F.lit(constant_value))\n",
                "    return df\n",
                "\n",
                "def apply_data_transformations(df, edfi_item, source_path):\n",
                "    if edfi_item in ['budgetExts', 'actualExts']:\n",
                "        df = df.withColumn('educationOrganizationId', lit(sap_utilities.extract_district_id(source_path)))\n",
                "    elif edfi_item == 'staffs':\n",
                "        if 'raceDescriptor' in df.columns:\n",
                "            df = df.withColumn('raceDescriptor', concat(lit('0'), col('raceDescriptor')))\n",
                "        if 'pkTeacherRequirementDescriptor' in df.columns:\n",
                "            df = df.withColumn('pkTeacherRequirementDescriptor', concat(lit('0'), col('pkTeacherRequirementDescriptor')))\n",
                "    elif edfi_item == 'staffEducationOrganizationAssignmentAssociations':\n",
                "        df = sap_utilities.format_digit_vals(df, 'staffClassificationDescriptor')\n",
                "    elif edfi_item == 'staffEducationOrganizationEmploymentAssociations':\n",
                "        pass\n",
                "\n",
                "    return df\n",
                "\n",
                "def transform_descriptor_columns(df, descriptor_columns):\n",
                "    # Transform descriptor columns\n",
                "    beforeTransform = df.count()\n",
                "    for descriptor in descriptor_columns:\n",
                "        if not(descriptor.startswith('race')):\n",
                "            if descriptor == 'highestCompletedLevelOfEducationDescriptor':\n",
                "                descriptorKey = 'levelOfEducationDescriptor'\n",
                "            else:\n",
                "                descriptorKey = descriptor\n",
                "            descriptorKey = f\"{descriptorKey}s\"\n",
                "            df = sap_utilities.transform_dataframe(df, descriptor, sap_process_client.descriptorsDFRef[descriptorKey])\n",
                "    afterTransform = df.count()\n",
                "    if beforeTransform != afterTransform:\n",
                "        logger.info(f\"[REFINEMENT DESCRIPTOR JOIN] NUMBER OF RECORDS MISMATCHED - {edfi_item}\")\n",
                "        return None\n",
                "    return df\n",
                "\n",
                "def process_edfi_item(df, edfi_item):\n",
                "    # Process the specific edfi_item\n",
                "    processing_functions = {\n",
                "        'budgetExts': sap_process_client.processBudgetExts,\n",
                "        'actualExts': sap_process_client.processActualExts,\n",
                "        'staffEducationOrganizationAssignmentAssociations': sap_process_client.processStaffEducationOrganizationAssignmentAssociations,\n",
                "        'payrollExts': sap_process_client.processPayrollExts,\n",
                "        'contractedInstructionalStaffFTEExts': sap_process_client.processContractedInstructionalStaffFTEExts,\n",
                "        'staffs': sap_process_client.processStaffs,\n",
                "        'staffEducationOrganizationEmploymentAssociations': sap_process_client.processStaffEducationOrganizationEmploymentAssociations\n",
                "    }\n",
                "\n",
                "    if edfi_item in processing_functions:\n",
                "        df = processing_functions[edfi_item](df)\n",
                "        if edfi_item == 'payrollExts':\n",
                "            df = df.withColumn('beginDate', to_date(col('beginDate'), \"M/d/yyyy\"))\n",
                "    return df\n",
                "\n",
                "def transform_sap_to_edfi(source_path, sap_pipeline,sap_pipelineType, item,edfi_item=None, edfi_version=None):\n",
                "    global column_mappings\n",
                "    logger.info(\"[REFINEMENT TRANSFORMING SAP TO EDFI]\")\n",
                "    sink_general_path, _ = sap_utilities.get_sink_general_sensitive_paths(source_path = source_path,\n",
                "                                                                          edfi_version = edfi_version,\n",
                "                                                                          edfi_item = edfi_item,\n",
                "                                                                          partitioning = True,\n",
                "                                                                          SAP_SUB='data-submissions',\n",
                "                                                                          TEST_MODE = False)\n",
                "    entity_column_mapping = column_mappings[sap_pipeline][sap_pipelineType][f'tx/{edfi_item}'].asDict()\n",
                "    df_changes = oea.get_latest_changes(source_path, \n",
                "                                        sink_general_path,\n",
                "                                        filtering_date = 'rundate',\n",
                "                                          primary_key = ['NATURAL_KEY_HASH'],\n",
                "                                          debugMode = False)\n",
                "    df_changes = sap_utilities.map_to_hard_values(df_changes, edfi_item)\n",
                "    df_changes = sap_utilities.map_columns(df_changes, entity_column_mapping)\n",
                "    descriptor_columns = sap_utilities.infer_descriptor_columns(df_changes.columns)\n",
                "    \n",
                "    df_changes = apply_data_transformations(df_changes, edfi_item, source_path)\n",
                "    df_changes = transform_descriptor_columns(df_changes, descriptor_columns)\n",
                "    df_changes = process_edfi_item(df_changes, edfi_item)\n",
                "    return df_changes\n",
                "\n",
                "def upsert_edfi_complex_data(df_changes, sink_general_path, primary_key):\n",
                "    # TODO: Make if better for passing the right params instead of global declarations\n",
                "    global districtId, schoolYear\n",
                "    primary_key = ['DistrictId', 'SchoolYear', 'sap_pipeline','sap_pipelineType', 'NATURAL_KEY_HASH']\n",
                "    partitioning_cols = ['DistrictId', 'SchoolYear', 'sap_pipeline', 'sap_pipelineType']\n",
                "    # FIXME: 2024-01-29 - storing df_changes.count() to changes_count as a temp fix\n",
                "    changes_count = df_changes.count()\n",
                "    if changes_count > 0:\n",
                "        # FIXME: 2024-01-29 - JOIN BASED UPSERT Under Review\n",
                "        oea.upsert(df_changes, \n",
                "                   sink_general_path, \n",
                "                   primary_key = primary_key,#primary_key, \n",
                "                   partitioning = True, \n",
                "                   partitioning_cols = partitioning_cols,\n",
                "                   join_based_upsert = False\n",
                "                    )\n",
                "        oea.add_to_lake_db(sink_general_path, overwrite=True)\n",
                "        # FIXME: 2024-01-29 - df_changes.count() during logging is 0 (zero) - Why?\n",
                "        logger.info(f'[REFINEMENT UPSERT EDFI COMPLEX / NESTED] SAP to Ed-Fi API: Processed {changes_count} updated rows into stage2/Refined')\n",
                "    else:\n",
                "        logger.info('[REFINEMENT UPSERT EDFI COMPLEX / NESTED] SAP to Ed-Fi API: No updated rows to process.')\n",
                "    return changes_count\n",
                "\n",
                "def refine_item_to_edfi_complex(table_path, edfi_version, item, sap_pipeline, sap_pipelineType,districtId, schoolYear):\n",
                "    logger.info(f\"[REFINEMENT ITEM TO EDFI COMPLEX / NESTED] Refining the item {item} to Ed-Fi Complex Standard\")\n",
                "    edfi_item = sap_process_client.sap_to_edfi_complex[item] if not item.lower().endswith('descriptors') else item\n",
                "    df_processed = transform_sap_to_edfi(table_path, sap_pipeline, sap_pipelineType,item,edfi_item, edfi_version)\n",
                "    df_processed = add_metadata_columns(df_processed,\n",
                "                                        overwrite = False, \n",
                "                                        DistrictId = districtId, \n",
                "                                        SchoolYear = schoolYear,\n",
                "                                        RECORD_VERSION = 1,\n",
                "                                        SUBMISSION_RECORD_IS_ACTIVE = True)\n",
                "    \n",
                "    if sap_utilities.has_column(df_processed, 'RECORD'):\n",
                "        df_processed = df_processed.withColumn('lakeId', f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.col('NATURAL_KEY_HASH')).cast(\"String\"))\n",
                "        df_processed = df_processed.withColumn('validationRecordId', f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.col('NATURAL_KEY_HASH')).cast(\"String\"))\n",
                "    else:\n",
                "        df_processed = df_processed.withColumn('lakeId', f.lit(None).cast(\"String\"))\n",
                "        df_processed = df_processed.withColumn('validationRecordId', f.lit(None).cast(\"String\"))\n",
                "    return df_processed\n",
                "\n",
                "def process_and_refine_sap_entity(tables_source, edfi_version, sap_pipeline, sap_pipelineType, item, test_mode, schoolYear, districtId):\n",
                "    global deletePrevSubmissions\n",
                "    table_path = tables_source + '/' + item\n",
                "    if item == 'metadata.csv':\n",
                "        logger.info('Ignore metadata processing, since this is not a table to be ingested')\n",
                "    else:\n",
                "        df_processed = refine_item_to_edfi_complex(table_path, edfi_version, item, sap_pipeline, sap_pipelineType, districtId, schoolYear)\n",
                "        try:\n",
                "            sink_general_path, _ = sap_utilities.get_sink_general_sensitive_paths(source_path = table_path,\n",
                "                                                                                  edfi_version = edfi_version,\n",
                "                                                                                  edfi_item = sap_process_client.sap_to_edfi_complex[item],\n",
                "                                                                                  partitioning = True,\n",
                "                                                                                  SAP_SUB='data-submissions',\n",
                "                                                                                  TEST_MODE = False)\n",
                "\n",
                "            # FIXME: 2024-01-29 - JOIN BASED UPSERT Under Review\n",
                "            try:\n",
                "                oea.merge_deletes_into_delta_lake(df = df_processed.select('NATURAL_KEY_HASH', 'rundate').cache(), \n",
                "                                                  destination_path = sink_general_path, \n",
                "                                                  func_enabled = deletePrevSubmissions)\n",
                "                df_processed = oea.get_df_latest_records_by_join(df = df_processed, \n",
                "                                                                destination_path = sink_general_path, \n",
                "                                                                func_enabled = True)\n",
                "            except:\n",
                "                logger.info('[REFINEMENT PROCESS AND REFINE SAP] Delta Lake does not exist JOIN BASED UPSERT disabled')\n",
                "                df_processed = oea.get_df_latest_records_by_join(df = df_processed, \n",
                "                                                                destination_path = sink_general_path, \n",
                "                                                                func_enabled = False)\n",
                "            changes_count = upsert_edfi_complex_data(df_changes = df_processed, \n",
                "                                                     sink_general_path = sink_general_path, \n",
                "                                                     primary_key = 'lakeId')\n",
                "            \n",
                "            # FIXME: 2024-01-30: TEMP FIX TO BYPASS LEFT-ANTI DATA CORRUPUTION\n",
                "            sink_df = oea.query(sink_general_path, f'select max(rundate) maxdatetime')\n",
                "            maxdatetime = sink_df.first()['maxdatetime']\n",
                "            df_processed = oea.load(sink_general_path).where(f\"rundate >= '{maxdatetime}'\")\n",
                "            logger.info(f\"[REFINEMENT PROCESS AND REFINE SAP] SAP to Ed-Fi API: ALERT!!! - LATEST RECORDS LOADED FROM THE NESTED DELTA FOR ANALYTICS - {df_processed.count()}\")\n",
                "        except Exception as e:\n",
                "            logger.exception(f\"[REFINEMENT PROCESS AND REFINE SAP] {e}\")\n",
                "            return e\n",
                "        \n",
                "        return df_processed"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Flatten + Transform"
            ],
            "outputs": []
        },
        {
            "execution_count": 69,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "def process_transform_and_refine_SAP_entity(item, tables_source, edfi_version, sap_pipeline, sap_pipelineType, test_mode):\n",
                "    global districtId, schoolYear\n",
                "    table_path = tables_source + '/' + item\n",
                "\n",
                "    if not(item.lower().endswith('descriptors')):\n",
                "        edfi_item = sap_process_client.sap_to_edfi_complex.get(item)\n",
                "        if edfi_item is None:\n",
                "            logger.info(f'[REFINEMENT PROCESS, TRANSFORM AND REFINE SAP] New SAP item detected - {item} (Mapping Not Available)')\n",
                "            return None\n",
                "    else:\n",
                "        edfi_item = item\n",
                "\n",
                "    logger.info(f\"[REFINEMENT PROCESS, TRANSFORM AND REFINE SAP] Processing table: {edfi_item}\")\n",
                "    try:\n",
                "        if not(item.lower().endswith('descriptors')): \n",
                "            # FIXME: Under Review\n",
                "            source_path = f'stage2/Ingested/{table_path}'\n",
                "            sink_general_path, sink_sensitive_path = sap_utilities.get_sink_general_sensitive_paths(source_path,\n",
                "                                                                                                  edfi_version,\n",
                "                                                                                                  edfi_item,\n",
                "                                                                                                  partitioning=True,\n",
                "                                                                                                  SAP_SUB='data-submissions',  # 'FINAL',\n",
                "                                                                                                  TEST_MODE=test_mode)\n",
                "            df_changes = oea.get_latest_changes(source_path, \n",
                "                                          sink_general_path, \n",
                "                                          filtering_date = 'rundate',\n",
                "                                          primary_key = ['NATURAL_KEY_HASH'],\n",
                "                                          debugMode = False)\n",
                "            if df_changes.count() > 0:\n",
                "                logger.info('[REFINEMENT PROCESS, TRANSFORM AND REFINE SAP] SAP to Ed-Fi API: ' + item + ' from: ' + table_path)\n",
                "                df = process_and_refine_sap_entity(f\"stage2/Ingested/{tables_source}\",\n",
                "                                                edfi_version=edfi_version,\n",
                "                                                sap_pipeline=sap_pipeline,\n",
                "                                                sap_pipelineType = sap_pipelineType,\n",
                "                                                item=item,\n",
                "                                                test_mode=test_mode,\n",
                "                                                districtId = districtId,\n",
                "                                                schoolYear = schoolYear)\n",
                "            \n",
                "        source_path = f'stage2/Ingested/{table_path}'\n",
                "        sink_general_path, sink_sensitive_path = sap_utilities.get_sink_general_sensitive_paths(source_path,\n",
                "                                                                                                  edfi_version,\n",
                "                                                                                                  edfi_item,\n",
                "                                                                                                  partitioning=True,\n",
                "                                                                                                  SAP_SUB='analytics',  # 'FINAL',\n",
                "                                                                                                  TEST_MODE=test_mode)\n",
                "        df_changes = oea.get_latest_changes(source_path,\n",
                "                                          sink_general_path,\n",
                "                                          filtering_date = 'rundate',\n",
                "                                          primary_key = ['NATURAL_KEY_HASH'],\n",
                "                                          debugMode = False)\n",
                "        logger.info(f\"[REFINEMENT PROCESS, TRANSFORM AND REFINE SAP] {df_changes.count()}\")\n",
                "        if df_changes.count() > 0:\n",
                "            logger.info('[REFINEMENT PROCESS, TRANSFORM AND REFINE SAP] Ed-Fi to Ed-Fi Relationship Model: ' + edfi_item)\n",
                "\n",
                "            # NOTE: if-else condition is necessary so that descriptor tables and SAP tables\n",
                "            # are correctly processed\n",
                "            if not(item.lower().endswith('descriptors')):\n",
                "                df_changes = df\n",
                "            else:\n",
                "                pass\n",
                "\n",
                "            current_timestamp = datetime.now()\n",
                "            df_changes = add_metadata_columns(df_changes, \n",
                "                                              overwrite = False,\n",
                "                                              DistrictId = districtId, \n",
                "                                              SchoolYear = schoolYear,\n",
                "                                              LastModifiedDate = current_timestamp,\n",
                "                                              RECORD_VERSION = 1,\n",
                "                                              SUBMISSION_RECORD_IS_ACTIVE = True)\n",
                "      \n",
                "            df = sap_to_edfi_client.transform(df = df_changes,\n",
                "                                              schema_name = schema_name,\n",
                "                                              table_name = edfi_item,\n",
                "                                              primary_key = 'NATURAL_KEY_HASH',\n",
                "                                              ext_entity = ext_entity,\n",
                "                                              sink_general_path = sink_general_path,\n",
                "                                              parent_schema_name = None,\n",
                "                                              parent_table_name = None)\n",
                "        else:\n",
                "            logger.info(f'[REFINEMENT PROCESS, TRANSFORM AND REFINE SAP] Ed-Fi to Ed-Fi Relationship Model: No updated rows in {source_path} to process.')\n",
                "\n",
                "    except AnalysisException as e:\n",
                "        logger.exception(f\"[REFINEMENT PROCESS, TRANSFORM AND REFINE SAP] {e}\")\n",
                "    except Exception as e:\n",
                "        logger.exception(f\"[REFINEMENT PROCESS, TRANSFORM AND REFINE SAP] {e}\")\n",
                "\n",
                "def refine_and_transform_descriptor(item, tables_source, edfi_version, sap_pipeline, sap_pipelineType, test_mode):\n",
                "    global districtId, schoolYear\n",
                "    table_path = tables_source + '/' + item\n",
                "\n",
                "    edfi_item = item\n",
                "    logger.info(f\"Processing table: {edfi_item}\")\n",
                "    try:\n",
                "        source_path = f'stage2/Ingested/{table_path}'\n",
                "        \n",
                "        path_dict = oea.parse_path(source_path)  \n",
                "        entity_parent_path = path_dict['entity_parent_path']\n",
                "        sink_general_path = entity_parent_path.replace('Ingested', 'Refined').replace('SAP', f'SAP/analytics') +'/general/' + edfi_item\n",
                "        partitioning = True\n",
                "        if partitioning:\n",
                "            pattern = re.compile(r'DistrictId=.*?/|SchoolYear=.*?/')\n",
                "            sink_general_path = re.sub(pattern, '', sink_general_path)\n",
                "        \n",
                "        df_changes = oea.get_latest_changes(source_path,\n",
                "                                            sink_general_path,\n",
                "                                            filtering_date = 'rundate',\n",
                "                                            primary_key = ['NATURAL_KEY_HASH'],\n",
                "                                            debugMode = False)\n",
                "        if df_changes.count() > 0:\n",
                "            logger.info('[REFINEMENT PROCESS, TRANSFORM AND REFINE DESCRIPTOR] Ed-Fi to Ed-Fi Relationship Model: ' + edfi_item)\n",
                "\n",
                "            current_timestamp = datetime.now()\n",
                "            df_changes = add_metadata_columns(df_changes, \n",
                "                                              overwrite = False,\n",
                "                                              DistrictId = districtId, \n",
                "                                              SchoolYear = schoolYear,\n",
                "                                              LastModifiedDate = current_timestamp,\n",
                "                                              RECORD_VERSION = 1,\n",
                "                                              SUBMISSION_RECORD_IS_ACTIVE = True)\n",
                "                                              \n",
                "            df = sap_to_edfi_client.transform(df = df_changes,\n",
                "                                              schema_name = schema_name,\n",
                "                                              table_name = edfi_item,\n",
                "                                              primary_key = 'NATURAL_KEY_HASH',\n",
                "                                              ext_entity = ext_entity,\n",
                "                                              sink_general_path = sink_general_path,\n",
                "                                              parent_schema_name = None,\n",
                "                                              parent_table_name = None)\n",
                "        else:\n",
                "            logger.info(f'Ed-Fi to Ed-Fi Relationship Model: No updated rows in {source_path} to process.')\n",
                "\n",
                "    except AnalysisException as e:\n",
                "        logger.info(e)\n",
                "\n",
                "\n",
                "def refine_and_explode_data(sap_pipeline, sap_pipelineType, schema_name, tables_source, ext_entity, transform_mode, test_mode, items, sapVersion, edfi_version):\n",
                "    global districtId, schoolYear\n",
                "    if items is None:\n",
                "        items = oea.get_folders(f\"stage2/Ingested/{tables_source}\")\n",
                "    else:\n",
                "        temp_items = set(oea.get_folders(f\"stage2/Ingested/{tables_source}\"))\n",
                "        items = list(temp_items.intersection(items))\n",
                "\n",
                "    with ThreadPoolExecutor(max_workers=8) as tpe:\n",
                "        logger.info('[REFINEMENT REFINE AND EXPLODE SAP] Entered Threadpool')\n",
                "        for item in items:\n",
                "            if item == 'metadata.csv' or item == 'descriptorTables':\n",
                "                logger.info('Ignore Metadata, since this is not a table to be ingested')\n",
                "            else:\n",
                "                if item.lower().endswith('descriptors'):\n",
                "                    tpe.submit(refine_and_transform_descriptor, item, tables_source, edfi_version, sap_pipeline, sap_pipelineType, test_mode)\n",
                "                else:                \n",
                "                    tpe.submit(process_transform_and_refine_SAP_entity, item, tables_source, edfi_version, sap_pipeline, sap_pipelineType, test_mode)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Empty Schema - Utilities"
            ],
            "outputs": []
        },
        {
            "execution_count": 70,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "def upsert_data(df_changes, \n",
                "                metadata,\n",
                "                schema_name, \n",
                "                transform_mode,\n",
                "                table_name,\n",
                "                primary_key,\n",
                "                ext_entity,\n",
                "                sink_general_path,\n",
                "                sink_sensitive_path):\n",
                "        # NOTE: Here using debugging = True so as to circumvent the issue of metadata = []\n",
                "        df_pseudo, df_lookup = oea.pseudonymize(df_changes, \n",
                "                                                metadata,\n",
                "                                                transform_mode,\n",
                "                                                True)\n",
                "                                \n",
                "        df = sap_to_edfi_client.transform(df = df_pseudo,\n",
                "                                              schema_name = schema_name,\n",
                "                                              table_name = table_name,\n",
                "                                              primary_key = 'NATURAL_KEY_HASH',\n",
                "                                              ext_entity = ext_entity,\n",
                "                                              sink_general_path = sink_general_path,\n",
                "                                              parent_schema_name = None,\n",
                "                                              parent_table_name = None)\n",
                "\n",
                "def threaded_task_empty_schema(input_tuple):\n",
                "    item,schema_name,s2r_path,ext_entity,transform_mode,districtId,schoolYear,sap_pipeline,sap_pipelineType = input_tuple\n",
                "    table_name = item\n",
                "    edfi_item = item\n",
                "    metadata = []\n",
                "    try:                       \n",
                "        sink_general_path = f'{s2r_path}/general/{schema_name}/{item}'\n",
                "        sink_sensitive_path = f'{s2r_path}/sensitive/{schema_name}/{item}_lookup'\n",
                "                        \n",
                "        sink_general_path = sap_to_edfi_client.sink_path_cleanup(sink_general_path)\n",
                "        sink_sensitive_path = sap_to_edfi_client.sink_path_cleanup(sink_sensitive_path)\n",
                "        if not oea.path_exists(sink_general_path):\n",
                "            logger.info(f'[REFINEMENT EMPTY SCHEMA DUMPING THREAD] Path does not exist - attempting to create empty data frame - {item}')            \n",
                "            target_schema = copy.deepcopy(sap_to_edfi_client.schemas[table_name])    \n",
                "            df_changes = spark.createDataFrame(data = [],\n",
                "                                                schema = target_schema)\n",
                "            current_timestamp = datetime.now()\n",
                "            df_changes = add_metadata_columns(df_changes,\n",
                "                                                overwrite = True, \n",
                "                                                DistrictId = districtId, \n",
                "                                                SchoolYear = schoolYear,\n",
                "                                                LastModifiedDate = current_timestamp,\n",
                "                                                sap_pipeline = sap_pipeline,\n",
                "                                                sap_pipelineType = sap_pipelineType,\n",
                "                                                RECORD = '1',\n",
                "                                                rundate = '2023-01-01',\n",
                "                                                NATURAL_KEY_HASH = 'PLACEHOLDER',\n",
                "                                                RECORD_HASH = 'PLACEHOLDER',\n",
                "                                                RECORD_VERSION = 1,\n",
                "                                                SUBMISSION_RECORD_IS_ACTIVE = True\n",
                "                                                )\n",
                "                            \n",
                "\n",
                "            if 'id' in df_changes.columns:\n",
                "                upsert_data(df_changes, \n",
                "                            metadata, #NOTE: This is empty list\n",
                "                            schema_name, \n",
                "                            transform_mode,\n",
                "                            table_name,\n",
                "                            'id',\n",
                "                            ext_entity,\n",
                "                            sink_general_path,\n",
                "                            sink_sensitive_path)\n",
                "            else:\n",
                "                pass#logger.info(f'{item} does not have id as primary key - flagged for future')\n",
                "    except Exception as error:\n",
                "        logger.exception(f\"[REFINEMENT EMPTY SCHEMA DUMPING THREAD] {item} {error}\")\n",
                "\n",
                "def dump_empty_schemas(sap_pipeline,\n",
                "                       sap_pipelineType,\n",
                "                       schema_name, \n",
                "                       s2r_path,\n",
                "                       ext_entity,\n",
                "                       transform_mode, \n",
                "                       items = []):\n",
                "    global districtId,schoolYear\n",
                "    if schema_name is None:\n",
                "        schema_name = 'ed-fi'\n",
                "    \n",
                "    with ThreadPoolExecutor(max_workers=12) as tpe:\n",
                "        logger.info('[REFINEMENT EMPTY SCHEMA DUMPING] Entered Threadpool')\n",
                "        tpe.map(threaded_task_empty_schema,[(item,schema_name if not item.lower().endswith('exts') else 'tx',s2r_path,ext_entity,transform_mode,districtId,schoolYear, sap_pipeline,sap_pipelineType) for item in items])\n",
                " \n",
                "\n",
                "def get_non_ext_entities(entities_meta_info):\n",
                "    non_ext_table_names = list()\n",
                "    for entity_meta_info in entities_meta_info:\n",
                "        non_ext_table_names.append(entity_meta_info['resource'].split('/')[-1])\n",
                "    return non_ext_table_names\n",
                "\n",
                "def add_all_empty_tables_to_lake_db(empty_tables_path, schema_name, emptyTables = None, suffix = '_analytics'):\n",
                "    if emptyTables is None:\n",
                "        empty_tables_source = oea.to_url(empty_tables_path)\n",
                "        items = oea.get_folders(empty_tables_source)\n",
                "    else:\n",
                "        items = emptyTables\n",
                "    if schema_name == 'ed-fi':\n",
                "        extension = None\n",
                "    else:\n",
                "        extension = schema_name  \n",
                "    \n",
                "    with ThreadPoolExecutor(max_workers=12) as tpe:\n",
                "        logger.info('[REFINEMENT EMPTY SCHEMA ADD TO LAKE DB] Entered Threadpool')\n",
                "        for item in items:\n",
                "            source_entity_path = empty_tables_path + '/' + item \n",
                "            tpe.submit(add_empty_table_to_lake_db,source_entity_path,False,extension, suffix)\n",
                "\n",
                "def add_empty_table_to_lake_db(source_entity_path, overwrite = False, extension = None, suffix = '_analytics'):\n",
                "        # FIXME: Temporary Fix for Empty Schemas\n",
                "        \"\"\" Adds the given entity as a table (if the table doesn't already exist) to the proper lake db based on the path.\n",
                "            This method will also create the lake db if it doesn't already exist.\n",
                "            eg: add_to_lake_db('stage2/Ingested/contoso_sis/v0.6/students')\n",
                "\n",
                "            Note that a spark db that points to source data in the delta format can't be queried via SQL serverless pool. More info here: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#delta-lake\n",
                "        \"\"\"\n",
                "        source_dict = oea.parse_path(source_entity_path)\n",
                "        if '/emptySchemas/' in source_entity_path:\n",
                "            try:\n",
                "                base_db_name = source_dict['ldb_name']\n",
                "                base_table_name = source_dict['entity']\n",
                "                for submission_type in [suffix]:     \n",
                "                    if extension is not None:\n",
                "                        if not(extension.startswith('_')):\n",
                "                            extension = '_' + extension\n",
                "                        source_dict['entity'] = base_table_name + str(extension)\n",
                "                    \n",
                "                    db_name = base_db_name + submission_type\n",
                "\n",
                "                    logger.info(f\"[REFINEMENT EMPTY SCHEMA ADD TO LAKE DB] Adding: Lake DB: {db_name}; Table: {source_dict['entity']}\")\n",
                "                    spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\n",
                "                    if overwrite:\n",
                "                        spark.sql(f\"drop table if exists {db_name}.{source_dict['entity']}\")\n",
                "\n",
                "                    spark.sql(f\"create table if not exists {db_name}.{source_dict['entity']} using DELTA location '{oea.to_url(source_dict['entity_path'])}'\")\n",
                "            except Exception as error:\n",
                "                logger.error(f'[REFINEMENT EMPTY SCHEMA ADD TO LAKE DB] {error}')"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Main Tables"
            ],
            "outputs": []
        },
        {
            "execution_count": 71,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "def return_sap_entities(sap_pipeline, sap_pipelineType):\n",
                "    if sap_pipeline == 'TEA':\n",
                "        if sap_pipelineType == 'PEIMS_FALL':\n",
                "            return ['YHROHPM04', 'YHROHPM07', 'YHROHPM08', 'YHROHPM09', 'YHROHPM10', 'YFMOHPEIM']\n",
                "        elif sap_pipelineType == 'PEIMS_MIDYR':\n",
                "            return ['YFIOHPEIM']\n",
                "        elif sap_pipelineType == 'PEIMS_EXYR':\n",
                "            return ['YHROHPM04']\n",
                "        elif sap_pipelineType == 'TSDS_ECDS_KG':\n",
                "            return ['YHROHPM03']\n",
                "        elif sap_pipelineType == 'TSDS_CLASS_ROSTER_FALL':\n",
                "            return ['YHROHPM05']\n",
                "        elif sap_pipelineType == 'TSDS_ECDS_PK':\n",
                "            return ['YHROHPM02']\n",
                "    return None"
            ],
            "outputs": []
        },
        {
            "execution_count": 72,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# sap_pipeline = \"peims-submissions\"\n",
                "# FIXME: Ed-Fi pseudonymization metadata is NOT being used in here \n",
                "metadata = []#oea.get_metadata_from_path(path = f'stage1/Transactional/SAP/pipeline={sap_pipeline}/{sapVersion}')\n",
                "\n",
                "schema_name = 'ed-fi'\n",
                "ext_entity = 'TX'\n",
                "natural_upsert_mode = True\n",
                "items = return_sap_entities(sap_pipeline, sap_pipelineType)"
            ],
            "outputs": []
        },
        {
            "execution_count": 73,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from datetime import datetime\n",
                "descriptors_base_path = f'stage2/Ingested/SAP/descriptorTables/{sapVersion}/DistrictId={districtId}/SchoolYear={schoolYear}'\n",
                "column_mapping_file_path = f\"stage1/Transactional/SAP/metadata-assets/ingestion-mappings.json\"\n",
                "    \n",
                "column_mappings = initialize_sap_pipeline_vars(sapVersion = sapVersion,\n",
                "                                    districtId = districtId,\n",
                "                                    schoolYear = schoolYear,\n",
                "                                    base_path = descriptors_base_path,\n",
                "                                    column_mapping_file_path = column_mapping_file_path)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### PEIMS"
            ],
            "outputs": []
        },
        {
            "execution_count": 74,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "test_mode = False\n",
                "transform_mode = True\n",
                "edfi_version = apiVersion\n",
                "\n",
                "if etlProcessing:\n",
                "    if sap_pipeline == 'TEA' and (sap_pipelineType.lower().startswith('peims') or sap_pipelineType.lower().startswith('tsds')):\n",
                "        tables_source = f'SAP/pipeline={sap_pipeline}/pipelineType={sap_pipelineType}/{sapVersion}/DistrictId={districtId}/SchoolYear={schoolYear}'\n",
                "        sap_to_edfi_client = SAPToEdFiRefine(workspace = workspace, \n",
                "                                            oea = oea, \n",
                "                                            spark = spark,\n",
                "                                            sap_oea_utils = oea_utils,\n",
                "                                            sap_process_client = sap_process_client,\n",
                "                                            logger = logger,\n",
                "                                            schema_gen = schema_gen, \n",
                "                                            moduleName = moduleName, \n",
                "                                            authUrl = authUrl, \n",
                "                                            swaggerUrl = swaggerUrl, \n",
                "                                            dataManagementUrl = dataManagementUrl, \n",
                "                                            changeQueriesUrl = changeQueriesUrl, \n",
                "                                            dependenciesUrl = dependenciesUrl, \n",
                "                                            apiVersion = apiVersion, \n",
                "                                            schoolYear = schoolYear,\n",
                "                                            districtId = districtId, \n",
                "                                            test_mode = False,\n",
                "                                            pipelineExecutionId = pipelineExecutionId,\n",
                "                                            error_logger = error_logger,\n",
                "                                            natural_upsert_mode = natural_upsert_mode,\n",
                "                                            sap_essential_columns = sap_essential_columns)\n",
                "        sap_to_edfi_client.set_params(params = {'sap_pipeline': sap_pipeline,\n",
                "                                                'sap_pipelineType': sap_pipelineType})\n",
                "\n",
                "        df = refine_and_explode_data(sap_pipeline = sap_pipeline, \n",
                "                                    sap_pipelineType = sap_pipelineType,\n",
                "                                    schema_name = schema_name, \n",
                "                                    tables_source = tables_source,\n",
                "                                    ext_entity = ext_entity,\n",
                "                                    transform_mode = transform_mode, \n",
                "                                    test_mode = test_mode,\n",
                "                                    items = items,\n",
                "                                    sapVersion = sapVersion,\n",
                "                                    edfi_version = edfi_version) "
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Descriptor Tables"
            ],
            "outputs": []
        },
        {
            "execution_count": 28,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "if prepareSAPMetaData: # and sap_pipeline in ['All', 'analytics', 'validation']:\n",
                "    for edfi_extension in ['ed-fi', 'TX', 'tpdm']:\n",
                "        tables_source = f'SAP/descriptorTables/{sapVersion}/DistrictId={districtId}/SchoolYear={schoolYear}/{edfi_extension}'\n",
                "\n",
                "        test_mode = False\n",
                "        transform_mode = True\n",
                "        edfi_version = apiVersion\n",
                "\n",
                "        swagger_url = swaggerUrl = edfi_api_manager.get_referenced_url('Descriptors')\n",
                "        oea_utils = schema_gen = SAPOpenAPIUtilChild(swagger_url)\n",
                "        oea_utils.create_definitions()\n",
                "        schemas = schema_gen.create_spark_schemas()\n",
                "\n",
                "        sap_to_edfi_client = SAPToEdFiRefine( workspace = workspace, \n",
                "                                                oea = oea, \n",
                "                                                spark = spark,\n",
                "                                                sap_oea_utils = oea_utils,\n",
                "                                                sap_process_client = sap_process_client,\n",
                "                                                logger = logger,\n",
                "                                                schema_gen = schema_gen, \n",
                "                                                moduleName = moduleName, \n",
                "                                                authUrl = authUrl, \n",
                "                                                swaggerUrl = swaggerUrl, \n",
                "                                                dataManagementUrl = dataManagementUrl, \n",
                "                                                changeQueriesUrl = changeQueriesUrl, \n",
                "                                                dependenciesUrl = dependenciesUrl, \n",
                "                                                apiVersion = apiVersion, \n",
                "                                                schoolYear = schoolYear,\n",
                "                                                districtId = districtId, \n",
                "                                                test_mode = False,\n",
                "                                                pipelineExecutionId = pipelineExecutionId,\n",
                "                                                error_logger = error_logger,\n",
                "                                                natural_upsert_mode = natural_upsert_mode,\n",
                "                                                sap_essential_columns = sap_essential_columns)\n",
                "\n",
                "        sap_to_edfi_client.set_params(params = {'sap_pipeline': sap_pipeline,\n",
                "                                                'sap_pipelineType': sap_pipelineType})\n",
                "\n",
                "        df = refine_and_explode_data(sap_pipeline = sap_pipeline,\n",
                "                                    sap_pipelineType = sap_pipelineType,\n",
                "                                        schema_name = schema_name, \n",
                "                                        tables_source = tables_source,\n",
                "                                        ext_entity = ext_entity,\n",
                "                                        transform_mode = transform_mode, \n",
                "                                        test_mode = test_mode,\n",
                "                                        items = None,#items,\n",
                "                                        sapVersion = sapVersion,\n",
                "                                        edfi_version = edfi_version)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Entity Level Logs"
            ],
            "outputs": []
        },
        {
            "execution_count": 29,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "if error_logger.entity_logs != []:\n",
                "    df = error_logger.create_spark_df('entity')\n",
                "    error_logger.write_logs_to_delta_lake(df = df, \n",
                "                                log_type = 'entity',\n",
                "                                destination_url = error_logger.to_logs_url('etl-logs/log_type=entity'))\n",
                "    error_logger.add_etl_logs_to_lake_db(db_name = f'ldb_{workspace}_sap_etl_logs',\n",
                "                                        logs_base_path = 'etl-logs',\n",
                "                                        log_type = 'entity',\n",
                "                                        overwrite = True)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Empty Schemas"
            ],
            "outputs": []
        },
        {
            "execution_count": 30,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# FIXME: Undergoing Major Changes\n",
                "# prepareSAPMetaData = True\n",
                "transform_mode = True\n",
                "\n",
                "if prepareSAPMetaData: # and sap_pipeline in ['All', 'analytics']:\n",
                "    if sap_pipeline == 'All':\n",
                "        sap_pipelines = ['peims-submissions', 'tsds_crf-submissions', 'analytics']\n",
                "    else:\n",
                "        sap_pipelines = [sap_pipeline]\n",
                "\n",
                "    edfiAPIClient = edfi = EdFiClient(workspace = workspace, \n",
                "                                    kvName = kvName, #NOTE: Default to None \n",
                "                                    moduleName = moduleName, \n",
                "                                    authUrl = authUrl, \n",
                "                                    dataManagementUrl = dataManagementUrl, \n",
                "                                    changeQueriesUrl = changeQueriesUrl, \n",
                "                                    dependenciesUrl = dependenciesUrl, \n",
                "                                    apiVersion = apiVersion, \n",
                "                                    batchLimit = batchLimit, \n",
                "                                    minChangeVer = minChangeVer, \n",
                "                                    maxChangeVer = maxChangeVer,\n",
                "                                    schoolYear = schoolYear,\n",
                "                                    districtId = districtId,\n",
                "                                    kvSecret_clientId = kvSecret_clientId,\n",
                "                                    kvSecret_clientSecret = kvSecret_clientSecret,\n",
                "                                    retry_strategy = None, \n",
                "                                    threadMode = False, \n",
                "                                    devMode = False)\n",
                "    \n",
                "    entities_meta_info = edfiAPIClient.getEntities()#[0]['resource']\n",
                "    non_ext_table_names = get_non_ext_entities(entities_meta_info) #TODO: To Be Reviewed\n",
                "    non_ext_table_names = ['schoolYearTypes'] + non_ext_table_names\n",
                "\n",
                "    for swagger_resource_type in ['Resources', 'Descriptors']:\n",
                "        swagger_url = swaggerUrl = edfi_api_manager.get_referenced_url(swagger_resource_type)\n",
                "        oea_utils = schema_gen = SAPOpenAPIUtilChild(swagger_url)\n",
                "        oea_utils.create_definitions()\n",
                "        schemas = schema_gen.create_spark_schemas()\n",
                "        \n",
                "        sap_to_edfi_client = SAPToEdFiRefine(workspace = workspace, \n",
                "                                                    oea = oea, \n",
                "                                                    spark = spark,\n",
                "                                                    sap_oea_utils = oea_utils,\n",
                "                                                    sap_process_client = sap_process_client,\n",
                "                                                    logger = logger,\n",
                "                                                    schema_gen = schema_gen, \n",
                "                                                    moduleName = moduleName, \n",
                "                                                    authUrl = authUrl, \n",
                "                                                    swaggerUrl = swaggerUrl, \n",
                "                                                    dataManagementUrl = dataManagementUrl, \n",
                "                                                    changeQueriesUrl = changeQueriesUrl, \n",
                "                                                    dependenciesUrl = dependenciesUrl, \n",
                "                                                    apiVersion = apiVersion, \n",
                "                                                    schoolYear = schoolYear,\n",
                "                                                    districtId = districtId, \n",
                "                                                    test_mode = False,\n",
                "                                                    pipelineExecutionId = pipelineExecutionId,\n",
                "                                                    error_logger = error_logger,\n",
                "                                                    natural_upsert_mode = natural_upsert_mode)\n",
                "        sap_to_edfi_client.set_params(params = {'sap_pipeline': sap_pipeline,\n",
                "                                                'sap_pipelineType': sap_pipelineType})\n",
                "\n",
                "        # non_ext_table_names = sap_to_edfi_client.return_non_ext_tables()  \n",
                "        if swagger_resource_type == 'Resources':\n",
                "            transform_items = [item for item in non_ext_table_names if not(item.lower().endswith('descriptors'))]\n",
                "        elif swagger_resource_type == 'Descriptors':\n",
                "            transform_items = [item for item in non_ext_table_names if item.lower().endswith('descriptors')]\n",
                "      \n",
                "        s2r_path = f'stage2/Refined/SAP/analytics/emptySchemas'\n",
                "\n",
                "        dump_empty_schemas(sap_pipeline = None, \n",
                "                        sap_pipelineType = None,#NOTE: To Be Reviewed\n",
                "                        schema_name = schema_name , \n",
                "                        s2r_path = s2r_path,\n",
                "                        ext_entity = ext_entity,\n",
                "                        transform_mode = transform_mode, \n",
                "                        items = transform_items)"
            ],
            "outputs": []
        },
        {
            "execution_count": 31,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "if prepareSAPMetaData:\n",
                "    # FIXME: Under Review\n",
                "    if True:#sap_pipeline == 'analytics' or sap_pipeline == 'All':\n",
                "        tables_source = f'SAP/pipeline={sap_pipeline}/{sapVersion}/DistrictId={districtId}/SchoolYear={schoolYear}'\n",
                "        \n",
                "        edfi_mainTables = oea.get_folders(f\"stage2/Refined/SAP/analytics/pipeline={sap_pipeline}/pipelineType={sap_pipelineType}/{sapVersion}/general/ed-fi\")#oea.get_folders(f\"stage2/Ingested/{tables_source}\")\n",
                "        tx_mainTables = oea.get_folders(f\"stage2/Refined/SAP/analytics/pipeline={sap_pipeline}/pipelineType={sap_pipelineType}/{sapVersion}/general/tx\")#oea.get_folders(f\"stage2/Ingested/{tables_source}\") \n",
                "        mainTables = edfi_mainTables + tx_mainTables\n",
                "        # if mainTables != []:\n",
                "        #    mainTables = [sap_to_edfi_complex.get(item) for item in mainTables if item != 'descriptorTables']\n",
                "        \n",
                "        txDescriptorTables = [sap_to_edfi_complex.get(item, item) for item in oea.get_folders(f\"stage2/Ingested/SAP/descriptorTables/{sapVersion}/DistrictId={districtId}/SchoolYear={schoolYear}/TX\") if item != 'descriptorTables']\n",
                "        edfiDescriptorTables = [sap_to_edfi_complex.get(item, item) for item in oea.get_folders(f\"stage2/Ingested/SAP/descriptorTables/{sapVersion}/DistrictId={districtId}/SchoolYear={schoolYear}/ed-fi\") if item != 'descriptorTables']\n",
                "        tpdmDescriptorTables = [sap_to_edfi_complex.get(item, item) for item in oea.get_folders(f\"stage2/Ingested/SAP/descriptorTables/{sapVersion}/DistrictId={districtId}/SchoolYear={schoolYear}/tpdm\") if item != 'descriptorTables']\n",
                "        descriptorTables = txDescriptorTables + edfiDescriptorTables + tpdmDescriptorTables\n",
                "\n",
                "        edfi_emptyTables = oea.get_folders('stage2/Refined/SAP/analytics/emptySchemas/general/ed-fi')\n",
                "        edfi_emptyTables = sap_to_edfi_client.non_common_elements(edfi_emptyTables, \n",
                "                                                             mainTables + descriptorTables)\n",
                "        tx_emptyTables = oea.get_folders('stage2/Refined/SAP/analytics/emptySchemas/general/tx')\n",
                "        tx_emptyTables = sap_to_edfi_client.non_common_elements(tx_emptyTables, \n",
                "                                                             mainTables + descriptorTables)\n",
                "\n",
                "        emptyTables_path = f'stage2/Refined/SAP/analytics/emptySchemas/general/ed-fi'\n",
                "        add_all_empty_tables_to_lake_db(emptyTables_path, 'ed-fi', edfi_emptyTables, pipeline_suffix_mappings[sap_pipelineType])\n",
                "\n",
                "        emptyTables_path = f'stage2/Refined/SAP/analytics/emptySchemas/general/tx'\n",
                "        add_all_empty_tables_to_lake_db(emptyTables_path, 'tx', tx_emptyTables, pipeline_suffix_mappings[sap_pipelineType])"
            ],
            "outputs": []
        }
    ]
}