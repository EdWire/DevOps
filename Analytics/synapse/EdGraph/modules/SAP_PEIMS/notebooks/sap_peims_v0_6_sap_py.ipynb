{
    "nbformat": 4,
    "nbformat_minor": 2,
    "metadata": {
        "save_output": true,
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "import pyspark\n",
                "\n",
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql import DataFrame\n",
                "from pyspark.sql.utils import AnalysisException\n",
                "from pyspark.sql.types import StringType, StructType, StructField, IntegerType\n",
                "\n",
                "from pyspark.sql.functions import col, substring, regexp_extract, split, lit, struct, to_date, from_unixtime, date_format\n",
                "from pyspark.sql.functions import create_map, lit, when, array, coalesce, concat_ws\n",
                "from pyspark.sql.functions import collect_list, create_map, lit, struct, array, concat\n",
                "from pyspark.sql.functions import expr\n",
                "from pyspark.sql.types import DecimalType\n",
                "\n",
                "from pyspark.sql import functions as F\n",
                "from pyspark.sql.functions import hash\n",
                "import pyspark.sql.functions as f\n",
                "from pyspark.sql.functions import udf\n",
                "from concurrent.futures import ThreadPoolExecutor\n",
                "\n",
                "import json\n",
                "import os\n",
                "import pandas as pd\n",
                "import re\n",
                "import uuid\n",
                "\n",
                "import copy\n",
                "from itertools import chain\n",
                "from datetime import datetime\n",
                "\n",
                "global_df_test = None\n",
                "df_staffs = None\n",
                "schema_name = None"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "%run OEA/modules/Ed-Fi/v0.7/src/utilities/edfi_v0_7_edfi_py"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "class SAPEdFiOEAChild(EdFiOEAChild):\n",
                "    \"\"\" \n",
                "    NOTE: This class inherits features from the base class OEA and therefore,\n",
                "    should be created / executed after running the notebook OEA_py\n",
                "    \"\"\"\n",
                "    def __init__(self, workspace='dev', logging_level=logging.INFO, storage_account=None, keyvault=None, timezone=None, sap_pipeline = None, sap_pipelineType = None):\n",
                "        # Call the base class constructor to initialize inherited attributes\n",
                "        super().__init__(workspace, logging_level, storage_account, keyvault, timezone)\n",
                "        spark = SparkSession.builder.config(\"spark.kryoserializer.buffer.max\", \"3000m\").getOrCreate()\n",
                "        spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # more info here: https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/optimize-write-for-apache-spark\n",
                "        self.sap_pipeline = sap_pipeline\n",
                "        self.sap_pipelineType = sap_pipelineType\n",
                "    \n",
                "    def get_latest_changes(self, source_path, sink_path, filtering_date = 'rundate',primary_key = ['id'],debugMode = False):\n",
                "        return super().get_latest_changes(source_path = source_path,\n",
                "                                          sink_path = sink_path,\n",
                "                                          filtering_date = filtering_date,\n",
                "                                          primary_key = primary_key,\n",
                "                                          debugMode = debugMode)\n",
                "\n",
                "    def process(self, source_path,foreach_batch_function, batch_type, natural_key = None,landingDateTimeFormat = 'yyyyMMddHHmmss',options={}):\n",
                "        # FIXME: 2024-02-08 (Under Dev for high granularity and de-dup)\n",
                "        \"\"\" This simplifies the process of using structured streaming when processing transformations.\n",
                "            Provide a source_path and a function that receives a dataframe to work with (which will be a dataframe with data from the given source_path).\n",
                "            Use it like this...\n",
                "            def refine_contoso_dataset(df_source):\n",
                "                metadata = oea.get_metadata_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/gene/v0.7dev/modules/module_catalog/Student_and_School_Data_Systems/metadata.csv')\n",
                "                df_pseudo, df_lookup = oea.pseudonymize(df, metadata['studentattendance'])\n",
                "                oea.upsert(df_pseudo, 'stage2/Refined/contoso_sis/v0.1/studentattendance/general')\n",
                "                oea.upsert(df_lookup, 'stage2/Refined/contoso_sis/v0.1/studentattendance/sensitive')\n",
                "            oea.process('stage2/Ingested/contoso_sis/v0.1/studentattendance', refine_contoso_dataset)             \n",
                "        \"\"\"\n",
                "        if not self.path_exists(source_path):\n",
                "            raise ValueError(f'The given path does not exist: {source_path} (which resolves to: {self.to_url(source_path)})') \n",
                "\n",
                "        if natural_key is not None:\n",
                "            natural_key_expr = [f.col(key_component).cast('string') for key_component in natural_key]\n",
                "        \n",
                "        def wrapped_function(df, batch_id):\n",
                "            current_timestamp = datetime.now()\n",
                "            df = df.withColumn('LastModifiedDate', F.lit(current_timestamp))\n",
                "            df = df.withColumn(\"rundate\", F.to_timestamp(F.col(\"rundate\").cast('string'), landingDateTimeFormat))\n",
                "            df = df.withColumn('sap_pipeline', F.lit(self.sap_pipeline))\n",
                "            df = df.withColumn('sap_pipelineType', F.lit(self.sap_pipelineType))\n",
                "            # df = df.orderBy(F.col(\"rundate\").desc()).dropDuplicates([\"id\"])\n",
                "            # FIXME: 2024-03-13 TEMP FIX For amount precision\n",
                "            if 'AMOUNT' in df.columns and 'PU_FC_SUB2' in df.columns:\n",
                "                df = df.withColumn(\"AMOUNT\", df[\"AMOUNT\"].cast(DecimalType(precision=20, scale=2)).cast('string'))\n",
                "            if natural_key is not None:\n",
                "                df = df.withColumn(\"NATURAL_KEY_HASH\",F.sha2(F.concat(*[F.concat(F.coalesce(column, F.lit('')), F.lit('_')) for column in natural_key_expr]), 256))\n",
                "            if batch_type != 'delete':\n",
                "                df = df.withColumn(\"rowIsActive\", F.lit(True))\n",
                "            \n",
                "            df.persist() # cache the df so it doesn't get read in multiple times when we write to multiple destinations. See: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#foreachbatch\n",
                "            foreach_batch_function(df, batch_id)\n",
                "            df.unpersist()\n",
                "\n",
                "        spark.sql(\"set spark.sql.streaming.schemaInference=true\")\n",
                "        #source_path = source_path.replace(':', '\\:')\n",
                "        print(f\"source_path is: {source_path}\")\n",
                "        streaming_df = spark.readStream.load(self.to_url(source_path), **options)\n",
                "        streaming_df = streaming_df.withColumn('stage1_source_url', F.input_file_name())\n",
                "\n",
                "        # for more info on append vs complete vs update modes for structured streaming: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#basic-concepts\n",
                "        query = streaming_df.writeStream.format('delta').outputMode('append').trigger(once=True).option('checkpointLocation', self.to_url(source_path) + '/_checkpoints').foreachBatch(wrapped_function).start()\n",
                "        query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\n",
                "        number_of_new_inbound_rows = query.lastProgress[\"numInputRows\"]\n",
                "        logger.info(f'[SAPEDFIOEACHILD INGESTION STRUCTURED STREAMING PROCESS]: Number of new inbound rows processed: {number_of_new_inbound_rows}')\n",
                "        logger.debug(query.lastProgress)\n",
                "        return number_of_new_inbound_rows\n",
                "    \n",
                "    def add_to_lake_db(self, source_entity_path, overwrite = False, extension = None, table_type = 'non-empty-non-descriptor', pipeline_type = None):\n",
                "        \"\"\" Adds the given entity as a table (if the table doesn't already exist) to the proper lake db based on the path.\n",
                "            This method will also create the lake db if it doesn't already exist.\n",
                "            eg: add_to_lake_db('stage2/Ingested/contoso_sis/v0.1/students')\n",
                "\n",
                "            Note that a spark db that points to source data in the delta format can't be queried via SQL serverless pool. More info here: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#delta-lake\n",
                "        \"\"\"\n",
                "        source_dict = self.parse_path(source_entity_path)\n",
                "        db_name = source_dict['ldb_name']\n",
                "        if ('/emptySchemas/' not in source_entity_path and table_type == 'non-empty-non-descriptor') or (table_type == 'ingested-intermediate'):\n",
                "            if extension is not None:\n",
                "                if not(extension.startswith('_')):\n",
                "                    extension = '_' + extension\n",
                "                source_dict['entity'] = source_dict['entity'] + str(extension)\n",
                "            \n",
                "            spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\n",
                "            if overwrite:\n",
                "                spark.sql(f\"drop table if exists {db_name}.{source_dict['entity']}\")\n",
                "\n",
                "            spark.sql(f\"create table if not exists {db_name}.{source_dict['entity']} using DELTA location '{self.to_url(source_dict['entity_path'])}'\")\n",
                "        elif table_type == 'non-empty-descriptor':\n",
                "            target_db_names = [db_name]            \n",
                "            for target_db_name in target_db_names:\n",
                "                spark.sql(f'CREATE DATABASE IF NOT EXISTS {target_db_name}')\n",
                "                if overwrite:\n",
                "                    spark.sql(f\"drop table if exists {target_db_name}.{source_dict['entity']}\")\n",
                "\n",
                "                spark.sql(f\"create table if not exists {target_db_name}.{source_dict['entity']} using DELTA location '{self.to_url(source_dict['entity_path'])}'\")\n",
                "        \n",
                "    def merge_deletes_into_delta_lake(self, df, destination_path, func_enabled = False):\n",
                "        df = df.cache()\n",
                "        entity_name = destination_path.split('/')[-1]\n",
                "        if not func_enabled:\n",
                "            return 0\n",
                "        else:\n",
                "            logger.info('[SAPEDFIOEACHILD REFINEMENT SUBMISSION DELETES] MERGING DELETES BEFORE JOIN BASED COMPARSIONS / DELTA COMPARISONS / UPSERT IS ENABLED')                 \n",
                "            df_destination = self.load(destination_path)\n",
                "            if 'SUBMISSION_RECORD_IS_ACTIVE' in df_destination.columns:   \n",
                "                df_destination = df_destination.drop('SUBMISSION_RECORD_IS_ACTIVE')\n",
                "            df.createOrReplaceTempView('temp_vw_df_source_table')\n",
                "            df_destination.createOrReplaceTempView('temp_vw_df_destination_table')\n",
                "\n",
                "            query = f'select max(rundate) maxdatetime from temp_vw_df_source_table'\n",
                "            maxdatetime = spark.sql(query).first()['maxdatetime']\n",
                "            query = f\"\"\"SELECT temp_vw_df_destination_table.*,\n",
                "                        CASE \n",
                "                            WHEN temp_vw_df_source_table.NATURAL_KEY_HASH IS NULL THEN False \n",
                "                            ELSE True \n",
                "                        END AS SUBMISSION_RECORD_IS_ACTIVE\n",
                "                        FROM temp_vw_df_destination_table  \n",
                "                        LEFT JOIN temp_vw_df_source_table\n",
                "                            ON temp_vw_df_destination_table.NATURAL_KEY_HASH = temp_vw_df_source_table.NATURAL_KEY_HASH\n",
                "                    \"\"\"\n",
                "            df_joined = spark.sql(query)\n",
                "            df_joined = df_joined.filter(df_joined[\"SUBMISSION_RECORD_IS_ACTIVE\"] == False)\n",
                "            df_joined.createOrReplaceTempView('temp_vw_df_inactive_records_staging')\n",
                "            \n",
                "            query = f\"\"\"WITH maxPipelineExecutionIdCTE AS\n",
                "                (\n",
                "                    SELECT pipelineExecutionId\n",
                "                    FROM ldb_{self.workspace}_sap_etl_logs.etlsubmissionslogs\n",
                "                    WHERE entity_name = '{entity_name}'\n",
                "                      AND operation_type != 'delete'\n",
                "                    ORDER BY start_time desc, end_time desc\n",
                "                    LIMIT 1\n",
                "                )\n",
                "                SELECT DISTINCT edfi_id,\n",
                "                                edfi_location,\n",
                "                                NATURAL_KEY_HASH \n",
                "                FROM ldb_{self.workspace}_sap_etl_logs.etlsubmissionslogs\n",
                "                INNER JOIN maxPipelineExecutionIdCTE \n",
                "                    ON etlsubmissionslogs.pipelineExecutionId = maxPipelineExecutionIdCTE.pipelineExecutionId\n",
                "                WHERE entity_name = '{entity_name}'\n",
                "                  AND response_status_code LIKE '2%' \n",
                "                  AND response_status_code NOT LIKE '204'\n",
                "                \"\"\"\n",
                "            df_last_submission_logs = spark.sql(query)\n",
                "            df_last_submission_logs.createOrReplaceTempView('temp_vw_df_last_submission_logs')\n",
                "            \n",
                "            query = \"\"\"SELECT temp_vw_df_last_submission_logs.NATURAL_KEY_HASH\n",
                "                        FROM temp_vw_df_last_submission_logs\n",
                "                        INNER JOIN  temp_vw_df_inactive_records_staging\n",
                "                            ON  temp_vw_df_last_submission_logs.NATURAL_KEY_HASH = temp_vw_df_inactive_records_staging.NATURAL_KEY_HASH\n",
                "                    \"\"\"\n",
                "            df_final = spark.sql(query)\n",
                "            df_final = df_final.withColumn('sap_pipeline', F.lit(self.sap_pipeline))\n",
                "            df_final = df_final.withColumn('sap_pipelineType', F.lit(self.sap_pipelineType))\n",
                "            df_final = df_final.withColumn('rundate', F.lit(maxdatetime))\n",
                "            #df_final = df_final.withColumn('LastModifiedDateTime', F.lit(datetime.now()))\n",
                "            df_final = df_final.withColumn('SUBMISSION_RECORD_IS_ACTIVE', F.lit(False))\n",
                "\n",
                "            primary_key = ['NATURAL_KEY_HASH', 'sap_pipeline','sap_pipelineType']\n",
                "            update_cols = {\"sink.rundate\": \"updates.rundate\", \n",
                "                           #\"sink.LastModifiedDateTime\": \"updates.LastModifiedDateTime\", \n",
                "                           \"sink.SUBMISSION_RECORD_IS_ACTIVE\": \"updates.SUBMISSION_RECORD_IS_ACTIVE\"}\n",
                "            destination_url = self.to_url(destination_path)\n",
                "            pk_statement = self.return_pk_statement(primary_key)\n",
                "            logger.info(f\"[SAPEDFIOEACHILD REFINEMENT SUBMISSION DELETES] NUMBER OF RECORDS TO BE MARKED AS INACTIVE FOR SUBMISSION - {df_final.count()}\")\n",
                "            if DeltaTable.isDeltaTable(spark, destination_url):\n",
                "                logger.info('[SAPEDFIOEACHILD REFINEMENT SUBMISSION DELETES] DELETE MERGES')\n",
                "                delta_table_sink = DeltaTable.forPath(spark, destination_url)\n",
                "                delta_table_sink.alias('sink').merge(df_final.alias('updates'), pk_statement).whenMatchedUpdate(set = update_cols).execute()# .whenNotMatchedInsert(values = insert_cols).execute()\n",
                "            else:\n",
                "                logger.error(f'Invalid stage 3 delta location for the item - {entity_name}')\n",
                "            return 1\n",
                ""
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "class SAPOpenAPIUtilChild(OpenAPIUtil):\n",
                "    def __init__(self, swagger_url):\n",
                "        super().__init__(swagger_url)\n",
                "        self.pluralization_mappings = dict()\n",
                "\n",
                "    def depluralize(self, noun):\n",
                "        if noun == 'people':\n",
                "            return 'person'\n",
                "        if noun == 'surveys':\n",
                "            return 'survey'\n",
                "        if re.search('[sxz]es$', noun):\n",
                "            return re.sub('es$', '', noun)\n",
                "        if re.search('ies$', noun):\n",
                "            return re.sub('ies$', 'y', noun)\n",
                "        if re.search('s$', noun):\n",
                "            return re.sub('s$', '', noun)\n",
                "        return noun\n",
                "    \n",
                "    def create_definitions(self):\n",
                "        self.swagger_json['definitions'] = self.swagger_json.get('definitions', self.swagger_json['components']['schemas'])\n",
                "        for entity in self.swagger_json['definitions']:\n",
                "            properties = self.swagger_json['definitions'][entity]['properties']\n",
                "            table_name = entity.split('_')[-1]\n",
                "            table_schema = {}\n",
                "\n",
                "            for prop in properties:\n",
                "                if 'description' in properties[prop].keys():\n",
                "                    properties[prop].pop('description')\n",
                "                field_info = properties[prop]\n",
                "                if 'required' in self.swagger_json['definitions'][entity].keys():\n",
                "                    field_info['required'] = True if prop in self.swagger_json['definitions'][entity]['required'] else False\n",
                "                else:\n",
                "                    field_info['required'] = False\n",
                "                field_info['table_name'] = entity.split('_')[-1]\n",
                "                field_info['column_name'] = prop\n",
                "                if 'x-Ed-Fi-pseudonymization' in field_info:\n",
                "                    field_info['pseudonymization'] = field_info['x-Ed-Fi-pseudonymization']\n",
                "                    field_info.pop('x-Ed-Fi-pseudonymization')\n",
                "                for header in [x for x in self.metadata_headers if x not in field_info] : field_info[header] = None\n",
                "                table_schema[prop] = field_info\n",
                "\n",
                "            self.definitions[table_name] = table_schema\n",
                "        self.tables = [x for x in self.definitions.keys()]\n",
                "    \n",
                "    def create_spark_schemas_from_definitions(self):\n",
                "        for entity in self.dependency_order:\n",
                "            table_schema = self.definitions[entity]\n",
                "            spark_schema = []\n",
                "            if(entity == 'localEducationAgencyReference'):\n",
                "                print(entity)\n",
                "            for col_name in table_schema:\n",
                "                col_metadata = {}\n",
                "                if('pseudonymization' in table_schema[col_name]): col_metadata['pseudonymization'] = table_schema[col_name]['pseudonymization']\n",
                "                if('x-Ed-Fi-isIdentity' in table_schema[col_name]): col_metadata['x-Ed-Fi-isIdentity'] = table_schema[col_name]['x-Ed-Fi-isIdentity']\n",
                "                \n",
                "                col_metadata['required'] = table_schema[col_name]['required']\n",
                "                referenced_table = self.get_reference(table_schema[col_name])\n",
                "                if table_schema[col_name]['type'] == 'array':\n",
                "                    datatype = ArrayType(self.schemas[self.pluralize(referenced_table)])\n",
                "                    if('x-Ed-Fi-explode' in table_schema[col_name]):\n",
                "                        col_metadata['x-Ed-Fi-explode'] = table_schema[col_name]['x-Ed-Fi-explode']\n",
                "                elif table_schema[col_name]['$ref'] != None:\n",
                "                    datatype = self.schemas[self.pluralize(referenced_table)]\n",
                "                    if('x-Ed-Fi-fields-to-pluck' in table_schema[col_name]):\n",
                "                        col_metadata['x-Ed-Fi-fields-to-pluck'] = table_schema[col_name]['x-Ed-Fi-fields-to-pluck']\n",
                "                else:\n",
                "                    datatype = self.get_data_type(table_schema[col_name]['type'], table_schema[col_name]['format'])\n",
                "                col_spark_schema = StructField(col_name, datatype, not(table_schema[col_name]['required']))\n",
                "                col_spark_schema.metadata = col_metadata\n",
                "                spark_schema.append(col_spark_schema)\n",
                "            self.schemas[self.pluralize(entity)] = StructType(spark_schema)\n",
                "            self.pluralization_mappings[self.pluralize(entity)] = entity"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "class SAPUtilities:\n",
                "    def __init__(self, spark, oea, sap_essential_columns = None):\n",
                "        self.spark = spark\n",
                "        self.oea = oea\n",
                "        self.sap_essential_columns = sap_essential_columns if sap_essential_columns is not None else ['DistrictId', 'SchoolYear', 'sap_pipeline', 'sap_pipelineType', 'lakeId', 'validationRecordId', 'LastModifiedDate', 'rundate', 'stage1_source_url', 'RECORD', 'NATURAL_KEY_HASH', 'RECORD_HASH', 'RECORD_VERSION']\n",
                "    \n",
                "    ## Descriptor Utilities\n",
                "    def loadDescriptors(self, path):\n",
                "        df = self.spark.read.format('delta').load(self.oea.to_url(path)).select(\"namespace\", \"codeValue\")\n",
                "        return df \n",
                "        \n",
                "    def returnNamespaces(self, descriptorsDFRef, descriptor):\n",
                "        namespaces = descriptorsDFRef[descriptor].select(\"namespace\").distinct().rdd.flatMap(lambda x: x).collect()\n",
                "        namespaces_num = len(namespaces)\n",
                "        if namespaces_num == 1:\n",
                "            return descriptor, namespaces[0]\n",
                "        else:\n",
                "            return 0,namespaces\n",
                "    \n",
                "    ## Column Mapping\n",
                "    def map_child_elements(self, child_list, mapping_dict):\n",
                "        mapped_child_list = [mapping_dict[item] for item in child_list]\n",
                "        return mapped_child_list\n",
                "\n",
                "    def refine_entities_in_order(self, sap_to_edfi_complex, unmapped_child_list):\n",
                "        master_list = list(sap_to_edfi_complex.values())\n",
                "        child_list = self.map_child_elements(unmapped_child_list, sap_to_edfi_complex)\n",
                "        \n",
                "        # Create a dictionary to store the indices of elements in the Master List\n",
                "        master_indices = {item: index for index, item in enumerate(master_list)}\n",
                "\n",
                "        # Sort the Child List based on the indices in the Master List\n",
                "        sorted_child_list = sorted(child_list, key=lambda item: master_indices[item])\n",
                "\n",
                "        # Map the sorted_child_list back to the original values\n",
                "        original_sorted_child_list = [item for item in unmapped_child_list if sap_to_edfi_complex[item] in sorted_child_list]\n",
                "        return original_sorted_child_list\n",
                "\n",
                "    def extract_refined_cols_mapping(self, file_path):\n",
                "        # FIXME: To Be Revised\n",
                "        df = self.spark.read.json(file_path)\n",
                "        #data_dict = df.toPandas().to_dict(orient='records')[0]\n",
                "        data_dict = df.head(1)[0].asDict()\n",
                "        return data_dict\n",
                "\n",
                "    def map_columns(self, df, column_mapping):\n",
                "        for column_name, new_column_name in column_mapping.items():\n",
                "            if column_name in df.columns:\n",
                "                df = df.withColumnRenamed(column_name, new_column_name)\n",
                "        return df\n",
                "\n",
                "    def map_to_hard_values(self, df, edfi_item):\n",
                "        # FIXME: TO BE REVIEWED (DISTRICTID = ???)\n",
                "        # df = df.withColumn(\"DistrictId\", lit(101912))\n",
                "        if edfi_item == 'staffEducationOrganizationAssignmentAssociations':\n",
                "            #df = df.withColumn(\"staffServiceDescriptor\", lit(\"SS013000\"))\n",
                "            pass\n",
                "        elif edfi_item == 'staffEducationOrganizationEmploymentAssociations':\n",
                "            if 'employmentStatusDescriptor' not in df.columns:\n",
                "                # FIXME: HARDCODED SINCE COLUMN IS ABSENT\n",
                "                df = df.withColumn(\"employmentStatusDescriptor\", lit('Other'))\n",
                "        else:\n",
                "            pass\n",
                "        return df\n",
                "    \n",
                "    ## DATA Cleaning\n",
                "    def infer_descriptor_columns(self, final_columns):\n",
                "        descriptor_columns = [col for col in final_columns if col.endswith(\"Descriptor\")]\n",
                "        return descriptor_columns\n",
                "\n",
                "    def transform_dataframe(self, df, descriptor_col, df_namespace_mapping):\n",
                "        try:\n",
                "            joined_df = df.join(df_namespace_mapping, col(descriptor_col) == col('codeValue'), 'left')\n",
                "\n",
                "            transformed_df = joined_df.withColumn(descriptor_col, concat(col('namespace'), lit('#'),col(descriptor_col)))        \n",
                "            final_df = transformed_df.select(*df.columns)\n",
                "\n",
                "            # Replace null values in the descriptor_col\n",
                "            final_df = final_df.withColumn(descriptor_col, \\\n",
                "                when(col(descriptor_col).isNull(), lit('uriPlaceholder#NA')).otherwise(col(descriptor_col)))\n",
                "            \n",
                "            return final_df\n",
                "        \n",
                "        except AnalysisException as e:\n",
                "            logger.info(f\"An error occurred during transformation: - {e}\")\n",
                "            return df \n",
                "\n",
                "    def create_struct_id_sets(self, df, beginDate, endDate, descriptor, struct_name, **additional_columns):\n",
                "        # FIXME: Temporary Fix for Staffs\n",
                "        # Create a struct column using the input columns\n",
                "        if beginDate is None:\n",
                "            beginDate_expr = F.lit(None).alias('beginDate')\n",
                "        elif beginDate == 'staffService_beginDate':\n",
                "            beginDate_expr = F.col(beginDate).alias('staffServiceBeginDate')\n",
                "        else:\n",
                "            beginDate_expr = F.col(beginDate).alias('beginDate')\n",
                "        \n",
                "        if endDate is None:\n",
                "            endDate_expr = F.lit(None).alias('endDate')\n",
                "        elif endDate == 'staffService_endDate':\n",
                "            endDate_expr = F.col(endDate).alias('staffServiceEndDate')\n",
                "\n",
                "        else:\n",
                "            endDate_expr = F.col(endDate).alias('endDate')\n",
                "        \n",
                "        # Create expressions for additional columns\n",
                "        additional_exprs = [F.col(col_name).alias(col_name) for col_name in list(additional_columns.values())]\n",
                "        if descriptor is not None:\n",
                "            struct_column = F.struct(\n",
                "                beginDate_expr,\n",
                "                endDate_expr,\n",
                "                F.col(descriptor).alias(descriptor),\n",
                "                *additional_exprs\n",
                "            )\n",
                "        else:\n",
                "            struct_column = F.struct(\n",
                "                beginDate_expr,\n",
                "                endDate_expr,\n",
                "                *additional_exprs\n",
                "            )\n",
                "        # Collect the structs into an array for each row\n",
                "        df = df.withColumn(struct_name, F.array(struct_column).alias(struct_name))\n",
                "        return df\n",
                "\n",
                "    def convert_to_TX_ext_struct(self, df, columns):\n",
                "        # Create a list of map expressions for the specified columns\n",
                "        tx_map_expr = []\n",
                "        for col_name in columns:\n",
                "            tx_map_expr.append(col(col_name).alias(col_name))\n",
                "        tx_struct = struct(*tx_map_expr)\n",
                "        tx_struct = F.struct(tx_struct.alias(\"TX\"))\n",
                "        # Add the TX struct to the _ext column\n",
                "        df_with_tx_struct = df.withColumn(\"_ext\", tx_struct)\n",
                "        return df_with_tx_struct\n",
                "\n",
                "\n",
                "    def convert_id_to_struct(self, df, struct_name, id_column):\n",
                "        df = df.withColumn(struct_name, struct(col(id_column).alias(id_column)))\n",
                "        #df = df.drop(id_column)\n",
                "        return df\n",
                "\n",
                "    def convert_column_to_array(self,\n",
                "                                df, \n",
                "                                source_columns = [\"raceDescriptor\"],\n",
                "                                target_key = \"raceDescriptor\",\n",
                "                                target_column = 'races'):\n",
                "        # Filter out null values from each race column and create struct array\n",
                "        array_expr = []\n",
                "        for column in source_columns:\n",
                "            array_expr.append(F.when(F.col(column).isNotNull(), F.struct(F.col(column).alias(target_key))))\n",
                "        df = df.withColumn(target_column, F.array(*array_expr))\n",
                "        return df\n",
                "\n",
                "    def convert_date_columns_to_standard_format(self, df):\n",
                "        date_columns = [column for column in df.columns if \"date\" in column.lower()]\n",
                "        for column in date_columns:\n",
                "            df = df.withColumn(column, to_date(col(column), \"yyyyMMdd\").cast(StringType()))\n",
                "        return df\n",
                "\n",
                "    def format_digit_vals(self, df, col_name):\n",
                "        condition = when(col(col_name).cast(\"string\").rlike(\"^[0-9]{1}$\"), concat_ws(\"\", lit(\"00\"), col(col_name)))\n",
                "        condition = condition.when(col(col_name).cast(\"string\").rlike(\"^[0-9]{2}$\"), concat_ws(\"\", lit(\"0\"), col(col_name))).otherwise(col(col_name))\n",
                "        formatted_df = df.withColumn(col_name, condition)\n",
                "        return formatted_df\n",
                "\n",
                "    def replace_null_with_default(self, df, column, default_value):\n",
                "        return df.na.fill(value=default_value,subset=[column])\n",
                "\n",
                "    def drop_completely_null_columns(self, df):\n",
                "        non_null_counts = df.select([col(c).isNotNull().alias(c) for c in df.columns])\n",
                "        non_null_columns = [col for col in non_null_counts.columns if non_null_counts.select(col).first()[col]]\n",
                "        \n",
                "        return df.select(*non_null_columns)\n",
                "\n",
                "    def filter_columns(self, df, column_list):\n",
                "        existing_columns = [col for col in column_list if col in df.columns]\n",
                "        return df.select(existing_columns)\n",
                "\n",
                "    ## Other Utilities\n",
                "    def get_sink_general_path(self, entity_parent_path, edfi_version, edfi_item, partitioning,SAP_SUB, TEST_MODE = False):\n",
                "        if edfi_item.endswith('Exts'):\n",
                "            item_domain = f'tx/{edfi_item}'\n",
                "        else:\n",
                "            item_domain = f'ed-fi/{edfi_item}'\n",
                "        destination_path = entity_parent_path.replace('Ingested', 'Refined').replace('SAP', f'SAP/{SAP_SUB}') + '/general/' + item_domain #.replace('SAP', 'Ed-Fi').replace('1.0', edfi_version)\n",
                "        if partitioning:\n",
                "            pattern = re.compile(r'DistrictId=.*?/|SchoolYear=.*?/')\n",
                "            destination_path = re.sub(pattern, '', destination_path)\n",
                "        \n",
                "        if TEST_MODE:\n",
                "            destination_path = destination_path.replace('/Refined/', '/TEST/Refined/')\n",
                "        return destination_path\n",
                "\n",
                "    def get_sink_sensitive_path(self, entity_parent_path, edfi_version, edfi_item, partitioning,SAP_SUB, TEST_MODE = False):\n",
                "        if edfi_item.endswith('Exts'):\n",
                "            item_domain = f'tx/{edfi_item}'\n",
                "        else:\n",
                "            item_domain = f'ed-fi/{edfi_item}'\n",
                "        destination_path = entity_parent_path.replace('Ingested', 'Refined').replace('SAP', f'SAP/{SAP_SUB}') + '/sensitive/' + f'{item_domain}_lookup' #.replace('SAP', 'Ed-Fi').replace('1.0', edfi_version)\n",
                "        if partitioning:\n",
                "            pattern = re.compile(r'DistrictId=.*?/|SchoolYear=.*?/')\n",
                "            destination_path = re.sub(pattern, '', destination_path)\n",
                "        \n",
                "        if TEST_MODE:\n",
                "            destination_path = destination_path.replace('/Refined/', '/TEST/Refined/')\n",
                "        return destination_path\n",
                "    def get_sink_general_sensitive_paths(self, \n",
                "                                        source_path, \n",
                "                                        edfi_version, \n",
                "                                        edfi_item, \n",
                "                                        partitioning = False,\n",
                "                                        SAP_SUB = 'FINAL',\n",
                "                                        TEST_MODE = False):\n",
                "        \n",
                "        path_dict = self.oea.parse_path(source_path)  \n",
                "        sink_general_path = self.get_sink_general_path(path_dict['entity_parent_path'], \n",
                "                                                edfi_version, \n",
                "                                                edfi_item, \n",
                "                                                partitioning,\n",
                "                                                SAP_SUB,\n",
                "                                                TEST_MODE)\n",
                "        sink_sink_path = self.get_sink_sensitive_path(path_dict['entity_parent_path'], \n",
                "                                                edfi_version, \n",
                "                                                edfi_item, \n",
                "                                                partitioning,\n",
                "                                                SAP_SUB,\n",
                "                                                TEST_MODE)\n",
                "        return sink_general_path, sink_sink_path\n",
                "            \n",
                "    def extract_district_id(self, input_string):\n",
                "        # Extract DistrictId using regular expression\n",
                "        district_id_match = re.search(r'DistrictId=(\\d+)', input_string)\n",
                "        if district_id_match:\n",
                "            district_id = district_id_match.group(1)\n",
                "        else:\n",
                "            district_id = None\n",
                "\n",
                "        # Extract Component using regular expression\n",
                "        component_match = re.search(r'(\\w+) into', input_string)\n",
                "        if component_match:\n",
                "            component = component_match.group(1)\n",
                "        else:\n",
                "            component = None\n",
                "        return district_id\n",
                "    \n",
                "    def has_column(self, df, col):\n",
                "        try:\n",
                "            df[col]\n",
                "            return True\n",
                "        except AnalysisException:\n",
                "            return False\n",
                "    \n",
                "    def process_date_column(self, \n",
                "                            df, \n",
                "                            date_col_name,\n",
                "                            date_format = \"yyyyMMdd\", \n",
                "                            exclude_cols = []):\n",
                "        # NOTE: WIP - Subject to Revisions\n",
                "        if date_col_name in df.columns:\n",
                "            df = df.withColumn(date_col_name, to_date(col(date_col_name), date_format))\n",
                "            df = df.withColumn(date_col_name, col(date_col_name).cast(DateType()))\n",
                "        else:\n",
                "            # TODO: Requires Review\n",
                "            df = df.withColumn(date_col_name, F.lit(None).cast(DateType()))\n",
                "        return df\n",
                "\n",
                "    def preprocess_race_columns(self, df, raceDescriptorsRef):\n",
                "        race_columns = [f'race{i}_Descriptor' for i in range(1, 6)]\n",
                "        for i in range(1,6):\n",
                "            col_name = f\"race{i}_Descriptor\"\n",
                "            df = df.withColumn(col_name, F.when(F.col(col_name) == '1', F.lit(f\"0{i}\")).otherwise(F.lit(\"uriPlaceholder#NA\")))\n",
                "\n",
                "            df = self.transform_dataframe(df, \n",
                "                                          col_name, \n",
                "                                          raceDescriptorsRef #sap_process_client.descriptorsDFRef['raceDescriptors']\n",
                "                                          )\n",
                "        return df\n",
                "\n",
                "    def combine_race_columns(self, df):\n",
                "        race_columns = [f'race{i}_Descriptor' for i in range(1, 6)]\n",
                "        race_array = F.array([F.struct(F.col(col).alias('raceDescriptor')) for col in race_columns])\n",
                "        \n",
                "        df = df.withColumn('races', race_array)\n",
                "        df = df.drop(*race_columns)\n",
                "        \n",
                "        races_schema = StructType([StructField('raceDescriptor', StringType(), True)])\n",
                "        array_type = ArrayType(races_schema, True)\n",
                "        struct_field = StructField('races', array_type, True)\n",
                "        df = df.withColumn('races', F.col('races').cast(array_type))\n",
                "        return df\n",
                "    \n",
                "    def create_record_hash_column(self, df):\n",
                "        # TODO: Under DEV\n",
                "        df_cols = [col for col in df.columns if col not in self.sap_essential_columns]\n",
                "        non_struct_array_cols = [col for col in df_cols if not isinstance(df.schema[col].dataType, (StructType, ArrayType))]\n",
                "        non_struct_array_cols = ['NATURAL_KEY_HASH'] + non_struct_array_cols # FIXME: UNDER DEV\n",
                "        \n",
                "        logger.info(f'Creating hash column - RECORD_HASH - of the following columns: {non_struct_array_cols}')\n",
                "        record_hash_expr = [f.col(key_component).cast('string') for key_component in non_struct_array_cols]\n",
                "        df = df.withColumn(\"RECORD_HASH\",F.sha2(F.concat(*[F.concat(F.coalesce(column, F.lit('')), F.lit('_')) for column in record_hash_expr]), 256))\n",
                "        return df"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "class SAPProcessClient:\n",
                "    def __init__(self, \n",
                "                 spark, \n",
                "                 oea,\n",
                "                 sap_utilities,\n",
                "                 sap_to_edfi_complex,\n",
                "                 final_columns,\n",
                "                 _ext_TX_cols, \n",
                "                 descriptorsDFRef,\n",
                "                 descriptors):\n",
                "        self.spark = spark\n",
                "        self.oea = oea\n",
                "        self.sap_utilities = sap_utilities\n",
                "        self.sap_to_edfi_complex = sap_to_edfi_complex\n",
                "        self.final_columns = final_columns \n",
                "        self._ext_TX_cols = _ext_TX_cols \n",
                "        self.descriptorsDFRef = descriptorsDFRef\n",
                "        self.descriptors = descriptors\n",
                "    def processBudgetExts(self, df):\n",
                "        df = sap_utilities.process_date_column(df = df,\n",
                "                                               date_col_name = 'beginDate',\n",
                "                                               date_format = \"yyyyMMdd\",\n",
                "                                               exclude_cols = [])\n",
                "        df = sap_utilities.process_date_column(df = df,\n",
                "                                               date_col_name = 'endDate',\n",
                "                                               date_format = \"yyyyMMdd\",\n",
                "                                               exclude_cols = [])\n",
                "\n",
                "        df = df.withColumn('educationOrganizationId', col('educationOrganizationId').cast(IntegerType()))\n",
                "        df = self.sap_utilities.convert_id_to_struct(df, 'educationOrganizationReference', 'educationOrganizationId')\n",
                "        df = self.sap_utilities.filter_columns(df, self.final_columns['budgetExts'])\n",
                "        df = self.sap_utilities.create_record_hash_column(df)\n",
                "        return df\n",
                "\n",
                "    def processActualExts(self, df):\n",
                "        df = sap_utilities.process_date_column(df = df,\n",
                "                                               date_col_name = 'beginDate',\n",
                "                                               date_format = \"yyyyMMdd\",\n",
                "                                               exclude_cols = [])\n",
                "        df = sap_utilities.process_date_column(df = df,\n",
                "                                               date_col_name = 'endDate',\n",
                "                                               date_format = \"yyyyMMdd\",\n",
                "                                               exclude_cols = [])\n",
                "                                               \n",
                "        df = df.withColumn('educationOrganizationId', col('educationOrganizationId').cast(IntegerType()))\n",
                "        df = self.sap_utilities.convert_id_to_struct(df, 'educationOrganizationReference', 'educationOrganizationId')\n",
                "        df = self.sap_utilities.filter_columns(df, self.final_columns['actualExts'])\n",
                "        df = self.sap_utilities.create_record_hash_column(df)\n",
                "        return df\n",
                "\n",
                "    def processStaffs(self, df):\n",
                "        self.spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
                "        if 'birthDate' in df.columns:\n",
                "            df = df.withColumn(\"birthDate\",\n",
                "                                to_date(col(\"birthDate\"),\"yyyyMMdd\"))\n",
                "        else:\n",
                "            df = df.withColumn('birthDate', \n",
                "                               to_date(F.lit(None),\"yyyyMMdd\"))\n",
                "        \n",
                "        # pkTeacherRequirementDescriptor\n",
                "        # TODO: To Be Reviewed\n",
                "        if 'pkTeacherRequirementDescriptor' not in df.columns:\n",
                "            df = df.withColumn('pkTeacherRequirementDescriptor', F.lit('uriPlaceholder#NA'))\n",
                "        if 'staffTypeDescriptor' not in df.columns:\n",
                "            df = df.withColumn('staffTypeDescriptor', F.lit('uriPlaceholder#NA'))\n",
                "        if 'generationCodeDescriptor' not in df.columns:\n",
                "            df = df.withColumn('generationCodeDescriptor', F.lit('uriPlaceholder#NA'))\n",
                "        if 'totalYearsProfExperience' not in df.columns:\n",
                "            df = df.withColumn('totalYearsProfExperience', F.lit(None).cast('int'))\n",
                "        if 'yearsExperienceInDistrict' not in df.columns:\n",
                "            df = df.withColumn('yearsExperienceInDistrict', F.lit(None).cast('int'))\n",
                "        \n",
                "        # TYPESET\n",
                "        df = df.withColumn('staffTypeSet_beginDate', to_date(col('staffTypeSet_beginDate'), \"yyyyMMdd\").cast(StringType()))\n",
                "        df = df.withColumn('staffTypeSet_endDate', to_date(col('staffTypeSet_endDate'), \"yyyyMMdd\").cast(StringType()))\n",
                "        \n",
                "        df = df.withColumn('staffTypeSet_beginDate', col('staffTypeSet_beginDate').cast(DateType()))\n",
                "        df = df.withColumn('staffTypeSet_endDate', col('staffTypeSet_endDate').cast(DateType()))\n",
                "\n",
                "        # PARAPROFESSIONAL\n",
                "        df = df.withColumn('paraprofessional_beginDate', to_date(col('paraprofessional_beginDate'), \"yyyyMMdd\").cast(StringType()))\n",
                "        df = df.withColumn('paraprofessional_beginDate', to_date(col('paraprofessional_beginDate'), \"yyyyMMdd\").cast(StringType()))\n",
                "        \n",
                "        df = df.withColumn('paraprofessional_beginDate', col('paraprofessional_beginDate').cast(DateType()))\n",
                "        df = df.withColumn('paraprofessional_endDate', col('paraprofessional_endDate').cast(DateType()))\n",
                "\n",
                "        # BOOLEAN TYPES\n",
                "        df = df.withColumn('staffDoNotReportTSDS', col('staffDoNotReportTSDS').cast(BooleanType()))\n",
                "        df = df.withColumn('hispanicLatinoEthnicity', col('hispanicLatinoEthnicity').cast(BooleanType()))\n",
                "        df = df.withColumn('paraprofessionalCertification', col('paraprofessionalCertification').cast(BooleanType()))\n",
                "\n",
                "        # RACES\n",
                "        df = self.sap_utilities.preprocess_race_columns(df, self.descriptorsDFRef['raceDescriptors'])\n",
                "        df = self.sap_utilities.combine_race_columns(df)\n",
                "        \n",
                "        if 'raceDescriptor' in df.columns:\n",
                "            df = self.sap_utilities.convert_column_to_array(df, \n",
                "                                            source_columns = [\"raceDescriptor\"],\n",
                "                                            target_key = \"raceDescriptor\",\n",
                "                                            target_column = 'races')\n",
                "        df = self.sap_utilities.create_struct_id_sets(df = df, \n",
                "                                    beginDate = 'staffTypeSet_beginDate', \n",
                "                                    endDate = 'staffTypeSet_endDate',\n",
                "                                    descriptor = 'staffTypeDescriptor',\n",
                "                                    struct_name = 'typeSets')\n",
                "        \n",
                "        df = self.sap_utilities.create_struct_id_sets(df = df, \n",
                "                                    beginDate = 'paraprofessional_beginDate', \n",
                "                                    endDate = 'paraprofessional_endDate',\n",
                "                                    descriptor = None,\n",
                "                                    struct_name = 'paraprofessionalCertificationSet',\n",
                "                                    paraprofessionalCertification = 'paraprofessionalCertification')\n",
                "        #TODO: Potentially Debug\n",
                "        _ext_TX_cols_staffs = self._ext_TX_cols['staffs']\n",
                "        if 'paraprofessionalCertificationSet' not in _ext_TX_cols_staffs:\n",
                "            _ext_TX_cols_staffs.append('paraprofessionalCertificationSet')\n",
                "        if 'staffDoNotReportTSDS' not in _ext_TX_cols_staffs:\n",
                "            _ext_TX_cols_staffs.append('staffDoNotReportTSDS')\n",
                "        df = self.sap_utilities.convert_to_TX_ext_struct(df, \n",
                "                                          columns = _ext_TX_cols_staffs)\n",
                "        df = self.sap_utilities.filter_columns(df, self.final_columns['staffs'])\n",
                "        df = self.sap_utilities.create_record_hash_column(df)\n",
                "        return df\n",
                "\n",
                "    def processContractedInstructionalStaffFTEExts(self, df):\n",
                "        df = df.withColumnRenamed(\"localEducationAgencyId\", \"educationOrganizationId\")\n",
                "\n",
                "        df = df.withColumn('educationOrganizationId', col('educationOrganizationId').cast(IntegerType()))\n",
                "        df = self.sap_utilities.convert_id_to_struct(df, 'educationOrganizationReference', 'educationOrganizationId')\n",
                "        \n",
                "        df = df.withColumn('schoolId', col('schoolId').cast(IntegerType()))\n",
                "        df = self.sap_utilities.convert_id_to_struct(df, 'schoolReference', 'schoolId')\n",
                "        df = self.sap_utilities.filter_columns(df, self.final_columns['contractedInstructionalStaffFTEExts'])\n",
                "        df = self.sap_utilities.create_record_hash_column(df)\n",
                "        return df\n",
                "\n",
                "    def processStaffEducationOrganizationEmploymentAssociations(self, df):\n",
                "        df = df.withColumn('percentDayEmployed', col('percentDayEmployed').cast('integer'))\n",
                "        df = df.withColumn('numberDaysEmployed', col('numberDaysEmployed').cast('integer'))\n",
                "        # df = self.sap_utilities.replace_null_with_default(df, 'employmentStatusDescriptor', 'Other')\n",
                "\n",
                "        df = df.withColumnRenamed(\"localEducationAgencyId\", \"educationOrganizationId\")\n",
                "        df = df.withColumn('educationOrganizationId', col('educationOrganizationId').cast(IntegerType()))\n",
                "\n",
                "        df = self.sap_utilities.convert_id_to_struct(df, 'educationOrganizationReference', 'educationOrganizationId')\n",
                "        \n",
                "        #df = df.withColumn(\"auxiliaryRoleIdSet_beginDate\", col(\"hireDate\"))\n",
                "        \n",
                "        df = df.withColumn('auxiliaryRoleIdSet_beginDate', to_date(col('auxiliaryRoleIdSet_beginDate'), \"yyyyMMdd\"))\n",
                "        df = df.withColumn('hireDate', to_date(col('hireDate'), \"yyyyMMdd\"))\n",
                "        df = df.withColumn('endDate', to_date(col('endDate'), \"yyyyMMdd\"))\n",
                "        df = df.withColumn('auxiliaryRoleIdSet_endDate', to_date(col('auxiliaryRoleIdSet_endDate'), \"yyyyMMdd\"))\n",
                "        \n",
                "        df = self.sap_utilities.create_struct_id_sets(df = df, \n",
                "                                                      beginDate = 'auxiliaryRoleIdSet_beginDate', \n",
                "                                                      endDate = 'auxiliaryRoleIdSet_endDate',\n",
                "                                                      descriptor = 'auxiliaryRoleIdDescriptor',\n",
                "                                                      struct_name = 'auxiliaryRoleIdSets')\n",
                "                            \n",
                "        df = self.sap_utilities.convert_id_to_struct(df, 'staffReference', 'staffUniqueId')\n",
                "        df = self.sap_utilities.convert_to_TX_ext_struct(df, \n",
                "                                        columns = self._ext_TX_cols['staffEducationOrganizationEmploymentAssociations']\n",
                "                                        )\n",
                "        df = self.sap_utilities.filter_columns(df, self.final_columns['staffEducationOrganizationEmploymentAssociations'])\n",
                "        df = self.sap_utilities.create_record_hash_column(df)\n",
                "        return df\n",
                "\n",
                "\n",
                "    def processStaffEducationOrganizationAssignmentAssociations(self, df):\n",
                "        df = df.withColumn('staffService_beginDate', to_date(col('staffService_beginDate'), \"yyyyMMdd\"))\n",
                "        df = df.withColumn('staffService_endDate', to_date(col('staffService_endDate'), \"yyyyMMdd\"))\n",
                "        \n",
                "        df = df.withColumn('staffService_beginDate', col('staffService_beginDate').cast(DateType()))\n",
                "        df = df.withColumn('staffService_endDate', col('staffService_endDate').cast(DateType()))\n",
                "\n",
                "        df = df.withColumnRenamed(\"localEducationAgencyId\", \"educationOrganizationId\")\n",
                "        df = df.withColumn(\"educationOrganizationId\", col(\"educationOrganizationId\").cast(IntegerType()))\n",
                "\n",
                "        df = self.sap_utilities.convert_id_to_struct(df, 'educationOrganizationReference', 'educationOrganizationId')\n",
                "        # df = df.withColumn(\"beginDate\", to_date(lit(\"2023-08-01\")))\n",
                "\n",
                "        df = df.withColumn('beginDate', to_date(col('beginDate'), \"yyyyMMdd\"))\n",
                "        df = df.withColumn('endDate', to_date(col('endDate'), \"yyyyMMdd\"))\n",
                "        \n",
                "        df = df.withColumn('beginDate', col('beginDate').cast(DateType()))\n",
                "        df = df.withColumn('endDate', col('endDate').cast(DateType()))\n",
                "\n",
                "        df = df.withColumn('monthlyMinutes', col('monthlyMinutes').cast(IntegerType()))\n",
                "\n",
                "        df = self.sap_utilities.format_digit_vals(df, 'staffClassificationDescriptor')\n",
                "        df = self.sap_utilities.create_struct_id_sets(df = df, \n",
                "                                    beginDate = 'staffService_beginDate', \n",
                "                                    endDate = 'staffService_endDate',\n",
                "                                    descriptor = 'staffServiceDescriptor',\n",
                "                                    struct_name = 'staffServiceSets',\n",
                "                                    monthlyMinutes = \"monthlyMinutes\",\n",
                "                                    populationServedDescriptor = \"populationServedDescriptor\")\n",
                "        df = self.sap_utilities.convert_id_to_struct(df, 'schoolReference', 'schoolId')\n",
                "        \n",
                "        df = self.sap_utilities.convert_id_to_struct(df, 'staffReference', 'staffUniqueId')\n",
                "        df = self.sap_utilities.convert_to_TX_ext_struct(df, \n",
                "                                        columns = self._ext_TX_cols['staffEducationOrganizationAssignmentAssociations']\n",
                "                                        )\n",
                "        df = self.sap_utilities.filter_columns(df, self.final_columns['staffEducationOrganizationAssignmentAssociations'])\n",
                "        df = self.sap_utilities.create_record_hash_column(df)\n",
                "        return df\n",
                "\n",
                "\n",
                "    def processPayrollExts(self, df):\n",
                "        df = df.withColumnRenamed(\"localEducationAgencyId\", \"educationOrganizationId\")\n",
                "        df = df.withColumn('educationOrganizationId', col('educationOrganizationId').cast(IntegerType()))\n",
                "        \n",
                "        df = self.sap_utilities.convert_id_to_struct(df, 'educationOrganizationReference', 'educationOrganizationId')\n",
                "\n",
                "        df = df.withColumn('beginDate', to_date(col('beginDate'), \"yyyyMMdd\"))\n",
                "        df = df.withColumn('endDate', to_date(col('endDate'), \"yyyyMMdd\"))\n",
                "        \n",
                "        df = df.withColumn('beginDate', col('beginDate').cast(DateType()))\n",
                "        df = df.withColumn('endDate', col('endDate').cast(DateType()))\n",
                "\n",
                "        \n",
                "        df = df.withColumn(\"fiscalYear\", col(\"fiscalYear\").cast(IntegerType()))\n",
                "        df = df.withColumn(\"organization\", col(\"organization\").cast(IntegerType()))\n",
                "        df = self.sap_utilities.convert_id_to_struct(df, 'staffReference', 'staffUniqueId')\n",
                "        df = self.sap_utilities.filter_columns(df, self.final_columns['payrollExts'])\n",
                "        df = self.sap_utilities.create_record_hash_column(df)\n",
                "        return df\n",
                ""
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "class SAPToEdFiRefine(EdFiRefine):\n",
                "    def __init__(self, \n",
                "                 workspace, \n",
                "                 oea,\n",
                "                 spark, \n",
                "                 sap_oea_utils,\n",
                "                 sap_process_client,\n",
                "                 logger,\n",
                "                 schema_gen, \n",
                "                 moduleName, \n",
                "                 authUrl, \n",
                "                 swaggerUrl, \n",
                "                 dataManagementUrl, \n",
                "                 changeQueriesUrl, \n",
                "                 dependenciesUrl, \n",
                "                 apiVersion, \n",
                "                 schoolYear,\n",
                "                 districtId, \n",
                "                 pipelineExecutionId,\n",
                "                 error_logger,\n",
                "                 test_mode,\n",
                "                 natural_upsert_mode = False,\n",
                "                 sap_essential_columns = None):\n",
                "        super().__init__(workspace = workspace, \n",
                "                       oea = oea, \n",
                "                       spark = spark,\n",
                "                       schema_gen = schema_gen, \n",
                "                       moduleName = moduleName, \n",
                "                       authUrl = authUrl, \n",
                "                       swaggerUrl = swaggerUrl, \n",
                "                       dataManagementUrl = dataManagementUrl, \n",
                "                       changeQueriesUrl = changeQueriesUrl, \n",
                "                       dependenciesUrl = dependenciesUrl, \n",
                "                       apiVersion = apiVersion, \n",
                "                       schoolYear = schoolYear, \n",
                "                       districtId = districtId, \n",
                "                       pipelineExecutionId = pipelineExecutionId,\n",
                "                       error_logger = error_logger,\n",
                "                       test_mode = test_mode)\n",
                "        self.sap_oea_utils = sap_oea_utils\n",
                "        self.logger = logger\n",
                "        self._ext_TX_cols = sap_process_client._ext_TX_cols #SAPProcessClient()._ext_TX_cols\n",
                "        self.schemas = sap_oea_utils.create_spark_schemas() \n",
                "        self.primitive_datatypes = ['timestamp', 'date', 'decimal', 'boolean', 'integer', 'string', 'long'] \n",
                "        # self.error_logger = ErrorLogging(spark = spark, \n",
                "        #                                 oea = oea, \n",
                "        #                                 logger = logger)\n",
                "        self.natural_upsert_mode = natural_upsert_mode\n",
                "        self.sap_essential_columns = sap_essential_columns if sap_essential_columns is not None else ['DistrictId', 'SchoolYear', 'sap_pipeline', 'sap_pipelineType', 'lakeId', 'validationRecordId', 'LastModifiedDate', 'rundate', 'stage1_source_url', 'RECORD', 'NATURAL_KEY_HASH', 'RECORD_HASH', 'RECORD_VERSION']\n",
                "\n",
                "    def set_params(self, params = {}):\n",
                "        for key, value in params.items():\n",
                "            if key == 'sap_pipeline':\n",
                "                self.sap_pipeline = value\n",
                "    \n",
                "    def modify_target_schema(self, target_schema, incl_columns, excl_columns):\n",
                "        for column in incl_columns:\n",
                "            if column not in excl_columns:\n",
                "                if column.lower().endswith('date'):\n",
                "                    target_schema = target_schema.add(StructField(column, TimestampType()))\n",
                "                else:\n",
                "                    target_schema = target_schema.add(StructField(column, StringType()))\n",
                "        return target_schema              \n",
                "    \n",
                "    def modify_descriptor_value(self, df, col_name):\n",
                "        if col_name in df.columns:\n",
                "            # TODO: @Abhinav, I do not see where you made the changes to use the descriptorId instead of Namespace/CodeValue\n",
                "            df = df.withColumn(f\"{col_name}LakeId\", f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.regexp_replace(col_name, '#', '_')))\n",
                "            #df = df.drop(col_name)\n",
                "        else:\n",
                "            df = df.withColumn(f\"{col_name}LakeId\", f.lit(None).cast(\"String\"))\n",
                "\n",
                "        return df\n",
                "\n",
                "    \n",
                "    def extract_pk_from_definitions_exp(self, table_name, depluralize = True):\n",
                "        # NOTE: This function is under review\n",
                "        '''\n",
                "        EXAMPLE:\n",
                "        pk, _ = extract_pk_from_definitions('payrollExts', depluralize = True)\n",
                "        fk = self.extract_reference_key(schemas['payrollExts']['educationOrganizationReference'])\n",
                "        '''\n",
                "        target_schema = copy.deepcopy(self.schemas[table_name])\n",
                "        identity_cols = []\n",
                "        cleanup_cols = []\n",
                "        for col_struct in target_schema:\n",
                "            col_name = col_struct.name\n",
                "            if not(col_struct.nullable):\n",
                "                cleanup_cols.append(col_name)\n",
                "            if not(col_struct.nullable) and not(re.search('Reference$', col_name)):\n",
                "                identity_cols.append(col_name)\n",
                "            elif not(col_struct.nullable) and (re.search('Reference$', col_name)):\n",
                "                target_col = target_schema[col_name]\n",
                "                identity_cols = identity_cols + [f'{col_name}.{x}' for x in self.extract_reference_key(target_col)]\n",
                "                \n",
                "        return sorted(identity_cols), cleanup_cols\n",
                "    \n",
                "    def extract_pk_from_definitions(self, table_name, depluralize = True):\n",
                "        '''\n",
                "        EXAMPLE:\n",
                "        pk, _ = extract_pk_from_definitions('payrollExts', depluralize = True)\n",
                "        fk = self.extract_reference_key(schemas['payrollExts']['educationOrganizationReference'])\n",
                "        '''\n",
                "        target_schema = copy.deepcopy(self.schemas[table_name])\n",
                "        if depluralize:\n",
                "            table_name = self.sap_oea_utils.pluralization_mappings[table_name] #self.sap_oea_utils.depluralize(table_name)\n",
                "        identity_cols = []\n",
                "        cleanup_cols = []\n",
                "        for col_name in self.sap_oea_utils.definitions[table_name]:\n",
                "            target_col = target_schema[col_name]\n",
                "            col_schema = self.sap_oea_utils.definitions[table_name][col_name]\n",
                "            key = self.sap_oea_utils.pluralize(table_name)\n",
                "            if 'x-Ed-Fi-isIdentity' in col_schema and col_schema['x-Ed-Fi-isIdentity'] != [\"*\"]:\n",
                "                identity_cols = identity_cols + [col_name]\n",
                "                cleanup_cols = identity_cols\n",
                "            if (re.search('Reference$', col_name) is not None) and (col_schema['required']):\n",
                "                cleanup_cols = identity_cols + [col_name]\n",
                "                identity_cols = identity_cols + [f'{col_name}.{x}' for x in self.extract_reference_key(target_col)]\n",
                "                \n",
                "        return sorted(identity_cols), cleanup_cols\n",
                "\n",
                "    # Use this function to extract reference Key columns and create a FK\n",
                "    def extract_reference_key(self, target_col):\n",
                "        reference_keys = []\n",
                "        field_names = target_col.dataType.fields\n",
                "        for candidate_field in field_names:\n",
                "            candidate_field_name = candidate_field.name\n",
                "            if candidate_field.nullable == False:\n",
                "                identifier_field_name = f\"{candidate_field_name}\"\n",
                "                reference_keys = reference_keys + [identifier_field_name]\n",
                "        \n",
                "        return sorted(reference_keys)\n",
                "\n",
                "    def upsert_with_logging(self, \n",
                "                            df, \n",
                "                            sap_pipeline,\n",
                "                            destination_path, \n",
                "                            primary_key, \n",
                "                            partitioning, \n",
                "                            partitioning_cols,\n",
                "                            table_name,\n",
                "                            ext_entity,\n",
                "                            parent = True):\n",
                "        # NOTE: partitioning_cols is a legacy param\n",
                "        start_time = self.thread_local.start_times.get(table_name, datetime.now())\n",
                "        if self.natural_upsert_mode:\n",
                "            primary_key = ['NATURAL_KEY_HASH', 'DistrictId', 'SchoolYear']\n",
                "        else:\n",
                "            primary_key = ['RECORD', 'DistrictId', 'SchoolYear']\n",
                "        if parent:\n",
                "            numInputRows, numOutputRows, numTargetRowsInserted, numTargetRowsUpdated = self.oea.upsert(df = df, \n",
                "                                                                    destination_path = destination_path,\n",
                "                                                                    primary_key = primary_key,#['RECORD', 'DistrictId', 'SchoolYear'],\n",
                "                                                                    partitioning = True,\n",
                "                                                                    partitioning_cols = [self.districtId_col_name, self.schoolYear_col_name],\n",
                "                                                                    surrogate_key = False)   \n",
                "        else:\n",
                "            numInputRows, numOutputRows, numTargetRowsInserted, numTargetRowsUpdated = self.oea.delete_then_insert(df = df, \n",
                "                                                                    destination_path = destination_path,\n",
                "                                                                    primary_key = primary_key,#['RECORD', 'DistrictId', 'SchoolYear'],\n",
                "                                                                    partitioning = True,\n",
                "                                                                    partitioning_cols = [self.districtId_col_name, self.schoolYear_col_name],\n",
                "                                                                    surrogate_key = False)                                    \n",
                "        end_time = datetime.now()\n",
                "        # FIXME: 2024-02-15: flagging empty Tab;es\n",
                "        if '/emptySchemas/' in destination_path:\n",
                "            emptySchemaMetadata = True\n",
                "        else:\n",
                "            emptySchemaMetadata = False\n",
                "        log_data = error_logger.create_log_dict(uniqueId = error_logger.generate_random_alphanumeric(10), # Generate a random 10-character alphanumeric value\n",
                "                                                pipelineExecutionId = self.pipelineExecutionId,#'TEST_1234',#executionId,\n",
                "                                                sparkSessionId = spark.sparkContext.applicationId,\n",
                "                                                sap_pipeline = sap_pipeline,\n",
                "                                                sap_pipelineType = sap_pipelineType,\n",
                "                                                stageName = \"Refinement\",\n",
                "                                                schemaFormat = 'ed-fi',\n",
                "                                                entityType = ext_entity.lower(), # TODO: To Be Reviewed\n",
                "                                                entityName = table_name,\n",
                "                                                numInputRows = numInputRows,\n",
                "                                                totalNumOutputRows = numOutputRows,\n",
                "                                                numTargetRowsInserted = numTargetRowsInserted,\n",
                "                                                numTargetRowsUpdated = numTargetRowsUpdated,\n",
                "                                                numRecordsSkipped = 0,\n",
                "                                                numRecordsDeleted = 0,\n",
                "                                                start_time = start_time,\n",
                "                                                end_time = end_time,\n",
                "                                                insertionType = 'append' if ingestionHistoryMode else 'upsert',\n",
                "                                                emptySchemaMetadata = emptySchemaMetadata)\n",
                "        error_logger.consolidate_logs(log_data,'entity')\n",
                "    \n",
                "    def process_ext_column(self,\n",
                "                           df,\n",
                "                           schema_name,\n",
                "                           table_name,\n",
                "                           ext_column_name,\n",
                "                           ext_entity,\n",
                "                           target_schema,\n",
                "                           sink_general_path):    \n",
                "        if '_ext' in df.columns:\n",
                "            self.logger.info(f\"Writing EXT Tables - {table_name}\")\n",
                "            # FIXME: Revise Logic\n",
                "            sink_general_path = sink_general_path.replace('/ed-fi/', f'/{ext_entity.lower()}/')\n",
                "            if table_name.startswith('staffs'):\n",
                "                df = df.select(self.sap_essential_columns + ['staffUniqueId'] + ['_ext']) # df.select(['RECORD','lakeId', 'DistrictId', 'SchoolYear', 'LastModifiedDate','staffUniqueId', '_ext', 'rundate', 'sap_pipeline', 'sap_pipelineType', 'validationRecordId', 'stage1_source_url','NATURAL_KEY_HASH'])\n",
                "            else:\n",
                "                df = df.select(self.sap_essential_columns + ['_ext']) # df.select(['RECORD','lakeId', 'DistrictId', 'SchoolYear', 'LastModifiedDate','_ext', 'rundate', 'sap_pipeline', 'sap_pipelineType', 'validationRecordId', 'stage1_source_url','NATURAL_KEY_HASH'])\n",
                "            \n",
                "            target_schema = self.get_ext_entities_schemas(table_name = table_name,\n",
                "                                                    ext_column_name = ext_column_name,\n",
                "                                                    default_value = ext_entity)\n",
                "            try:\n",
                "                ext_inner_cols =  self._ext_TX_cols[table_name]\n",
                "            except:\n",
                "                ext_inner_cols = target_schema.fieldNames()\n",
                "            \n",
                "            df = self.flatten_ext_column(df = df, \n",
                "                                         table_name = table_name, \n",
                "                                         ext_col = ext_column_name, \n",
                "                                         inner_key = ext_entity,\n",
                "                                         ext_inner_cols = ext_inner_cols)\n",
                "            \n",
                "            df = self.transform_sub_module(df, \n",
                "                                        target_schema, \n",
                "                                        sink_general_path, \n",
                "                                        schema_name,\n",
                "                                        table_name,\n",
                "                                        True,\n",
                "                                        ext_entity)\n",
                "\n",
                "            self.logger.info(f\"Writing EXT Table - {table_name}\")\n",
                "            self.upsert_with_logging(df = df, \n",
                "                                     sap_pipeline = self.sap_pipeline,\n",
                "                                     destination_path = f\"{sink_general_path}\", \n",
                "                                     primary_key = 'lakeId', \n",
                "                                     partitioning = True,\n",
                "                                     partitioning_cols = ['DistrictId', 'SchoolYear'], \n",
                "                                     table_name = table_name,\n",
                "                                     ext_entity = ext_entity,\n",
                "                                     parent = True)\n",
                "            \n",
                "            self.oea.add_to_lake_db(sink_general_path, overwrite = True, extension = ext_entity)\n",
                "\n",
                "    def flatten_reference_col(self, df, table_name,target_col, reference_key):\n",
                "        if target_col.name not in ['credentialReference']:\n",
                "            # reference_key =  self.extract_reference_key(self.schemas[table_name][target_col.name])\n",
                "            col_prefix = target_col.name.replace('Reference', '')\n",
                "            df = df.withColumn(f\"{col_prefix}LakeId\", f.when(f.col(target_col.name).isNotNull(), f.concat(f.col('DistrictId'), f.lit('_'), f.col('SchoolYear'), f.lit('_'), *[F.concat(F.col(f'{target_col.name}.{x}'), F.lit('_')) for x in reference_key[:-1]], F.col(f'{target_col.name}.{reference_key[-1]}'))))\n",
                "\n",
                "        return df\n",
                "\n",
                "    def modify_references_and_descriptors(self, df, table_name,target_col):\n",
                "        for ref_col in [x for x in df.columns if re.search('Reference$', x) is not None]:\n",
                "            reference_key = self.extract_reference_key(target_col.dataType.elementType[ref_col])\n",
                "            df = self.flatten_reference_col(df, table_name,target_col.dataType.elementType[ref_col], reference_key)\n",
                "        for desc_col in [x for x in df.columns if re.search('Descriptor$', x) is not None]:\n",
                "            df = self.modify_descriptor_value(df, desc_col)\n",
                "            \n",
                "            # TODO: Test Run\n",
                "            # if desc_col in df.columns:\n",
                "            #    df = df.drop(desc_col)\n",
                "        return df\n",
                "\n",
                "    def explode_arrays(self, df, sink_general_path,target_col, schema_name, table_name, ext_entity):\n",
                "        cols = self.sap_essential_columns #['RECORD', 'lakeId', 'DistrictId', 'SchoolYear', 'LastModifiedDate', 'rundate', 'sap_pipeline', 'sap_pipelineType', 'validationRecordId', 'stage1_source_url','NATURAL_KEY_HASH']\n",
                "        child_name = f\"{table_name}_{target_col.name}\"\n",
                "        self.store_start_time(child_name)\n",
                "\n",
                "        child_df = df.select(cols + [target_col.name])\n",
                "        child_df = child_df.withColumn(\"exploded\", f.explode(target_col.name))#.drop(target_col.name).select(cols + ['exploded.*'])\n",
                "\n",
                "        identity_cols = [x.name for x in target_col.dataType.elementType.fields if 'x-Ed-Fi-isIdentity' in x.metadata].sort()\n",
                "        if(identity_cols is not None and len(identity_cols) > 0):\n",
                "            child_df = child_df.withColumn(f\"{target_col.name}LakeId\", f.concat(f.col('DistrictId'), f.lit('_'), f.col('SchoolYear'), f.lit('_'), *[f.concat(f.col(x), f.lit('_')) for x in identity_cols]))\n",
                "        \n",
                "        child_df = child_df.select(cols + ['exploded.*']).drop(target_col.name) #FIXME: TO BE REVIEWED\n",
                "        child_df = self.modify_references_and_descriptors(child_df, table_name,target_col)\n",
                "\n",
                "        if ext_entity == 'ed-fi':\n",
                "            extension = None\n",
                "        else:\n",
                "            extension = ext_entity\n",
                "\n",
                "        for array_sub_col in [x for x in target_col.dataType.elementType.fields if x.dataType.typeName() == 'array' ]:\n",
                "            grand_child_name = f\"{table_name}_{target_col.name}_{array_sub_col.name}\"\n",
                "            self.store_start_time(grand_child_name)\n",
                "            \n",
                "            grand_child_df = child_df.withColumn('exploded', f.explode(array_sub_col.name)).select(child_df.columns + ['exploded.*']).drop(array_sub_col.name)\n",
                "            grand_child_df = self.modify_references_and_descriptors(grand_child_df, table_name,array_sub_col)\n",
                "\n",
                "            self.logger.info(f\"Writing Grand Child Table - {table_name}_{target_col.name}_{array_sub_col.name}\") \n",
                "            self.upsert_with_logging(df = grand_child_df, \n",
                "                                     sap_pipeline = self.sap_pipeline, #TODO: Generalize\n",
                "                                     destination_path = f\"{sink_general_path}_{target_col.name}_{array_sub_col.name}\", \n",
                "                                     primary_key = 'lakeId', \n",
                "                                     partitioning = True,\n",
                "                                     partitioning_cols = ['DistrictId', 'SchoolYear'], \n",
                "                                     table_name = f\"{table_name}_{target_col.name}_{array_sub_col.name}\",\n",
                "                                     ext_entity = ext_entity,\n",
                "                                     parent = False)\n",
                "            self.oea.add_to_lake_db(f\"{sink_general_path}_{target_col.name}_{array_sub_col.name}\", \n",
                "                                    overwrite = True,\n",
                "                                    extension = extension)\n",
                "            \n",
                "        \n",
                "        self.logger.info(f\"Writing Child Table - {table_name}_{target_col.name}\")\n",
                "        self.upsert_with_logging(df = child_df, \n",
                "                                     sap_pipeline = self.sap_pipeline, #TODO: Generalize\n",
                "                                     destination_path = f\"{sink_general_path}_{target_col.name}\", \n",
                "                                     primary_key = 'lakeId', \n",
                "                                     partitioning = True,\n",
                "                                     partitioning_cols = ['DistrictId', 'SchoolYear'], \n",
                "                                     table_name = f\"{table_name}_{target_col.name}\",\n",
                "                                     ext_entity = ext_entity,\n",
                "                                     parent = False)\n",
                "        self.oea.add_to_lake_db(f\"{sink_general_path}_{target_col.name}\", \n",
                "                                    overwrite = True,\n",
                "                                    extension = extension)\n",
                "    \n",
                "        df = df.drop(target_col.name)\n",
                "        return df\n",
                "\n",
                "    def transform(self,\n",
                "                df, \n",
                "                schema_name, \n",
                "                table_name, \n",
                "                primary_key,\n",
                "                ext_entity,\n",
                "                sink_general_path,\n",
                "                parent_schema_name, \n",
                "                parent_table_name):\n",
                "        # TODO: Pseudominization Pending\n",
                "        self.store_start_time(table_name)\n",
                "        if re.search('Descriptors$', table_name) is None:\n",
                "            target_schema = copy.deepcopy(self.schemas[table_name])\n",
                "            # Add primary key\n",
                "            if self.has_column(df, primary_key):\n",
                "                df = df.withColumn('lakeId', f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.col(primary_key)).cast(\"String\"))\n",
                "                df = df.withColumn('validationRecordId', f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.col(primary_key)).cast(\"String\"))\n",
                "            else:\n",
                "                df = df.withColumn('lakeId', f.lit(None).cast(\"String\"))\n",
                "                df = df.withColumn('validationRecordId', f.lit(None).cast(\"String\"))\n",
                "        else:\n",
                "            target_schema = self.get_descriptor_schema(table_name)\n",
                "            # Add primary key\n",
                "            if self.has_column(df, primary_key):\n",
                "                df = df.withColumn('lakeId', f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.col(primary_key)).cast(\"String\"))\n",
                "                df = df.withColumn('validationRecordId', f.concat_ws('_', f.col('DistrictId'), f.col('SchoolYear'), f.col(primary_key)).cast(\"String\"))\n",
                "            else:\n",
                "                df = df.withColumn('lakeId', f.lit(None).cast(\"String\"))\n",
                "                df = df.withColumn('validationRecordId', f.lit(None).cast(\"String\"))\n",
                "\n",
                "        # FIXME: Automate best on self.sap_essential_columns\n",
                "        target_schema = self.modify_target_schema(target_schema = target_schema, \n",
                "                                                  incl_columns = self.sap_essential_columns, \n",
                "                                                  excl_columns = ['lakeId'])\n",
                "        \n",
                "        # target_schema = target_schema.add(StructField('DistrictId', StringType()))\\\n",
                "        #                             .add(StructField('SchoolYear', StringType()))\\\n",
                "        #                             .add(StructField('LastModifiedDate', TimestampType()))\\\n",
                "        #                             .add(StructField('RECORD', StringType()))\\\n",
                "        #                             .add(StructField('rundate', TimestampType())) \\\n",
                "        #                             .add(StructField('sap_pipeline', StringType())) \\\n",
                "        #                             .add(StructField('validationRecordId', StringType())) \\\n",
                "        #                             .add(StructField('stage1_source_url', StringType())) \\\n",
                "        #                             .add(StructField('NATURAL_KEY_HASH', StringType()))\n",
                "        # FIXME: Temporary Fix\n",
                "        if table_name.lower().endswith('exts'):\n",
                "            ext_entity_flag = ext_entity\n",
                "        else:\n",
                "            ext_entity_flag = None\n",
                "        \n",
                "        df = self.transform_sub_module(df = df, \n",
                "                                       target_schema = target_schema, \n",
                "                                       sink_general_path = sink_general_path, \n",
                "                                       schema_name = schema_name,\n",
                "                                       table_name = table_name, \n",
                "                                       extension = False,\n",
                "                                       ext_entity = 'ed-fi' if ext_entity_flag is None else ext_entity_flag) #NOTE: This represents None for all Exts ending tables\n",
                "\n",
                "        self.process_ext_column(df = df,\n",
                "                        schema_name = schema_name,\n",
                "                        table_name = table_name,\n",
                "                        ext_column_name = '_ext',\n",
                "                        ext_entity = ext_entity, #NOTE: Should always represent 'TX'\n",
                "                        target_schema = target_schema,\n",
                "                        sink_general_path = sink_general_path)\n",
                "\n",
                "\n",
                "        self.logger.info(f\"Writing Main Table - {table_name}\")    \n",
                "        if '_ext' in df.columns:\n",
                "            df = df.withColumn('_ext', f.lit(None).cast(target_schema['_ext'].dataType))\n",
                "        self.upsert_with_logging(df = df, \n",
                "                                 sap_pipeline = self.sap_pipeline, \n",
                "                                 destination_path = f\"{sink_general_path}\", \n",
                "                                 primary_key = 'lakeId', \n",
                "                                 partitioning = True,\n",
                "                                 partitioning_cols = ['DistrictId', 'SchoolYear'], \n",
                "                                 table_name = table_name,\n",
                "                                 ext_entity = 'ed-fi' if ext_entity_flag is None else ext_entity_flag,\n",
                "                                 parent = True) \n",
                "        if table_name.lower().endswith('descriptors'):\n",
                "            # NOTE: Reverting to non-None logic of else\n",
                "            self.oea.add_to_lake_db(sink_general_path, overwrite = True, extension = ext_entity_flag, table_type = 'non-empty-descriptor', pipeline_type = None)\n",
                "        else:\n",
                "            self.oea.add_to_lake_db(sink_general_path, overwrite = True, extension = ext_entity_flag, table_type = 'non-empty-non-descriptor', pipeline_type = None)        \n",
                "        return df\n",
                "\n",
                "    def transform_sub_module(self, \n",
                "                             df, \n",
                "                             target_schema, \n",
                "                             sink_general_path, \n",
                "                             schema_name, \n",
                "                             table_name, \n",
                "                             extension = False, \n",
                "                             ext_entity = 'ed-fi'):\n",
                "        if not(table_name.lower().endswith('descriptors')):\n",
                "            primary_key, clean_up_cols = self.extract_pk_from_definitions(table_name, \n",
                "                                                                      depluralize = True)\n",
                "        else:\n",
                "            primary_key = ['namespace', 'codeValue']\n",
                "            clean_up_cols = []\n",
                "        flatten_cols = []\n",
                "        arr_cols = []\n",
                "        descriptor_cols = []\n",
                "        for col_name in target_schema.fieldNames():\n",
                "            target_col = target_schema[col_name]\n",
                "            if target_col.dataType.typeName() in self.primitive_datatypes:\n",
                "                # If it is a Descriptor\n",
                "                if re.search('Descriptor$', col_name) is not None:\n",
                "                    df = self.modify_descriptor_value(df, col_name)\n",
                "                    descriptor_cols.append(col_name)\n",
                "                else:\n",
                "                    if col_name in df.columns:\n",
                "                        # Casting columns to primitive data types\n",
                "                        df = df.withColumn(col_name, f.col(col_name).cast(target_col.dataType))\n",
                "                    else:\n",
                "                        # If Column not present in dataframe, add column with None values.\n",
                "                        df = df.withColumn(col_name, f.lit(None).cast(target_col.dataType))\n",
                "            # If Complex datatype, i.e. Object, Array\n",
                "            else:\n",
                "                if col_name not in df.columns:\n",
                "                    df = df.withColumn(col_name, f.lit(None).cast(target_col.dataType))\n",
                "                elif (col_name == '_ext'):# or (extension == True):\n",
                "                    df = df.withColumn(f\"{col_name}_json\", f.to_json(f.col(col_name))).drop(f\"{col_name}_json\")\n",
                "                else:\n",
                "                    # Generate JSON column as a Complex Type\n",
                "                    if (col_name.lower() == 'paraprofessionalcertificationset') and (target_col.dataType.typeName() != 'array'):\n",
                "                        # FIXME: Temporary Fix to deal with paraprofessionalcertificationset\n",
                "                        df = df.withColumn(col_name, f.array(f.col(col_name)))\n",
                "                        target_col.dataType = f.ArrayType(target_col.dataType)\n",
                "                    \n",
                "                    df = df.withColumn(f\"{col_name}_json\", f.to_json(f.col(col_name))) \\\n",
                "                        .withColumn(col_name, f.from_json(f.col(f\"{col_name}_json\"), target_col.dataType)) \\\n",
                "                        .drop(f\"{col_name}_json\")\n",
                "                \n",
                "                # Modify the links with surrogate keys\n",
                "                if re.search('Reference$', col_name) is not None:\n",
                "                    flatten_cols.append(target_col)\n",
                "        \n",
                "                if target_col.dataType.typeName() == 'array':\n",
                "                    arr_cols.append(target_col)\n",
                "\n",
                "        for target_col in flatten_cols:\n",
                "            reference_key =  self.extract_reference_key(self.schemas[table_name][target_col.name])\n",
                "            df = self.flatten_reference_col(df, table_name,target_col, reference_key)\n",
                "        \n",
                "        if not(extension):\n",
                "            if \"LakeId\" in df.columns:\n",
                "                df = df.drop('LakeId')\n",
                "\n",
                "            df = df.withColumn(\"LakeId\", f.concat(f.col('DistrictId'), f.lit('_'), f.col('SchoolYear'), f.lit('_'), *[F.concat(F.col(f'{x}'), F.lit('_')) for x in primary_key[:-1]], F.col(f'{primary_key[-1]}')))\n",
                "            \n",
                "        for col_name in clean_up_cols:\n",
                "            if col_name.lower().endswith('reference') or col_name.lower().endswith('descriptor'):\n",
                "                if col_name in df.columns:\n",
                "                    df = df.drop(col_name)\n",
                "            else:\n",
                "                self.logger.info(f\"{col_name} may be required for analytics - not being dropped\")\n",
                "        \n",
                "        for col_name in descriptor_cols:\n",
                "            if col_name in df.columns:\n",
                "                df = df.drop(col_name)\n",
                "\n",
                "        for target_col in arr_cols:\n",
                "            df = self.explode_arrays(df, sink_general_path,target_col, schema_name, table_name, ext_entity)\n",
                "        \n",
                "        return df\n",
                "\n",
                "    def get_ext_entities_schemas(self,\n",
                "                                table_name = 'staffs',\n",
                "                                ext_column_name = '_ext',\n",
                "                                default_value = 'TPDM'):\n",
                "        target_schema = copy.deepcopy(self.schemas[table_name])\n",
                "        for col_name in target_schema.fieldNames():\n",
                "            target_col = target_schema[col_name]\n",
                "            if target_col.name == ext_column_name:\n",
                "                if target_col.dataType[0].name == default_value:\n",
                "                    return target_col.dataType[0].dataType         \n",
                "                    \n",
                "    def flatten_ext_column(self,\n",
                "                            df, \n",
                "                        table_name, \n",
                "                        ext_col, \n",
                "                        inner_key,\n",
                "                        ext_inner_cols\n",
                "                        ):\n",
                "        #TODO: Modify the complex sub type field name logic\n",
                "        cols = self.sap_essential_columns # ['RECORD','lakeId', 'DistrictId', 'SchoolYear', 'LastModifiedDate', 'rundate', 'sap_pipeline', 'sap_pipelineType', 'validationRecordId', 'stage1_source_url', 'NATURAL_KEY_HASH']\n",
                "        if table_name == 'staffs':\n",
                "            cols = cols + ['staffUniqueId']\n",
                "        \n",
                "        flattened_cols = ext_inner_cols#[\"educatorPreparationPrograms\"] #_ext_TX_cols[table_name]\n",
                "        dict_col = F.col(ext_col)[inner_key]\n",
                "        complex_dtype_text = str(df.select('_ext').dtypes[0][1])\n",
                "\n",
                "        exprs = [dict_col.getItem(key).alias(key) for key in flattened_cols if str(key) in complex_dtype_text]\n",
                "        flattened_df = df.select(exprs + cols)\n",
                "\n",
                "        return flattened_df"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "class SAPEdFiClient(EdFiClient):\n",
                "    def __init__(self, \n",
                "                 workspace, \n",
                "                 kvName, \n",
                "                 moduleName, \n",
                "                 authUrl, \n",
                "                 dataManagementUrl, \n",
                "                 changeQueriesUrl, \n",
                "                 dependenciesUrl, \n",
                "                 apiVersion, \n",
                "                 batchLimit, \n",
                "                 minChangeVer=\"\", \n",
                "                 maxChangeVer=\"\",\n",
                "                 landingDateTimeFormat = \"yyyyMMddHHmmss\", \n",
                "                 schoolYear=None, \n",
                "                 districtId=None, \n",
                "                 kvSecret_clientId = None, \n",
                "                 kvSecret_clientSecret = None,\n",
                "                 retry_strategy = None, \n",
                "                 threadMode = False, \n",
                "                 devMode = False,\n",
                "                 oea = None, \n",
                "                 final_columns = {}, \n",
                "                 lookup_table_name = '',\n",
                "                 lookup_table_base_path = '', \n",
                "                 lookup_db_name = ''):\n",
                "        # Call the constructor of the parent class\n",
                "        super().__init__(workspace = workspace, \n",
                "                         kvName = kvName, \n",
                "                         moduleName = moduleName, \n",
                "                         authUrl = authUrl, \n",
                "                         dataManagementUrl = dataManagementUrl, \n",
                "                         changeQueriesUrl = changeQueriesUrl, \n",
                "                         dependenciesUrl = dependenciesUrl, \n",
                "                         apiVersion = apiVersion, \n",
                "                         batchLimit = batchLimit, \n",
                "                         minChangeVer = minChangeVer, \n",
                "                         maxChangeVer = maxChangeVer, \n",
                "                         landingDateTimeFormat = landingDateTimeFormat,\n",
                "                         schoolYear = schoolYear, \n",
                "                         districtId = districtId, \n",
                "                         kvSecret_clientId = kvSecret_clientId, \n",
                "                         kvSecret_clientSecret = kvSecret_clientSecret,\n",
                "                         retry_strategy = retry_strategy, \n",
                "                         threadMode = threadMode, \n",
                "                         devMode = devMode)\n",
                "\n",
                "        # Additional arguments specific to SAPEdFiClient\n",
                "        self.success_logs = []\n",
                "        self.error_logs = []\n",
                "        self.edfi_id_records = []\n",
                "        \n",
                "        self.edfi_id_record_schema = StructType([\n",
                "                                    StructField(\"edfi_location\", StringType(), True),\n",
                "                                    StructField(\"edfi_id\", StringType(), True),\n",
                "                                    StructField(\"edfi_id_modified\", StringType(), True),\n",
                "                                    StructField(\"entity_name\", StringType(), True),\n",
                "                                    StructField(\"resource\", StringType(), True),\n",
                "                                    StructField(\"staffUniqueId\", StringType(), True),\n",
                "                                    StructField(\"recordAPIRefreshDateTime\", StringType(), True),\n",
                "                                    StructField(\"isDeleted\", BooleanType(), True)\n",
                "                                    ])\n",
                "        self.log_schema = StructType([\n",
                "                        StructField(\"pipeline_execution_id\", StringType(), True),\n",
                "                        StructField(\"pipelineExecutionId\", StringType(), True),\n",
                "                        StructField(\"run_id\", StringType(), True),\n",
                "                        StructField(\"operation_type\", StringType(), True),\n",
                "                        StructField(\"request_payload\", StringType(), True),\n",
                "                        StructField(\"request_url\", StringType(), True),\n",
                "                        StructField(\"resource\", StringType(), True),\n",
                "                        StructField(\"entity_name\", StringType(), True),\n",
                "                        StructField(\"response_headers\", StringType(), True),\n",
                "                        StructField(\"response_status_code\", StringType(), True),\n",
                "                        StructField(\"response_text\", StringType(), True),\n",
                "                        StructField(\"rundate_time\", StringType(), True),\n",
                "                        StructField(\"start_time\", StringType(), True),\n",
                "                        StructField(\"end_time\", StringType(), True),\n",
                "                        StructField(\"record_id\", StringType(), True),\n",
                "                        StructField(\"lakeId\", StringType(), True),\n",
                "                        StructField(\"edfi_location\", StringType(), True),\n",
                "                        StructField(\"edfi_id\", StringType(), True),\n",
                "                        StructField(\"edfi_id_modified\", StringType(), True),                                    \n",
                "                        StructField(\"sap_pipeline\", StringType(), True),\n",
                "                        StructField(\"sap_pipelineType\", StringType(), True),\n",
                "                        StructField(\"stage1_source_url\", StringType(), True),\n",
                "                        StructField(\"NATURAL_KEY_HASH\", StringType(), True)                    \n",
                "                        ])\n",
                "        self.final_columns = final_columns\n",
                "        self.oea = oea\n",
                "        self.clientId = oea._get_secret(kvSecret_clientId) # oea._get_secret(\"oea-edfi-api-client-id\") if clientId is None else clientId\n",
                "        self.clientSecret = oea._get_secret(kvSecret_clientSecret) # oea._get_secret(\"oea-edfi-api-client-secret\") if clientSecret is None else clientSecretself.max_rundates = list()\n",
                "        self.lookup_table_name = lookup_table_name\n",
                "        self.lookup_table_path = f\"{lookup_table_base_path}/{lookup_table_name}\"\n",
                "        self.lookup_db_name = lookup_db_name\n",
                "        self.schoolYear = schoolYear \n",
                "        self.districtId = districtId\n",
                "        self.metadata_logging_keys = ['RECORD','lakeId','stage1_source_url', 'sap_pipeline', 'sap_pipelineType', 'NATURAL_KEY_HASH']\n",
                "        self.max_rundates = list()\n",
                "         \n",
                "    def cleanup_discriptor_col(self,\n",
                "                          descriptor_prefix, \n",
                "                          descriptor_key, \n",
                "                          descriptor_value, \n",
                "                          descriptor_type):\n",
                "        descriptor_col_name = f'{descriptor_prefix}{descriptor_type}'\n",
                "        descriptor_key = descriptor_key[0].upper() + descriptor_key[1:] \n",
                "        if descriptor_col_name == descriptor_key:\n",
                "            descriptor_value = descriptor_value.replace(f'/{descriptor_type}#', f'/{descriptor_key}#')\n",
                "        return descriptor_value\n",
                "\n",
                "    def cleanup_dict(self, original_item):\n",
                "        metadata_logging_values = dict()\n",
                "        item = copy.copy(original_item)\n",
                "        keys_to_remove = []\n",
                "\n",
                "        if 'staffUniqueId' in item:\n",
                "            staffUniqueId = item['staffUniqueId']\n",
                "        elif 'staffReference' in item:\n",
                "            staffUniqueId = item['staffReference']['staffUniqueId']\n",
                "        else:\n",
                "            staffUniqueId = 'INVALID_COLUMN'\n",
                "        \n",
                "        # FIXME: 2024-02-01 TEMP FIX\n",
                "        if ('fiscalYear' in item) and ('actualFundDescriptor' in item) or ('budgetFundDescriptor' in item):\n",
                "            item['fiscalYear'] = item['fiscalYear'][-1]\n",
                "\n",
                "        if 'races' in item:\n",
                "            final_races = []\n",
                "            for index, raceDescriptor in enumerate(item['races']):\n",
                "                for raceKey, raceValue in raceDescriptor.items():\n",
                "                    if raceValue != 'uriPlaceholder#NA':\n",
                "                        final_races.append(raceDescriptor)\n",
                "                item['races'] = final_races\n",
                "        \n",
                "        for key in item:\n",
                "            if key in self.metadata_logging_keys:\n",
                "                metadata_logging_values[key] = item[key]\n",
                "                keys_to_remove.append(key)\n",
                "\n",
                "            if key != '_ext':\n",
                "                if (item[key] == 'uriPlaceholder#NA') or (item[key] is None):\n",
                "                    keys_to_remove.append(key)\n",
                "            # FIXME: Temporary\n",
                "            payrollExt_expr = (key.lower().startswith('payroll')) and (key.lower().endswith('descriptor'))\n",
                "            budgetExt_expr = (key.lower().startswith('budget')) and (key.lower().endswith('descriptor'))\n",
                "            actualExt_expr = (key.lower().startswith('actual')) and (key.lower().endswith('descriptor'))\n",
                "            \n",
                "            \n",
                "            if payrollExt_expr: \n",
                "                item[key] = self.cleanup_discriptor_col('Payroll', key, item[key], 'FundDescriptor')\n",
                "                item[key] = self.cleanup_discriptor_col('Payroll', key, item[key], 'FunctionDescriptor')\n",
                "                item[key] = self.cleanup_discriptor_col('Payroll', key, item[key], 'ObjectDescriptor')\n",
                "                item[key] = self.cleanup_discriptor_col('Payroll', key, item[key], 'ProgramIntentDescriptor')\n",
                "\n",
                "            elif budgetExt_expr:\n",
                "                item[key] = self.cleanup_discriptor_col('Budget', key, item[key], 'FundDescriptor')\n",
                "                item[key] = self.cleanup_discriptor_col('Budget', key, item[key], 'FunctionDescriptor')\n",
                "                item[key] = self.cleanup_discriptor_col('Budget', key, item[key], 'ObjectDescriptor')\n",
                "                item[key] = self.cleanup_discriptor_col('Budget', key, item[key], 'ProgramIntentDescriptor')\n",
                "            \n",
                "            elif actualExt_expr:\n",
                "                item[key] = self.cleanup_discriptor_col('Actual', key, item[key], 'FundDescriptor')\n",
                "                item[key] = self.cleanup_discriptor_col('Actual', key, item[key], 'FunctionDescriptor')\n",
                "                item[key] = self.cleanup_discriptor_col('Actual', key, item[key], 'ObjectDescriptor')\n",
                "                item[key] = self.cleanup_discriptor_col('Actual', key, item[key], 'ProgramIntentDescriptor')\n",
                "            \n",
                "            elif key == 'ciStaffProgramIntentDescriptor':\n",
                "                item[key] = item[key].replace('/ProgramIntentDescriptor#', '/CIStaffProgramIntentDescriptor#')\n",
                "            \n",
                "        for key in keys_to_remove:\n",
                "            item.pop(key)\n",
                "\n",
                "        if ('_ext' in item) and ('TX' in item['_ext']):\n",
                "            inner_keys_to_remove = []\n",
                "            for ext_item in item['_ext']['TX']:\n",
                "                if (item['_ext']['TX'][ext_item] is None) or (item['_ext']['TX'][ext_item] == 'uriPlaceholder#NA'):\n",
                "                    inner_keys_to_remove.append(ext_item)\n",
                "                if (ext_item.endswith('Sets')):\n",
                "                    for inner_set_item_dict in item['_ext']['TX'][ext_item]:\n",
                "                        for inner_key, inner_value in inner_set_item_dict.items(): \n",
                "                            if inner_value == 'uriPlaceholder#NA':\n",
                "                                inner_keys_to_remove.append(ext_item)\n",
                "                                break\n",
                "                if (ext_item == 'staffDoNotReportTSDS'):\n",
                "                    item['_ext']['TX'][ext_item] = bool(item['_ext']['TX'][ext_item])\n",
                "                if (ext_item == 'paraprofessionalCertificationSet'):\n",
                "                    inner_set_item_dict = item['_ext']['TX'][ext_item][0]\n",
                "                    #for inner_set_item_dict in item['_ext']['TX'][ext_item]:\n",
                "                    if 'beginDate' not in inner_set_item_dict.keys():\n",
                "                        #print('beginDate ABSENT')\n",
                "                        inner_keys_to_remove.append(ext_item) \n",
                "                    item['_ext']['TX'][ext_item] = inner_set_item_dict\n",
                "\n",
                "            # Remove the keys from the dictionary\n",
                "            for key in inner_keys_to_remove:\n",
                "                item['_ext']['TX'].pop(key)\n",
                "        \n",
                "        if ('_ext' in item) and ('TX' in item['_ext']):\n",
                "            if item['_ext']['TX'] == {}:\n",
                "                item.pop('_ext')\n",
                "        return item, metadata_logging_values, staffUniqueId \n",
                "\n",
                "    def cast_column_to_bool(self, df, column_name):\n",
                "        # Define a user-defined function (UDF) to convert string to bool\n",
                "        from pyspark.sql.functions import udf\n",
                "        from pyspark.sql.types import BooleanType\n",
                "        \n",
                "        def str_to_bool(s):\n",
                "            return s == \"1\"\n",
                "        str_to_bool_udf = udf(str_to_bool, BooleanType())\n",
                "        df = df.withColumn(column_name, str_to_bool_udf(col(column_name)))\n",
                "        return df\n",
                "\n",
                "    def get_latest_submission_records(self, df, lookup_table_name, filtering_date = 'rundate',resource_name = '',sap_pipeline = '', sap_pipelineType = '',operationType = 'submission',debugMode = False):\n",
                "        # FIXME 2024-01-24: Under Refactoring\n",
                "        maxdatetime = None\n",
                "        maxRecordVersion = None\n",
                "        try:\n",
                "            lookup_df = spark.sql(f\"\"\"\n",
                "                                    SELECT lastSubmissionMaxRunDate as maxdatetime,\n",
                "                                           lastSubmissionMaxRecordVersion as maxRecordVersion\n",
                "                                    FROM {self.lookup_db_name}.{lookup_table_name} \n",
                "                                    WHERE resource_name = '{resource_name}'\n",
                "                                    AND sap_pipeline = '{sap_pipeline}'\n",
                "                                    AND sap_pipelineType = '{sap_pipelineType}'\n",
                "                                \"\"\")\n",
                "            lookup_df_first = lookup_df.first()\n",
                "            if lookup_df_first is not None:\n",
                "                maxdatetime = lookup_df_first['maxdatetime']\n",
                "                maxRecordVersion = lookup_df_first['maxRecordVersion']\n",
                "            else:\n",
                "                # FIXME: Temporary Fix\n",
                "                maxdatetime = '2020-11-25'\n",
                "                maxRecordVersion = 0\n",
                "        except AnalysisException as e:\n",
                "            pass\n",
                "\n",
                "        if maxdatetime and not(debugMode):\n",
                "            if operationType == 'submission':\n",
                "                df = df.where(f\"{filtering_date} > '{maxdatetime}'\").where(f'RECORD_VERSION > {maxRecordVersion}')\n",
                "            elif operationType == 'delete':\n",
                "                df = df.where(f\"{filtering_date} > '{maxdatetime}'\").where(f'RECORD_VERSION >= {maxRecordVersion}')\n",
                "            else:\n",
                "                df = df.where(f\"{filtering_date} > '{maxdatetime}'\")\n",
                "        return df\n",
                "    \n",
                "    def store_maxRunDates(self, df, resource_name):\n",
                "        # FIXME 2024-01-24: Under Refactoring\n",
                "        df.createOrReplaceTempView('SUBMISSION_SESSION_TEMP_VIEW')\n",
                "        result = spark.sql(\"\"\"SELECT max(rundate) as maxRunDate,\n",
                "                                     max(RECORD_VERSION) as maxRecordVersion, \n",
                "                                     sap_pipeline,\n",
                "                                     sap_pipelineType, \n",
                "                                     DistrictId, \n",
                "                                     SchoolYear \n",
                "                              FROM SUBMISSION_SESSION_TEMP_VIEW\n",
                "                              GROUP BY DistrictId, \n",
                "                                       SchoolYear, \n",
                "                                       sap_pipeline,\n",
                "                                       sap_pipelineType\"\"\")\n",
                "        if result.first() is not None:\n",
                "            maxRunDate = result.first()['maxRunDate']\n",
                "            maxRecordVersion = result.first()['maxRecordVersion']\n",
                "            sap_pipeline = result.first()['sap_pipeline']\n",
                "            sap_pipelineType = result.first()['sap_pipelineType']\n",
                "            submission_districtId = result.first()['DistrictId']\n",
                "            submission_schoolYear = result.first()['SchoolYear']\n",
                "\n",
                "            temp_dict = dict()\n",
                "            temp_dict['resource_name'] = resource_name\n",
                "            temp_dict['lastSubmissionMaxRunDate'] = maxRunDate\n",
                "            temp_dict['lastSubmissionMaxRecordVersion'] = maxRecordVersion\n",
                "            temp_dict['sap_pipeline'] = sap_pipeline\n",
                "            temp_dict['sap_pipelineType'] = sap_pipelineType\n",
                "            temp_dict['DistrictId'] = submission_districtId\n",
                "            temp_dict['SchoolYear'] = submission_schoolYear\n",
                "            temp_dict['recordAPIRefreshDateTime'] = datetime.now()\n",
                "            \n",
                "            with self.lock:\n",
                "                self.max_rundates.append(temp_dict)\n",
                "\n",
                "    def return_submission_type(self,\n",
                "                               df):\n",
                "        # FIXME Under dev and review\n",
                "        sap_pipeline = df.first()['sap_pipeline']\n",
                "        sap_pipelineType = df.first()['sap_pipelineType']\n",
                "        return sap_pipeline, sap_pipelineType\n",
                "    \n",
                "    def load_by_SY_DI(self, path):\n",
                "        # FIXME Under dev and review\n",
                "        df = spark.read.format('delta').load(oea.to_url(path)).filter(f\"DistrictId == {self.districtId}\").filter(f\"SchoolYear == {self.schoolYear}\")\n",
                "        return df\n",
                "    \n",
                "    def process_submission_entities(self, df, resource_name):\n",
                "        # FIXME Under dev and review\n",
                "        if resource_name == 'budgetExts':\n",
                "            df = df.withColumn(\"budgetAmount\", round(F.col(\"budgetAmount\")).cast(\"int\"))\n",
                "        if resource_name == 'actualExts':\n",
                "            df = df.withColumn(\"actualAmount\", round(F.col(\"actualAmount\")).cast(\"int\"))\n",
                "        if resource_name == 'payrollExts':\n",
                "            df = df.withColumn(\"payrollAmount\", round(F.col(\"payrollAmount\")).cast(\"int\"))\n",
                "        if resource_name == 'staffEducationOrganizationAssignmentAssociations':\n",
                "            pass\n",
                "        if resource_name == 'staffEducationOrganizationEmploymentAssociations':\n",
                "            pass\n",
                "            #df = df.withColumn('employmentStatusDescriptor', lit('uri://ed-fi.org/employmentStatusDescriptor#Other')) \n",
                "        if resource_name == 'staffs':\n",
                "            df = self.cast_column_to_bool(df, 'hispanicLatinoEthnicity')\n",
                "        return df\n",
                "    \n",
                "    def filter_out_invalid_vals(self, df, resource_name):\n",
                "        # FIXME: TEMP FIX to filter out unecessary variables\n",
                "        if resource_name == 'actualExts':\n",
                "            df.createOrReplaceTempView(\"temp_vw_actualExts\")\n",
                "            df = spark.sql(\"\"\"\n",
                "                            SELECT *\n",
                "                            FROM temp_vw_actualExts\n",
                "                            WHERE NOT (\n",
                "                                actualFundDescriptor LIKE '%uriPlaceholder#NA%' \n",
                "                                OR actualObjectDescriptor LIKE '%uriPlaceholder#NA%'\n",
                "                                OR actualProgramIntentDescriptor LIKE '%uriPlaceholder#NA%'\n",
                "                                OR actualFunctionDescriptor LIKE '%uriPlaceholder#NA%'\n",
                "                            )\n",
                "                        \"\"\")\n",
                "        return df\n",
                "        \n",
                "    def loadDataFromStage3IntoJSON(self,\n",
                "                                   resource_name, \n",
                "                                   file_path):\n",
                "        # FIXME Under dev and review\n",
                "        try:\n",
                "            df = self.load_by_SY_DI(file_path)\n",
                "            sap_pipeline, sap_pipelineType = self.return_submission_type(df)\n",
                "            df = self.get_latest_submission_records(df = df, \n",
                "                                                    lookup_table_name = self.lookup_table_name,#'submissions_lookup_table', \n",
                "                                                    filtering_date = 'rundate',\n",
                "                                                    resource_name = resource_name,\n",
                "                                                    sap_pipeline = sap_pipeline,\n",
                "                                                    sap_pipelineType = sap_pipelineType,\n",
                "                                                    operationType = 'submission',\n",
                "                                                    debugMode = False)\n",
                "            df = df.filter(df['SUBMISSION_RECORD_IS_ACTIVE'] == True)\n",
                "            self.store_maxRunDates(df, resource_name)\n",
                "            df = self.process_submission_entities(df, resource_name)\n",
                "            \n",
                "            post_cols = self.final_columns[resource_name]\n",
                "            df = self.filter_columns(df, post_cols)\n",
                "            df = self.filter_out_invalid_vals(df, resource_name)\n",
                "            df = df.cache()\n",
                "            \n",
                "            json_df = df.toJSON().map(lambda x: json.loads(x)).collect()\n",
                "            json_str = df.toJSON().collect()\n",
                "\n",
                "            return json_df, json_str,df\n",
                "        except Exception as error:\n",
                "            logger.exception(f'An Error Occured - {error}')\n",
                "            return None, None, None\n",
                "\n",
                "    def getDataForEdFiPosts(self,\n",
                "                        resource_names = None,\n",
                "                    file_path = None,\n",
                "                    resource_json_dict = dict()\n",
                "                    ):\n",
                "        for resource_name in resource_names:\n",
                "            resource_path = f'{file_path}/{resource_name}'            \n",
                "            logger.info(f\"Stage 3 Path - {resource_path}\")\n",
                "            json_list,json_str,temp_df = self.loadDataFromStage3IntoJSON(resource_name, \n",
                "                                                                    resource_path)\n",
                "            if json_list is not None:\n",
                "                resource_json_dict[resource_name] = json_list\n",
                "            else:\n",
                "                print(f'Error in loading in resource = {resource_name}')\n",
                "        return resource_json_dict\n",
                "    \n",
                "    def return_lookup_table_as_spark_df(self):\n",
                "        # FIXME Under dev and review\n",
                "        df = spark.createDataFrame(self.max_rundates)\n",
                "        return df\n",
                "    \n",
                "    def dump_lookup_table(self):\n",
                "        # FIXME Under dev and review\n",
                "        df = self.return_lookup_table_as_spark_df()\n",
                "        self.oea.upsert(df = df, \n",
                "                   destination_path = self.lookup_table_path,\n",
                "                   primary_key = ['resource_name', 'sap_pipeline', 'sap_pipelineType', 'DistrictId', 'SchoolYear'],\n",
                "                   partitioning = True,\n",
                "                   partitioning_cols = [],\n",
                "                   surrogate_key = False) \n",
                "    \n",
                "    def add_lookup_table_to_lake_db(self, overwrite = True):\n",
                "        # FIXME Under dev and review\n",
                "        spark.sql(f'CREATE DATABASE IF NOT EXISTS {self.lookup_db_name}')\n",
                "        if overwrite:\n",
                "            spark.sql(f\"drop table if exists {self.lookup_db_name}.{self.lookup_table_name}\")\n",
                "        spark.sql(f\"create table if not exists {self.lookup_db_name}.{self.lookup_table_name} using DELTA location '{self.oea.to_url(self.lookup_table_path)}'\")\n",
                "\n",
                "    def generate_edfi_id_record(self, **kwargs):\n",
                "        record = {'edfi_location': str(kwargs.get('edfi_location', '')),\n",
                "                  'edfi_id': str(kwargs.get('edfi_id', '')),\n",
                "                  'edfi_id_modified': f\"{self.districtId}_{self.schoolYear}_{str(kwargs.get('edfi_id', ''))}\",\n",
                "                  'entity_name' : str(kwargs.get('entity_name', '')),\n",
                "                  'resource': str(kwargs.get('resource', '')),\n",
                "                  'staffUniqueId': str(kwargs.get('staffUniqueId', '')),\n",
                "                  'recordAPIRefreshDateTime': str(kwargs.get('recordAPIRefreshDateTime', '')),\n",
                "                  'isDeleted': kwargs.get('isDeleted', False),\n",
                "                  }\n",
                "        return record\n",
                "\n",
                "    def generate_log_record(self, **kwargs):\n",
                "        # TODO: Add RUN ID which is <> pipeline id\n",
                "        # FIXME: pipelineExecutionId is the alias of pipeline_execution_id\n",
                "        log_record = {\n",
                "            'pipeline_execution_id': str(kwargs.get('pipeline_execution_id', '')),\n",
                "            'pipelineExecutionId': str(kwargs.get('pipeline_execution_id', '')),\n",
                "            'run_id': str(kwargs.get('run_id', '')),\n",
                "            'operation_type': str(kwargs.get('operation_type', '')),\n",
                "            'resource': str(kwargs.get('resource', '')),\n",
                "            'entity_name': str(kwargs.get('entity_name', '')),\n",
                "            'request_url': str(kwargs.get('request_url', '')),\n",
                "            'request_payload': str(kwargs.get('json_object', '')),\n",
                "            'response_status_code': str(kwargs.get('response_status_code', '')),\n",
                "            'response_text': str(kwargs.get('response_text', '')),\n",
                "            'response_headers': str(kwargs.get('response_headers', '')),\n",
                "            'rundate_time': str(kwargs.get('rundate_time', '')),\n",
                "            'start_time': str(kwargs.get('start_time', '')),\n",
                "            'end_time': str(kwargs.get('end_time', '')),\n",
                "            'record_id': str(kwargs.get('record_id', '')),\n",
                "            'lakeId': str(kwargs.get('lakeId', '')),\n",
                "            'edfi_location': str(kwargs.get('edfi_location', '')),\n",
                "            'edfi_id': str(kwargs.get('edfi_id', '')),\n",
                "            'edfi_id_modified': f\"{self.districtId}_{self.schoolYear}_{str(kwargs.get('edfi_id', ''))}\",\n",
                "            'sap_pipeline': str(kwargs.get('sap_pipeline', '')),\n",
                "            'sap_pipelineType': str(kwargs.get('sap_pipelineType', '')),\n",
                "            'stage1_source_url': str(kwargs.get('stage1_source_url', '')),\n",
                "            'NATURAL_KEY_HASH': str(kwargs.get('NATURAL_KEY_HASH', ''))\n",
                "            }\n",
                "        return log_record\n",
                "    \n",
                "    def send_delete_request(self,\n",
                "                        pipeline_execution_id,\n",
                "                        run_id,\n",
                "                        resource,\n",
                "                        chunk_num, \n",
                "                        chunk, \n",
                "                        url, \n",
                "                        success_logging = True,\n",
                "                        error_logging = False):\n",
                "        self.init_thread_local_vars()\n",
                "        rundate_time=datetime.now()\n",
                "        try:\n",
                "            for json_object in chunk:\n",
                "                edfi_id = json_object.get('edfi_id')\n",
                "                edfi_location = json_object.get('edfi_location')\n",
                "                delete_url = f\"{self.dataManagementUrl}{resource}/{edfi_id}\"\n",
                "                \n",
                "                start_time = datetime.now()\n",
                "                requests_session = self.getSession()\n",
                "                response = requests.delete(delete_url, headers={\"Authorization\": f\"Bearer {self.getAccessToken()}\"})\n",
                "                end_time = datetime.now()\n",
                "\n",
                "            \n",
                "                logged_record = self.generate_log_record(pipeline_execution_id = pipeline_execution_id,\n",
                "                                                            run_id = run_id,\n",
                "                                                            operation_type = 'delete',\n",
                "                                                            resource = resource,\n",
                "                                                            entity_name = resource.split('/')[-1],\n",
                "                                                            request_url = url,\n",
                "                                                            json_object = json_object,\n",
                "                                                            response_status_code = response.status_code,\n",
                "                                                            response_text = response.text,\n",
                "                                                            response_headers = response.headers,\n",
                "                                                            rundate_time = rundate_time,\n",
                "                                                            start_time = start_time,\n",
                "                                                            end_time = end_time,\n",
                "                                                            stage1_source_url = 'INVALID_COL_FOR_DELETE', # metadata_logging_values['stage1_source_url'],\n",
                "                                                            record_id = 'INVALID_COL_FOR_DELETE', # metadata_logging_values['RECORD'],\n",
                "                                                            lakeId = 'INVALID_COL_FOR_DELETE', # metadata_logging_values['lakeId'],\n",
                "                                                            edfi_location = response.headers.get('location', edfi_location), # FIXME: Under Review\n",
                "                                                            edfi_id = response.headers.get('location', edfi_id).split('/')[-1], # FIXME: Under Review\n",
                "                                                            sap_pipeline = 'INVALID_COL_FOR_DELETE', \n",
                "                                                            sap_pipelineType = 'INVALID_COL_FOR_DELETE', \n",
                "                                                            NATURAL_KEY_HASH = 'INVALID_COL_FOR_DELETE' # str(metadata_logging_values['NATURAL_KEY_HASH'])\n",
                "                                                            )\n",
                "\n",
                "                if response.status_code < 400:\n",
                "                    if success_logging:\n",
                "                        logger.info(f'SUCCESS - {response.status_code}')\n",
                "                        with self.lock:\n",
                "                            self.success_logs.append(logged_record)\n",
                "                        edfi_id_record = self.generate_edfi_id_record(edfi_location = edfi_location,# response.headers.get('location'),\n",
                "                                                                      edfi_id = edfi_id,# response.headers.get('location').split('/')[-1],\n",
                "                                                                      resource = resource,\n",
                "                                                                      entity_name = resource.split('/')[-1],\n",
                "                                                                      staffUniqueId = 'INVALID_PLACEHOLDER',\n",
                "                                                                      recordAPIRefreshDateTime = datetime.now(),\n",
                "                                                                      isDeleted = True)\n",
                "                        with self.lock:\n",
                "                            self.edfi_id_records.append(edfi_id_record)\n",
                "                else:\n",
                "                    if error_logging:\n",
                "                        logger.info(f\"ERROR Code - {response.status_code}\")\n",
                "                        with self.lock:\n",
                "                            self.error_logs.append(logged_record)\n",
                "        except Exception as e:\n",
                "            logger.info(f\"ERROR: {e}\")\n",
                "\n",
                "    def send_api_request(self,\n",
                "                        pipeline_execution_id,\n",
                "                        run_id,\n",
                "                        resource,\n",
                "                        chunk_num, \n",
                "                        chunk, \n",
                "                        url, \n",
                "                        success_logging = True,\n",
                "                        error_logging = False):\n",
                "        self.init_thread_local_vars()\n",
                "        rundate_time=datetime.now()\n",
                "        try:\n",
                "            for json_object in chunk:\n",
                "                start_time = datetime.now()\n",
                "                json_object, metadata_logging_values, staffUniqueId = self.cleanup_dict(json_object)\n",
                "                headers = {\n",
                "                        'Authorization': f\"Bearer {self.getAccessToken()}\",\n",
                "                        'Content-Type': 'application/json'\n",
                "                        }\n",
                "                requests_session = self.getSession()\n",
                "                response = requests_session.post(url, json=json_object, headers=headers)\n",
                "                end_time = datetime.now()\n",
                "\n",
                "                logged_record = self.generate_log_record(pipeline_execution_id = pipeline_execution_id,\n",
                "                                                            run_id = run_id,\n",
                "                                                            resource = resource,\n",
                "                                                            operation_type = 'insert-or-update',\n",
                "                                                            entity_name = resource.split('/')[-1],\n",
                "                                                            request_url = url,\n",
                "                                                            json_object = json_object,\n",
                "                                                            response_status_code = response.status_code,\n",
                "                                                            response_text = response.text,\n",
                "                                                            response_headers = response.headers,\n",
                "                                                            rundate_time = rundate_time,\n",
                "                                                            start_time = start_time,\n",
                "                                                            end_time = end_time,\n",
                "                                                            stage1_source_url = metadata_logging_values['stage1_source_url'],\n",
                "                                                            record_id = metadata_logging_values['RECORD'],\n",
                "                                                            lakeId = metadata_logging_values['lakeId'],\n",
                "                                                            edfi_location = response.headers.get('location', 'INVALID_VALUE_PLACEHOLDER'),\n",
                "                                                            edfi_id = response.headers.get('location', 'INVALID_VALUE_PLACEHOLDER').split('/')[-1],\n",
                "                                                            sap_pipeline = metadata_logging_values['sap_pipeline'],\n",
                "                                                            sap_pipelineType = metadata_logging_values['sap_pipelineType'],\n",
                "                                                            NATURAL_KEY_HASH = str(metadata_logging_values['NATURAL_KEY_HASH'])\n",
                "                                                            )\n",
                "\n",
                "                if response.status_code < 400:\n",
                "                    if success_logging:\n",
                "                        logger.info(f'SUCCESS - {response.status_code}')\n",
                "                        with self.lock:\n",
                "                            self.success_logs.append(logged_record)\n",
                "\n",
                "                        edfi_id_record = self.generate_edfi_id_record(edfi_location = response.headers.get('location'),\n",
                "                                                                      edfi_id = response.headers.get('location').split('/')[-1],\n",
                "                                                                      resource = resource,\n",
                "                                                                      entity_name = resource.split('/')[-1],\n",
                "                                                                      staffUniqueId = staffUniqueId,\n",
                "                                                                      recordAPIRefreshDateTime = datetime.now(),\n",
                "                                                                      isDeleted = False)\n",
                "                        with self.lock:\n",
                "                            self.edfi_id_records.append(edfi_id_record)\n",
                "                else:\n",
                "                    if error_logging:\n",
                "                        #logger.info(f\"There was an error submitting data for {resource}\")\n",
                "                        logger.error(f\"[POST TO ED-FI] {response.status_code}\")\n",
                "                        with self.lock:\n",
                "                            self.error_logs.append(logged_record)\n",
                "        except Exception as e:\n",
                "            logger.info(f\"ERROR: {e}\")\n",
                "\n",
                "    # Function to upsert records using multi-threading\n",
                "    def upsert_records(self,\n",
                "                    pipeline_execution_id,\n",
                "                    run_id,\n",
                "                    resource, \n",
                "                    resource_name, \n",
                "                    records, \n",
                "                    chunk_size = 500, \n",
                "                    num_threads = 10, \n",
                "                    function_name = 'post',\n",
                "                    success_logging = True, \n",
                "                    error_logging = False):\n",
                "        logger.info(f\"Initiating {resource}\")\n",
                "        logger.info(f\"Processing {len(records)} records\")\n",
                "        self.init_thread_local_vars()\n",
                "\n",
                "        url = f\"{self.dataManagementUrl}{resource}\"\n",
                "        chunks = [records[i:i + chunk_size] for i in range(0, len(records), chunk_size)]\n",
                "        threads = []\n",
                "        del records\n",
                "\n",
                "        with ThreadPoolExecutor(max_workers=num_threads) as tpe:\n",
                "            if function_name == 'post':\n",
                "                for chunk_num, chunk in enumerate(chunks):\n",
                "                    tpe.submit(self.send_api_request, pipeline_execution_id, run_id,resource, chunk_num, chunk, url,success_logging, error_logging)\n",
                "            elif function_name == 'delete':\n",
                "                for chunk_num, chunk in enumerate(chunks):\n",
                "                    tpe.submit(self.send_delete_request, pipeline_execution_id, run_id,resource, chunk_num, chunk, url,success_logging, error_logging)\n",
                "\n",
                "        # Print a completion message after all records have been upserted\n",
                "        logger.info(f\"All {resource} calls completed.\")\n",
                "        del chunks\n",
                "    \n",
                "    def deleteEntityById(self, resource, id):\n",
                "        try:\n",
                "            url = f\"{self.dataManagementUrl}{resource}/{id}\"\n",
                "            response = requests.delete(url, headers={\"Authorization\": f\"Bearer {self.getAccessToken()}\"})\n",
                "            if response.status_code == 404:\n",
                "                logger.info(\"RESOURCE NOT FOUND\")\n",
                "            if response.status_code < 400:\n",
                "                logger.info(f\"RESOURCE DELETED - {id}\")\n",
                "            return response\n",
                "        except Exception as error:\n",
                "            logger.error(error)\n",
                "    \n",
                "    def filter_columns(self, df, column_list):\n",
                "        # TODO: USE the one present in the class named SAP Utilities instead\n",
                "        existing_columns = [col for col in column_list if col in df.columns]\n",
                "        return df.select(existing_columns)"
            ],
            "outputs": []
        }
    ]
}