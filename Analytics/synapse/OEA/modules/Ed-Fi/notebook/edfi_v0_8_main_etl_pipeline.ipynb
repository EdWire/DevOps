{
    "nbformat": 4,
    "nbformat_minor": 2,
    "metadata": {
        "save_output": true,
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "execution_count": 61,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# NOTE: DO NOT REMOVE\r\n",
                "minChangeVer = None\r\n",
                "maxChangeVer = None\r\n",
                "parameterized = False\r\n",
                "# kvSecret_devMode = True \r\n",
                "\r\n",
                "# FIXME: Deprecated Params (Under Review)\r\n",
                "metadataUrl = '<DEPRECATED PARAM>'\r\n",
                "kvName = '<DEPRECATED PARAM>'"
            ],
            "outputs": []
        },
        {
            "execution_count": 62,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "apiLimit = batchLimit #= 200\r\n",
                "prepareEdFiMetaData = prepareEdFiMetadata #= False\r\n",
                "client_id = kvSecret_clientId\r\n",
                "client_secret = kvSecret_clientSecret"
            ],
            "outputs": []
        },
        {
            "execution_count": 63,
            "cell_type": "code",
            "source": [
                "from notebookutils import mssparkutils\r\n",
                "from datetime import datetime"
            ],
            "outputs": []
        },
        {
            "execution_count": 64,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "start_time = datetime.now()"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### URL Initializations"
            ],
            "outputs": []
        },
        {
            "execution_count": 65,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "%run OEA/modules/Ed-Fi/v0.7/src/utilities/edfi_v0_7_fetch_urls"
            ],
            "outputs": []
        },
        {
            "execution_count": 66,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "instance_id = instanceId\r\n",
                "school_year = schoolYear\r\n",
                "api_url = apiUrl\r\n",
                "\r\n",
                "edfi_api_manager = EdFiApiManager(api_url, instance_id, school_year)\r\n",
                "edfi_api_manager.update_urls()\r\n",
                "edfi_api_manager.set_other_metadata()\r\n",
                "\r\n",
                "dependenciesUrl = edfi_api_manager.dependencies_url\r\n",
                "openApiMetadataUrl = edfi_api_manager.openapi_metadata_url\r\n",
                "dataManagementUrl = edfi_api_manager.data_management_url\r\n",
                "authUrl = edfi_api_manager.auth_url\r\n",
                "\r\n",
                "changeQueriesUrl = edfi_api_manager.get_referenced_url('Change-Queries')\r\n",
                "changeQueriesUrl = changeQueriesUrl[:-13].replace('/metadata/', '/')\r\n",
                "swagger_url = swaggerUrl = edfi_api_manager.get_referenced_url('Resources')\r\n",
                "\r\n",
                "apiVersion = edfi_api_manager.api_version\r\n",
                "apiVersion = apiVersion[1:] if apiVersion.startswith('v') else apiVersion"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### OEA Initializations"
            ],
            "outputs": []
        },
        {
            "execution_count": 67,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "%run OEA/modules/Ed-Fi/v0.8/src/utilities/edfi_v0_8_edfi_py"
            ],
            "outputs": []
        },
        {
            "execution_count": 68,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from datetime import datetime\r\n",
                "oea = EdFiOEAChild()   \r\n",
                "oea.set_workspace(workspace)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Error Logging Initializations"
            ],
            "outputs": []
        },
        {
            "execution_count": 69,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "error_logger = ErrorLogging(spark = spark, \r\n",
                "                            oea = oea, \r\n",
                "                            logger = logger)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Other Initializations"
            ],
            "outputs": []
        },
        {
            "execution_count": 70,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# NOTE: 2024-03-27 Feature update to automate prepareEdFiMetadata\r\n",
                "from py4j.protocol import Py4JJavaError\r\n",
                "if not(prepareEdFiMetadata):\r\n",
                "    try:\r\n",
                "        metadata_root_path = f'stage1/Transactional/Ed-Fi/{apiVersion}/DistrictId={districtId}/SchoolYear={schoolYear}/metadata-assets'\r\n",
                "        metadata_root_url = oea.to_url(metadata_root_path) \r\n",
                "        files = mssparkutils.fs.ls(metadata_root_url)\r\n",
                "        metadata_exists = False\r\n",
                "        freq_etl_exists = False\r\n",
                "        for file in files:\r\n",
                "            if file.name.lower() == 'metadata.csv':\r\n",
                "                metadata_exists = True\r\n",
                "            elif file.name.lower() == 'frequency_etl.csv':\r\n",
                "                freq_etl_exists = True\r\n",
                "        prepareEdFiMetaData = prepareEdFiMetadata = metadata_exists and freq_etl_exists\r\n",
                "    except Py4JJavaError as e:\r\n",
                "        logger.info(\"Ed-Fi metadata files not detected - attempting to create them first\")\r\n",
                "        prepareEdFiMetaData = prepareEdFiMetadata = True"
            ],
            "outputs": []
        },
        {
            "execution_count": 71,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "import random\r\n",
                "import string\r\n",
                "\r\n",
                "characters = string.ascii_letters + string.digits\r\n",
                "ten_digit_alphanumeric = ''.join(random.choice(characters) for _ in range(10))\r\n",
                "\r\n",
                "# TODO: 2024-03-26 Under Review\r\n",
                "if 'pipelineExecutionId' in globals():\r\n",
                "    pipelineExecutionId = globals()['pipelineExecutionId']\r\n",
                "    if pipelineExecutionId.lower().startswith('edgraph_dw_edfi_'):\r\n",
                "        pipelineExecutionId = pipelineExecutionId[len('edgraph_dw_edfi_'):]\r\n",
                "    executionId = pipelineExecutionId\r\n",
                "    print(\"pipelineExecutionId exists in global variables and its value is:\", pipelineExecutionId)\r\n",
                "else:\r\n",
                "    print(\"pipelineExecutionId does not exist in global variables\")\r\n",
                "    pipelineExecutionId = executionId = f'TEST_{ten_digit_alphanumeric}'"
            ],
            "outputs": []
        },
        {
            "execution_count": 72,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# FIXME: Do not pass uncessary params\r\n",
                "input_params = {\r\n",
                "        \"kvSecret_devMode\": kvSecret_devMode,\r\n",
                "        \"kvName\": kvName,\r\n",
                "        \"kVName\": kvName,\r\n",
                "        \"workspace\": workspace,\r\n",
                "        \"apiUrl\": apiUrl,\r\n",
                "        \"ApiUrl\": apiUrl,\r\n",
                "        \"instanceId\": instanceId,\r\n",
                "        \"InstanceId\": instanceId,\r\n",
                "        \"moduleName\": moduleName,\r\n",
                "        \"apiLimit\": apiLimit,\r\n",
                "        \"minChangeVer\": minChangeVer,\r\n",
                "        \"maxChangeVer\": maxChangeVer,\r\n",
                "        \"schoolYear\": schoolYear,\r\n",
                "        \"SchoolYear\": schoolYear,\r\n",
                "        \"districtId\": districtId,\r\n",
                "        \"districtID\": districtId,\r\n",
                "        \"DistrictId\": districtId,\r\n",
                "        \"districtPath\": districtId,\r\n",
                "        \"edfi_url\": apiUrl,\r\n",
                "        \"pipelineExecutionId\" : pipelineExecutionId,\r\n",
                "        \"batchLimit\": apiLimit,\r\n",
                "        \"metadataUrl\": metadataUrl,\r\n",
                "        \"client_id\": kvSecret_clientId,\r\n",
                "        \"client_secret_id\": kvSecret_clientSecret,\r\n",
                "        \"kvSecret_clientId\":kvSecret_clientId,\r\n",
                "        \"kvSecret_clientSecret\":kvSecret_clientSecret,\r\n",
                "        \"prepareEdFiMetadata\": prepareEdFiMetadata,\r\n",
                "        \"parameterized\": parameterized,\r\n",
                "        \"highFrequentDelta\" : highFrequentDelta,\r\n",
                "        \"moderateFrequentDelta\" : moderateFrequentDelta,  \r\n",
                "        \"lowFrequentDelta\" : lowFrequentDelta,  \r\n",
                "        \"descriptorsDelta\" : descriptorsDelta,\r\n",
                "        \"etlProcessing\": etlProcessing,\r\n",
                "        \"fetchHistory\": fetchHistory,\r\n",
                "        \"ingestionHistoryMode\": ingestionHistoryMode,\r\n",
                "        \"landingDateTimeFormat\": landingDateTimeFormat,\r\n",
                "        \"landData\": landData,\r\n",
                "        \"ingestData\": ingestData,\r\n",
                "        \"refineData\": refineData\r\n",
                "        }"
            ],
            "outputs": []
        },
        {
            "execution_count": 73,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "def execute_etl_step(condition,\r\n",
                "                     nb_path,\r\n",
                "                     nb_timeout,\r\n",
                "                     nb_params,\r\n",
                "                     nb_error,\r\n",
                "                     etl_stage):\r\n",
                "    global etl_status\r\n",
                "    # FIXME: Parameterize etl_status \r\n",
                "    stage_start_time = datetime.now()\r\n",
                "    if nb_error is None:\r\n",
                "        etl_status = f\"{etl_stage} - RUNNING\"\r\n",
                "        try:\r\n",
                "            if condition:\r\n",
                "                mssparkutils.notebook.run(path = nb_path,\r\n",
                "                                          timeout_seconds = nb_timeout,\r\n",
                "                                          arguments = nb_params)\r\n",
                "                etl_status = f\"{etl_stage} - SUCCEEDED\"\r\n",
                "            else:\r\n",
                "                etl_status = f\"{etl_stage} - SKIPPED\"\r\n",
                "        except Exception as error_msg:\r\n",
                "            etl_status = f\"{etl_stage} - FAILED\"\r\n",
                "            logger.info(f\"{error_msg}\")\r\n",
                "            nb_error = error_msg\r\n",
                "    else:\r\n",
                "        pass\r\n",
                "    \r\n",
                "    stage_end_time = datetime.now()\r\n",
                "    log_data = pipeline_error_logger.create_log_dict(uniqueId = pipeline_error_logger.generate_random_alphanumeric(10), # Generate a random 10-character alphanumeric value\r\n",
                "                                            pipelineExecutionId = pipelineExecutionId,#'TEST_1234',#executionId,\r\n",
                "                                            sparkSessionId = spark.sparkContext.applicationId,\r\n",
                "                                            stageName = etl_stage,\r\n",
                "                                            start_time = stage_start_time,\r\n",
                "                                            end_time = stage_end_time,\r\n",
                "                                            run_status = 'SUCCEEDED' if nb_error is None else 'FAILED',\r\n",
                "                                            etl_status = etl_status,\r\n",
                "                                            error_description = str(nb_error))\r\n",
                "    pipeline_error_logger.consolidate_logs(log_data,'stage')\r\n",
                "    if ' - SUCCEEDED' in etl_status or ' - SKIPPED' in etl_status:\r\n",
                "        return etl_status, None\r\n",
                "    else:\r\n",
                "        return etl_status, nb_error\r\n",
                "\r\n",
                "def get_entity_logs_status(df, executionId, stageName):\r\n",
                "    df = df.filter((F.col('pipelineExecutionId') == executionId) & (F.col('stageName') == stageName))\r\n",
                "    df = df.withColumn('numRecordsFailed', F.col('totalNumOutputRows') - F.col('numInputRows'))\r\n",
                "\r\n",
                "    if df.filter(F.col(\"numRecordsFailed\") == 0).count() == df.count():\r\n",
                "        return 'success'\r\n",
                "    \r\n",
                "    if df.filter(F.col(\"numRecordsFailed\") != 0).count() == df.count():\r\n",
                "        return 'failure'\r\n",
                "    \r\n",
                "    if df.agg(F.sum(\"numRecordsFailed\")).collect()[0][0] != 0:\r\n",
                "        return 'partial-success'\r\n",
                "    \r\n",
                "    return 'unknown'"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Main Code"
            ],
            "outputs": []
        },
        {
            "execution_count": 78,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "nb_root = \"OEA/modules/Ed-Fi/v0.8/src/main\"\r\n",
                "nb_version = \"edfi_v0_8\"\r\n",
                "nb_error = None\r\n",
                "\r\n",
                "run_status = \"INITIATED\"\r\n",
                "etl_status = \"\""
            ],
            "outputs": []
        },
        {
            "execution_count": 79,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "pipeline_error_logger = ErrorLogging(spark = spark, \r\n",
                "                                     oea = oea, \r\n",
                "                                     logger = logger)\r\n",
                "pipeline_nb_error = None\r\n",
                "try:\r\n",
                "    run_status = \"RUNNING\"\r\n",
                "    # NOTE: Prepare Meta Data \r\n",
                "    etl_status, nb_error = execute_etl_step(condition = prepareEdFiMetadata,\r\n",
                "                                            nb_path = f\"{nb_root}/{nb_version}_prepare_metadata\",\r\n",
                "                                            nb_timeout = 300,\r\n",
                "                                            nb_params = input_params,\r\n",
                "                                            nb_error = nb_error,\r\n",
                "                                            etl_stage = \"ed-fi: Metadata Preparation\")\r\n",
                "    # NOTE: Landing \r\n",
                "    etl_status, nb_error = execute_etl_step(condition = landData,\r\n",
                "                                            nb_path = f\"{nb_root}/{nb_version}_land\",\r\n",
                "                                            nb_timeout = 3600,\r\n",
                "                                            nb_params = input_params,\r\n",
                "                                            nb_error = nb_error,\r\n",
                "                                            etl_stage = \"ed-fi: Landing\")\r\n",
                "    # NOTE: Ingestion\r\n",
                "    etl_status, nb_error = execute_etl_step(condition = ingestData,\r\n",
                "                                            nb_path = f\"{nb_root}/{nb_version}_ingest\",\r\n",
                "                                            nb_timeout = 7200,\r\n",
                "                                            nb_params = input_params,\r\n",
                "                                            nb_error = nb_error,\r\n",
                "                                            etl_stage = \"ed-fi: Ingestion\")\r\n",
                "     # NOTE: Refinement\r\n",
                "    etl_status, nb_error = execute_etl_step(condition = refineData,\r\n",
                "                                            nb_path = f\"{nb_root}/{nb_version}_refine\",\r\n",
                "                                            nb_timeout = 14400,\r\n",
                "                                            nb_params = input_params,\r\n",
                "                                            nb_error = nb_error,\r\n",
                "                                            etl_stage = \"ed-fi: Refinement\")\r\n",
                "    if nb_error is not None:\r\n",
                "        raise Exception(f\"{nb_error}\")\r\n",
                "    else:\r\n",
                "        run_status = \"SUCCEEDED\"\r\n",
                "    # TODO: 2024-03-26 Under Review\r\n",
                "    param_destination_path = f'etl-logs/log_type=run_params/ed-fi/{apiVersion}/pipelineExecutionId={pipelineExecutionId}/run_params.json'\r\n",
                "    param_destination_url = error_logger.to_logs_url(param_destination_path)\r\n",
                "    input_params_str = json.dumps(input_params)\r\n",
                "    mssparkutils.fs.put(param_destination_url, input_params_str, True)  # Overwrite if the file existed\r\n",
                "except Exception as pipeline_nb_error:\r\n",
                "    run_status = \"FAILED\"\r\n",
                "    print(f'Pipeline Executed with Error {pipeline_nb_error}')"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "destination_path = f'stage1/Transactional/Ed-Fi/{apiVersion}/DistrictId={districtId}/frequency-etl/frequency_etl.csv'\r\n",
                "processor = EntityFrequencyProcessor(oea = oea, \r\n",
                "                                     filepath = destination_path, \r\n",
                "                                     highFrequentDelta = highFrequentDelta,#0.005, \r\n",
                "                                     moderateFrequentDelta = moderateFrequentDelta, #5, \r\n",
                "                                     lowFrequentDelta = lowFrequentDelta, #10, \r\n",
                "                                     descriptorsDelta = descriptorsDelta) #360)\r\n",
                "\r\n",
                "processor.load_lookup_df(runStatusTracked = False)\r\n",
                "entities_to_etl = processor.return_entities_to_etl()\r\n",
                "processor.update_lookup_df()\r\n",
                "processor.write_lookup_df(destination_path)\r\n",
                ""
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Pipeline Level Logs"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "log_type = 'pipeline'\r\n",
                "\r\n",
                "end_time = datetime.now()\r\n",
                "log_data = pipeline_error_logger.create_log_dict(uniqueId = pipeline_error_logger.generate_random_alphanumeric(10), # Generate a random 10-character alphanumeric value\r\n",
                "                                                pipelineExecutionId = pipelineExecutionId,#'TEST_1234',#executionId,\r\n",
                "                                                sparkSessionId = spark.sparkContext.applicationId,\r\n",
                "                                                start_time = start_time,\r\n",
                "                                                end_time = end_time,\r\n",
                "                                                run_status = run_status,\r\n",
                "                                                etl_status = str(etl_status))\r\n",
                "pipeline_error_logger.consolidate_logs(log_data,'pipeline')\r\n",
                "df = pipeline_error_logger.create_spark_df('pipeline')\r\n",
                "\r\n",
                "\r\n",
                "pipeline_error_logger.write_logs_to_delta_lake(df = df, \r\n",
                "                             log_type = 'pipeline',\r\n",
                "                             destination_url = pipeline_error_logger.to_logs_url('etl-logs/log_type=pipeline'))\r\n",
                "pipeline_error_logger.add_etl_logs_to_lake_db(db_name = f'ldb_{workspace}_edfi_etl_logs',\r\n",
                "                                     logs_base_path = 'etl-logs',\r\n",
                "                                     log_type = 'pipeline',\r\n",
                "                                     overwrite = False)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Stage Level Logs"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# FIXME: WIP\r\n",
                "df = pipeline_error_logger.create_spark_df('stage')\r\n",
                "df = df.withColumn('stage_status', F.lit('Unknown'))\r\n",
                "\r\n",
                "log_type = 'stage'\r\n",
                "destination_url = pipeline_error_logger.to_logs_url('etl-logs/log_type=stage')\r\n",
                "\r\n",
                "pipeline_error_logger.write_logs_to_delta_lake(df = df, \r\n",
                "                                               log_type = log_type,\r\n",
                "                                               destination_url = destination_url)\r\n",
                "pipeline_error_logger.add_etl_logs_to_lake_db(db_name = f'ldb_{workspace}_edfi_etl_logs',\r\n",
                "                                     logs_base_path = 'etl-logs',\r\n",
                "                                     log_type = 'stage',\r\n",
                "                                     overwrite = False)"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "mssparkutils.session.stop()"
            ],
            "outputs": []
        }
    ]
}