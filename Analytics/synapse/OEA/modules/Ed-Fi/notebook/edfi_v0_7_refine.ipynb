{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "import copy\n",
                "import pyspark.sql.functions as f\n",
                "from concurrent.futures import ThreadPoolExecutor\n",
                "from datetime import datetime\n",
                "from requests.adapters import HTTPAdapter\n",
                "from requests.packages.urllib3.util.retry import Retry"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "from notebookutils import mssparkutils\n",
                "import configparser\n",
                "\n",
                "config_path = \"/tmp/conf.ini\"\n",
                "def copy_config_to_temp():\n",
                "    file_path = \"abfss://oea@yourstorageaccount.dfs.core.windows.net/sandboxes/configs/edfi-configs-2024-02-01.ini\" # oea.to_url(\"stage1/Transactional/SAP/metadata-assets/edfi-configs.ini\")\n",
                "    mssparkutils.fs.cp(file_path,\"file:/tmp/conf.ini\")\n",
                "\n",
                "def read_edfi_credentials(config_path):\n",
                "    config = configparser.ConfigParser()\n",
                "    config.read(config_path)\n",
                "\n",
                "    edfi_credentials = {}\n",
                "\n",
                "    if 'EdFi' in config:\n",
                "        edfi_credentials['client_id'] = config['EdFi'].get('client_id', '')\n",
                "        edfi_credentials['client_secret'] = config['EdFi'].get('client_secret', '')\n",
                "        edfi_credentials['instance_id'] = config['EdFi'].get('instance_id', '')\n",
                "\n",
                "    return edfi_credentials\n",
                "\n",
                "try:\n",
                "    copy_config_to_temp()\n",
                "    edfi_credentials = read_edfi_credentials(config_path)\n",
                "    client_id = edfi_credentials['client_id']\n",
                "    client_secret_id = edfi_credentials['client_secret']\n",
                "    instanceId = edfi_credentials['instance_id']\n",
                "except Exception as error:\n",
                "    print(f'Error Message - {error}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "instance = InstanceId = instanceId\n",
                "ApiUrl = apiUrl\n",
                "SchoolYear = schoolYear\n",
                "DistrictId = DistrictID = districtID = districtId\n",
                "apiLimit = batchLimit\n",
                "\n",
                "prepareEdFiMetaData = prepareEdFiMetadata"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### URL Initializations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "%run OEA/modules/Ed-Fi/v0.7/src/utilities/edfi_v0_7_fetch_urls"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "instance_id = instanceId\n",
                "school_year = schoolYear\n",
                "api_url = apiUrl\n",
                "\n",
                "edfi_api_manager = EdFiApiManager(api_url, instance_id, school_year)\n",
                "edfi_api_manager.update_urls()\n",
                "edfi_api_manager.set_other_metadata()\n",
                "\n",
                "dependenciesUrl = edfi_api_manager.dependencies_url\n",
                "openApiMetadataUrl = edfi_api_manager.openapi_metadata_url\n",
                "dataManagementUrl = edfi_api_manager.data_management_url\n",
                "authUrl = edfi_api_manager.auth_url\n",
                "\n",
                "changeQueriesUrl = edfi_api_manager.get_referenced_url('Change-Queries')\n",
                "changeQueriesUrl = changeQueriesUrl[:-13].replace('/metadata/', '/')\n",
                "swagger_url = swaggerUrl = edfi_api_manager.get_referenced_url('Resources')\n",
                "\n",
                "apiVersion = edfi_api_manager.api_version\n",
                "apiVersion = apiVersion[1:] if apiVersion.startswith('v') else apiVersion"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### OEA Initializations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "%run OEA/modules/Ed-Fi/v0.7/src/utilities/edfi_v0_7_edfi_py"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "# FIXME: 2024-02-12: ingestionHistoryMode Under Review\n",
                "oea = EdFiOEAChild()   \n",
                "oea.set_workspace(workspace)\n",
                "oea.ingestionHistoryMode = ingestionHistoryMode"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "# swagger_url = swaggerUrl = edfi_api_manager.get_referenced_url('Descriptors')\n",
                "oea_utils = schema_gen = OpenAPIUtil(swagger_url)\n",
                "oea_utils.create_definitions()\n",
                "schemas = schema_gen.create_spark_schemas()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Error Logging Initializations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "error_logger = ErrorLogging(spark = spark, \n",
                "                            oea = oea, \n",
                "                            logger = logger)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Threading Utilities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "def upsert_data(df_changes, \n",
                "                metadata,\n",
                "                schema_name, \n",
                "                transform_mode,\n",
                "                table_name,\n",
                "                primary_key,\n",
                "                ext_entity,\n",
                "                sink_general_path,\n",
                "                sink_sensitive_path):\n",
                "        df_pseudo, df_lookup = oea.pseudonymize(df_changes, \n",
                "                                                metadata,\n",
                "                                                transform_mode,\n",
                "                                                True)            \n",
                "        edfiRefineAgent.transform(df = df_pseudo, \n",
                "                schema_name = schema_name, \n",
                "                table_name = table_name, \n",
                "                primary_key = 'id_pseudonym', \n",
                "                ext_entity = ext_entity, \n",
                "                sink_general_path = sink_general_path,\n",
                "                districtId_col_name = 'DistrictId', \n",
                "                schoolYear_col_name = 'SchoolYear')\n",
                "        if '/emptySchemas/' not in sink_sensitive_path:            \n",
                "                oea.upsert(df = df_lookup, \n",
                "                        destination_path = sink_sensitive_path, \n",
                "                        primary_key = 'id',\n",
                "                        partitioning = True,\n",
                "                        partitioning_cols = ['DistrictId', 'SchoolYear'])    \n",
                "                oea.add_to_lake_db(source_entity_path = sink_sensitive_path,\n",
                "                                overwrite = True,\n",
                "                                extension = None)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "def threaded_task_empty_schema(input_tuple):\n",
                "    # FIXME: 2024-02-07: Threading Under Dev\n",
                "    item, schema_name, s2r_path, ext_entity, metadata, transform_mode, districtId, schoolYear = input_tuple\n",
                "    \n",
                "    table_name = item #sap_to_edfi_complex[item]\n",
                "    try:\n",
                "        logger.info('[REFINEMENT EMPTY SCHEMA DUMPING THREAD] Path does not exist - attempting to create empty data frame')                        \n",
                "        sink_general_path = f'{s2r_path}/general/{schema_name}/{item}'\n",
                "        sink_sensitive_path = f'{s2r_path}/sensitive/{schema_name}/{item}_lookup'\n",
                "                        \n",
                "        sink_general_path = edfiRefineAgent.sink_path_cleanup(sink_general_path)\n",
                "        sink_sensitive_path = edfiRefineAgent.sink_path_cleanup(sink_sensitive_path)\n",
                "        if not oea.path_exists(sink_general_path):  \n",
                "            # FIXME: 2024-02-07 TEMP FIX       \n",
                "            target_schema = copy.deepcopy(edfiRefineAgent.schemas[table_name])    \n",
                "            df_changes = spark.createDataFrame(data = [],\n",
                "                                                schema = target_schema)\n",
                "            df_changes = df_changes.withColumn('DistrictId', F.lit(districtId))\n",
                "            df_changes = df_changes.withColumn('SchoolYear', F.lit(schoolYear))\n",
                "            \n",
                "            current_timestamp = datetime.now()\n",
                "            df_changes = df_changes.withColumn('LastModifiedDate', F.lit(current_timestamp))\n",
                "            df_changes = df_changes.withColumn('rowIsActive', F.lit(True))\n",
                "            df_changes = df_changes.withColumn('rundate', F.lit(current_timestamp))                            \n",
                "            df_changes = df_changes.withColumn('stage1_source_url', F.lit('placeholder'))   \n",
                "            \n",
                "            if 'id' in df_changes.columns:\n",
                "                upsert_data(df_changes, \n",
                "                            metadata,\n",
                "                            schema_name, \n",
                "                            transform_mode,\n",
                "                            table_name,\n",
                "                            'id',\n",
                "                            ext_entity,\n",
                "                            sink_general_path,\n",
                "                            sink_sensitive_path)\n",
                "            else:\n",
                "                logger.info(f'[REFINEMENT EMPTY SCHEMA DUMPING THREAD] {item} does not have id as primary key - flagged for future')\n",
                "    except Exception as error:\n",
                "        logger.exception(f\"[REFINEMENT EMPTY SCHEMA DUMPING THREAD] {error}\")\n",
                "\n",
                "def dump_empty_schemas(schema_name, \n",
                "                       s2r_path,\n",
                "                       ext_entity,\n",
                "                       transform_mode, \n",
                "                       items = []):\n",
                "    global districtId,schoolYear, metadata\n",
                "    if schema_name is None:\n",
                "        schema_name = 'ed-fi'\n",
                "    \n",
                "    with ThreadPoolExecutor(max_workers=8) as tpe:\n",
                "        logger.info('[REFINEMENT EMPTY SCHEMA DUMPING] Entered Threadpool')\n",
                "        tpe.map(threaded_task_empty_schema,[(item,schema_name if not item.lower().endswith('exts') else 'tx',s2r_path,ext_entity,metadata,transform_mode,districtId,schoolYear) for item in items])\n",
                "    \n",
                "    \n",
                "def threaded_task(input_tuple):\n",
                "    item,schema_name,tables_source,ext_entity,metadata,transform_mode,test_mode = input_tuple\n",
                "    \n",
                "    # print('inside thread')\n",
                "    table_name = item #sap_to_edfi_complex[item]\n",
                "    table_path = f\"{tables_source}/{item}\"\n",
                "    logger.info(f\"[REFINEMENT ETL TABLE THREAD] Processing schema/table: {schema_name}/{table_name}\")\n",
                "    if item == 'metadata.csv':\n",
                "        logger.info('ignore metadata processing, since this is not a table to be ingested')\n",
                "    else: \n",
                "        try:\n",
                "            if not(oea.path_exists(f\"stage2/Ingested/{table_path}\")):\n",
                "                pass\n",
                "            else:\n",
                "                if not(transform_mode):\n",
                "                    df = oea.refine(table_path, \n",
                "                                    metadata = metadata[item], \n",
                "                                    primary_key = 'id')\n",
                "                if transform_mode:\n",
                "                    logger.info('[REFINEMENT ETL TABLE THREAD] Ed-Fi to Ed-Fi Relationship Model: ' + table_name)               \n",
                "                    source_path = f'stage2/Ingested/{table_path}'\n",
                "                    sink_general_path, sink_sensitive_path = oea.get_sink_general_sensitive_paths(source_path)\n",
                "                    \n",
                "                    sink_general_path = edfiRefineAgent.sink_path_cleanup(sink_general_path)\n",
                "                    sink_sensitive_path = edfiRefineAgent.sink_path_cleanup(sink_sensitive_path)\n",
                "                    df_changes = oea.get_latest_changes(source_path, sink_general_path, filtering_date = 'rundate')\n",
                "\n",
                "                    df_changes = df_changes.withColumn('DistrictId', F.lit(districtId))\n",
                "                    \n",
                "                    # FIXME TO BE REVISED\n",
                "                    if item != 'schoolYearTypes':\n",
                "                        df_changes = df_changes.withColumn('SchoolYear', F.lit(schoolYear))\n",
                "                    else:\n",
                "                        # df_changes = df_changes.withColumnRenamed(\"schoolYear\", \"SchoolYear\")\n",
                "                        pass\n",
                "                    \n",
                "                    current_timestamp = datetime.now()\n",
                "                    df_changes = df_changes.withColumn('LastModifiedDate', F.lit(current_timestamp))\n",
                "                    \n",
                "                    if df_changes.count() > 0:\n",
                "                        upsert_data(df_changes, \n",
                "                                    metadata,\n",
                "                                    schema_name, \n",
                "                                    transform_mode,\n",
                "                                    table_name,\n",
                "                                    'id_pseudonym',\n",
                "                                    ext_entity,\n",
                "                                    sink_general_path,\n",
                "                                    sink_sensitive_path)\n",
                "                    else:\n",
                "                        logger.info(f'[REFINEMENT ETL TABLE THREAD] No updated rows in {source_path} to process.')\n",
                "\n",
                "        except AnalysisException as e:\n",
                "            logger.info(F\"[REFINEMENT ETL TABLE THREAD] {e}\")\n",
                "        except Exception as e:\n",
                "            logger.info(F\"[REFINEMENT ETL TABLE THREAD] {e}\")\n",
                "\n",
                "def refine_and_explode_data(schema_name, \n",
                "                            tables_source,\n",
                "                            ext_entity,\n",
                "                            metadata, \n",
                "                            transform_mode, \n",
                "                            test_mode,\n",
                "                            items = []):\n",
                "    global districtId,schoolYear\n",
                "    if items == 'All':\n",
                "        items = oea.get_folders(f\"stage2/Ingested/{tables_source}\")\n",
                "        items.append('schoolYearTypes')\n",
                "    #items = ['accountCodes', 'accounts', 'grades', 'students', 'staffs']\n",
                "    with ThreadPoolExecutor(max_workers=8) as tpe:\n",
                "        logger.info('[REFINEMENT ETL TABLES] Entered Threadpool')\n",
                "        tpe.map(threaded_task,[(item,schema_name,tables_source,ext_entity,metadata,transform_mode,test_mode) for item in items])\n",
                "            \n",
                "\n",
                "def get_non_ext_entities(entities_meta_info):\n",
                "    non_ext_table_names = list()\n",
                "    for entity_meta_info in entities_meta_info:\n",
                "        non_ext_table_names.append(entity_meta_info['resource'].split('/')[-1])\n",
                "    return non_ext_table_names\n",
                "\n",
                "def add_all_empty_tables_to_lake_db(empty_tables_path, schema_name, emptyTables = None):\n",
                "    if emptyTables is None:\n",
                "        empty_tables_source = oea.to_url(empty_tables_path)\n",
                "        items = oea.get_folders(empty_tables_source)\n",
                "    else:\n",
                "        items = emptyTables\n",
                "    if schema_name == 'ed-fi':\n",
                "        extension = None\n",
                "    else:\n",
                "        extension = schema_name \n",
                "\n",
                "    with ThreadPoolExecutor(max_workers=8) as tpe:\n",
                "        logger.info('[REFINEMENT EMPTY SCHEMA ADD TO LAKE DB] Entered Threadpool')\n",
                "        for item in items:\n",
                "            source_entity_path = empty_tables_path + '/' + item \n",
                "            tpe.submit(add_empty_table_to_lake_db,source_entity_path,False,extension)\n",
                "     \n",
                "    # for item in items:\n",
                "    #     source_entity_path = empty_tables_path + '/' + item \n",
                "    #     add_empty_table_to_lake_db(source_entity_path, \n",
                "    #                               overwrite = False, \n",
                "    #                               extension = extension)\n",
                "\n",
                "def add_empty_table_to_lake_db(source_entity_path, overwrite = False, extension = None):\n",
                "        # FIXME: Temporary Fix for Empty Schemas\n",
                "        \"\"\" Adds the given entity as a table (if the table doesn't already exist) to the proper lake db based on the path.\n",
                "            This method will also create the lake db if it doesn't already exist.\n",
                "            eg: add_to_lake_db('stage2/Ingested/contoso_sis/v0.2/students')\n",
                "\n",
                "            Note that a spark db that points to source data in the delta format can't be queried via SQL serverless pool. More info here: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#delta-lake\n",
                "        \"\"\"\n",
                "        source_dict = oea.parse_path(source_entity_path)\n",
                "        if '/emptySchemas/' in source_entity_path:\n",
                "            try:\n",
                "                base_db_name = source_dict['ldb_name']\n",
                "                base_table_name = source_dict['entity']\n",
                "                for submission_type in ['']:     \n",
                "                    if extension is not None:\n",
                "                        if not(extension.startswith('_')):\n",
                "                            extension = '_' + extension\n",
                "                        source_dict['entity'] = base_table_name + str(extension)\n",
                "                    \n",
                "                    db_name = base_db_name + submission_type\n",
                "\n",
                "                    logger.info(f\"[REFINEMENT EMPTY SCHEMA ADD TO LAKE DB] Adding: Lake DB: {db_name}; Table: {source_dict['entity']}\")\n",
                "                    spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\n",
                "                    if overwrite:\n",
                "                        spark.sql(f\"drop table if exists {db_name}.{source_dict['entity']}\")\n",
                "\n",
                "                    spark.sql(f\"create table if not exists {db_name}.{source_dict['entity']} using DELTA location '{oea.to_url(source_dict['entity_path'])}'\")\n",
                "            except Exception as error:\n",
                "                logger.error(f'[REFINEMENT EMPTY SCHEMA ADD TO LAKE DB] {error}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Main Code"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "edfiRefineAgent = EdFiRefine(workspace = workspace, \n",
                "                             oea = oea, \n",
                "                             spark = spark,\n",
                "                             schema_gen = schema_gen,\n",
                "                             moduleName = moduleName, \n",
                "                             authUrl = authUrl,\n",
                "                             swaggerUrl = swaggerUrl, \n",
                "                             dataManagementUrl = dataManagementUrl, \n",
                "                             changeQueriesUrl = changeQueriesUrl, \n",
                "                             dependenciesUrl = dependenciesUrl, \n",
                "                             apiVersion = apiVersion, \n",
                "                             schoolYear = schoolYear, \n",
                "                             districtId = districtId,\n",
                "                             pipelineExecutionId = pipelineExecutionId,\n",
                "                             error_logger = error_logger,\n",
                "                             test_mode = False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "from datetime import datetime\n",
                "import math\n",
                "source_path = f'stage1/Transactional/Ed-Fi/{apiVersion}/DistrictId={districtId}/SchoolYear={schoolYear}/metadata-assets/frequency_etl.csv'  \n",
                "destination_path = source_path #f'stage1/Transactional/Ed-Fi/{apiVersion}/DistrictId={districtId}/SchoolYear={schoolYear}/metadata-assets/frequency_based_etl.csv'  \n",
                "logs_path = f\"stage1/Transactional/Ed-Fi/{apiVersion}/DistrictId={districtId}/SchoolYear={schoolYear}/metadata-assets/_frequency_etl_logs/run_logs_{datetime.today().strftime('%Y-%m-%d')}.csv\"\n",
                "\n",
                "processor = EntityFrequencyProcessor(oea = oea, \n",
                "                                     filepath = source_path, \n",
                "                                     highFrequentDelta = highFrequentDelta,#0.005, \n",
                "                                     moderateFrequentDelta = moderateFrequentDelta, #5, \n",
                "                                     lowFrequentDelta = lowFrequentDelta, #10, \n",
                "                                     descriptorsDelta = descriptorsDelta) #360)\n",
                "\n",
                "processor.load_lookup_df()\n",
                "_, entities_to_etl = processor.return_entities_to_etl()\n",
                "\n",
                "edfiEntities = \"All\" #['schoolYearTypes']\n",
                "tpdmEntities = 'All'\n",
                "\n",
                "edfiEntities = entities_to_etl.get('ed-fi', [])\n",
                "tpdmEntities = entities_to_etl.get('tpdm', [])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "from datetime import datetime\n",
                "schema_name = 'ed-fi'\n",
                "ext_entity = 'TPDM'\n",
                "test_mode = False\n",
                "transform_mode = True\n",
                "tables_source = f'{moduleName}/{apiVersion}/DistrictId={districtId}/SchoolYear={schoolYear}/{schema_name}'\n",
                "transform_items = edfiEntities #\"All#['staffs', 'students']#\" #non_ext_table_names#edfiEntities \n",
                "\n",
                "# Create or overwrite Metadata.csv\n",
                "metadataPath = f'stage1/Transactional/Ed-Fi/{apiVersion}/DistrictId={districtId}/SchoolYear={schoolYear}/metadata-assets'\n",
                "metadata = oea.get_metadata_from_path(metadataPath) # metadata = oea.get_metadata_from_url(metadataUrl)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "if etlProcessing:\n",
                "    df = refine_and_explode_data(schema_name, \n",
                "                            tables_source,\n",
                "                            ext_entity,\n",
                "                            metadata,\n",
                "                            transform_mode, \n",
                "                            test_mode,\n",
                "                            transform_items)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Empty Schemas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "from datetime import datetime\n",
                "transform_mode = True\n",
                "\n",
                "if prepareEdFiMetaData:\n",
                "    retry_strategy = Retry(total = 3,\n",
                "                       backoff_factor = 1,\n",
                "                       status_forcelist = [429, 500, 502, 503, 504],\n",
                "                       allowed_methods = [\"HEAD\", \"GET\", \"OPTIONS\", \"POST\", \"DELETE\"])\n",
                "\n",
                "    edfiAPIClient = EdFiClient(workspace = workspace, \n",
                "                                    kvName = kvName, #NOTE: Default to None \n",
                "                                    moduleName = moduleName, \n",
                "                                    authUrl = authUrl, \n",
                "                                    dataManagementUrl = dataManagementUrl, \n",
                "                                    changeQueriesUrl = changeQueriesUrl, \n",
                "                                    dependenciesUrl = dependenciesUrl, \n",
                "                                    apiVersion = apiVersion, \n",
                "                                    batchLimit = batchLimit, \n",
                "                                    minChangeVer = minChangeVer, \n",
                "                                    maxChangeVer = maxChangeVer,\n",
                "                                    schoolYear = schoolYear,\n",
                "                                    districtId = districtId,\n",
                "                                    kvSecret_clientId = client_id,\n",
                "                                    kvSecret_clientSecret = client_secret_id,\n",
                "                                    retry_strategy = retry_strategy,\n",
                "                                    threadMode = True,\n",
                "                                    devMode = True)\n",
                "\n",
                "    entities_meta_info = edfiAPIClient.getEntities()#[0]['resource']\n",
                "    non_ext_table_names = get_non_ext_entities(entities_meta_info) #TODO: To Be Reviewed\n",
                "    non_ext_table_names = ['schoolYearTypes'] + non_ext_table_names\n",
                "\n",
                "    for swagger_resource_type in ['Resources', 'Descriptors']:\n",
                "        swagger_url = swaggerUrl = edfi_api_manager.get_referenced_url(swagger_resource_type)\n",
                "        oea_utils = schema_gen = OpenAPIUtil(swagger_url)\n",
                "        oea_utils.create_definitions()\n",
                "        schemas = schema_gen.create_spark_schemas()\n",
                "\n",
                "        \n",
                "        edfiRefineAgent = EdFiRefine(workspace = workspace, \n",
                "                             oea = oea, \n",
                "                             spark = spark,\n",
                "                             schema_gen = schema_gen,\n",
                "                             moduleName = moduleName, \n",
                "                             authUrl = authUrl,\n",
                "                             swaggerUrl = swaggerUrl, \n",
                "                             dataManagementUrl = dataManagementUrl, \n",
                "                             changeQueriesUrl = changeQueriesUrl, \n",
                "                             dependenciesUrl = dependenciesUrl, \n",
                "                             apiVersion = apiVersion, \n",
                "                             schoolYear = schoolYear, \n",
                "                             districtId = districtId,\n",
                "                             pipelineExecutionId = pipelineExecutionId,\n",
                "                             error_logger = error_logger,\n",
                "                             test_mode = False)\n",
                "\n",
                "        # non_ext_table_names = sap_to_edfi_client.return_non_ext_tables()  \n",
                "        if swagger_resource_type == 'Resources':\n",
                "            transform_items = [item for item in non_ext_table_names if not(item.lower().endswith('descriptors'))]\n",
                "        elif swagger_resource_type == 'Descriptors':\n",
                "            transform_items = [item for item in non_ext_table_names if item.lower().endswith('descriptors')]\n",
                "      \n",
                "        s2r_path = f'stage2/Refined/Ed-Fi/{apiVersion}/emptySchemas'\n",
                "\n",
                "        dump_empty_schemas(schema_name = schema_name , \n",
                "                         s2r_path = s2r_path,\n",
                "                         ext_entity = ext_entity,\n",
                "                         transform_mode = transform_mode, \n",
                "                         items = transform_items)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "if prepareEdFiMetadata:\n",
                "    tables_source = f'Ed-Fi/{apiVersion}/ed-fi'\n",
                "    mainTables = [item for item in oea.get_folders(f\"stage2/Refined/{tables_source}/general\") if item != 'descriptorTables']\n",
                "\n",
                "    tables_source = f'Ed-Fi/{apiVersion}/{ext_entity.lower()}'\n",
                "    extTables = [item for item in oea.get_folders(f\"stage2/Refined/{tables_source}/general\") if item != 'descriptorTables']\n",
                "    if extTables != []:\n",
                "        mainTables = mainTables + extTables\n",
                "    edfi_emptyTables = oea.get_folders(f'stage2/Refined/Ed-Fi/{apiVersion}/emptySchemas/general/ed-fi')\n",
                "    edfi_emptyTables = edfiRefineAgent.non_empty_elements(edfi_emptyTables, \n",
                "                                                             mainTables)\n",
                "    ext_emptyTables = oea.get_folders(f'stage2/Refined/Ed-Fi/{apiVersion}/emptySchemas/general/{ext_entity.lower()}')\n",
                "    ext_emptyTables = edfiRefineAgent.non_empty_elements(ext_emptyTables, \n",
                "                                                             mainTables)\n",
                "\n",
                "    emptyTables_path = f'stage2/Refined/Ed-Fi/{apiVersion}/emptySchemas/general/ed-fi'\n",
                "    if edfi_emptyTables != list():\n",
                "        add_all_empty_tables_to_lake_db(emptyTables_path, 'ed-fi', edfi_emptyTables)\n",
                "\n",
                "    emptyTables_path = f'stage2/Refined/Ed-Fi/{apiVersion}/emptySchemas/general/{ext_entity.lower()}'\n",
                "    if ext_emptyTables != list():\n",
                "        add_all_empty_tables_to_lake_db(emptyTables_path, 'tpdm', ext_emptyTables)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 482,
            "metadata": {
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "if error_logger.entity_logs != []:\n",
                "    logger.info('[REFINEMENT ERROR LOGGING] Writing Entity Level Error Logs')\n",
                "    df = error_logger.create_spark_df('entity')\n",
                "    error_logger.write_logs_to_delta_lake(df = df, \n",
                "                                log_type = 'entity',\n",
                "                                destination_url = error_logger.to_logs_url('etl-logs/log_type=entity'))\n",
                "    error_logger.add_etl_logs_to_lake_db(db_name = f'ldb_{workspace}_edfi_etl_logs',\n",
                "                                        logs_base_path = 'etl-logs',\n",
                "                                        log_type = 'entity',\n",
                "                                        overwrite = True)"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        },
        "save_output": true
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
