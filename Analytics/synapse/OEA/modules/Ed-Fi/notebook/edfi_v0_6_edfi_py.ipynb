{
    "nbformat": 4,
    "nbformat_minor": 2,
    "metadata": {
        "save_output": true,
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "import requests\r\n",
                "import json\r\n",
                "import uuid\r\n",
                "from requests.auth import HTTPBasicAuth\r\n",
                "import logging\r\n",
                "import csv\r\n",
                "import pandas as pd\r\n",
                "from io import StringIO\r\n",
                "from pyspark.sql.window import Window as W\r\n",
                "\r\n",
                "from datetime import datetime, timedelta\r\n",
                "from notebookutils import mssparkutils\r\n",
                "\r\n",
                "logger = logging.getLogger('EdFiClient')"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "%run OEA/modules/Ed-Fi/v0.6/src/utilities/edfi_v0_6_oea_py"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### EdFi Extended Module of OEA"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "class EdFiOEAChild(OEA):\r\n",
                "    \"\"\" \r\n",
                "    NOTE: This class inherits features from the base class OEA and therefore,\r\n",
                "    should be created / executed after running the notebook OEA_py\r\n",
                "    \"\"\"\r\n",
                "    def __init__(self, workspace='dev', logging_level=logging.INFO, storage_account=None, keyvault=None, timezone=None):\r\n",
                "        # Call the base class constructor to initialize inherited attributes\r\n",
                "        super().__init__(workspace, logging_level, storage_account, keyvault, timezone)\r\n",
                "    \r\n",
                "    def get_latest_changes(self, source_path, sink_path, filtering_date = 'LastModifiedDate',debugMode = False):\r\n",
                "        \"\"\" Returns a dataframe representing the changes in the source data based on the max rundate in the sink data. \r\n",
                "            If the sink path is not found, all of the data from the source_path is returned (the assumption is that the sink delta table is being created for the first time).\r\n",
                "            eg, get_latest_changes('stage2/Ingested/contoso/v0.1/students', 'stage2/Refined/contoso/v0.1/students')\r\n",
                "        \"\"\"   \r\n",
                "        maxdatetime = None\r\n",
                "        try:\r\n",
                "            sink_df = self.query(sink_path, f'select max({filtering_date}) maxdatetime')\r\n",
                "            maxdatetime = sink_df.first()['maxdatetime']\r\n",
                "        except AnalysisException as e:\r\n",
                "            # This means that there is no delta table at the sink_path yet.\r\n",
                "            # We'll assume that the sink delta table is being created for the first time, meaning that all of the source data should be returned.\r\n",
                "            pass\r\n",
                "\r\n",
                "        changes_df = self.load(source_path)\r\n",
                "        if maxdatetime and not(debugMode):\r\n",
                "            # filter the source table for the latest changes (using the max rundate in the destination table as the watermark)\r\n",
                "            changes_df = changes_df.where(f\"{filtering_date} > '{maxdatetime}'\")        \r\n",
                "        return changes_df\r\n",
                "\r\n",
                "    \r\n",
                "    def process(self, source_path,foreach_batch_function, batch_type,options={}):\r\n",
                "        \"\"\" This simplifies the process of using structured streaming when processing transformations.\r\n",
                "            Provide a source_path and a function that receives a dataframe to work with (which will be a dataframe with data from the given source_path).\r\n",
                "            Use it like this...\r\n",
                "            def refine_contoso_dataset(df_source):\r\n",
                "                metadata = oea.get_metadata_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/gene/v0.7dev/modules/module_catalog/Student_and_School_Data_Systems/metadata.csv')\r\n",
                "                df_pseudo, df_lookup = oea.pseudonymize(df, metadata['studentattendance'])\r\n",
                "                oea.upsert(df_pseudo, 'stage2/Refined/contoso_sis/v0.1/studentattendance/general')\r\n",
                "                oea.upsert(df_lookup, 'stage2/Refined/contoso_sis/v0.1/studentattendance/sensitive')\r\n",
                "            oea.process('stage2/Ingested/contoso_sis/v0.1/studentattendance', refine_contoso_dataset)             \r\n",
                "        \"\"\"\r\n",
                "        if not self.path_exists(source_path):\r\n",
                "            raise ValueError(f'The given path does not exist: {source_path} (which resolves to: {self.to_url(source_path)})') \r\n",
                "\r\n",
                "        def wrapped_function(df, batch_id):\r\n",
                "            current_timestamp = datetime.now()\r\n",
                "            df = df.withColumn('LastModifiedDate', F.lit(current_timestamp))\r\n",
                "            if batch_type != 'delete':\r\n",
                "                df = df.withColumn(\"rowIsActive\", F.lit(True))\r\n",
                "            \r\n",
                "            df.persist() # cache the df so it doesn't get read in multiple times when we write to multiple destinations. See: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#foreachbatch\r\n",
                "            foreach_batch_function(df, batch_id)\r\n",
                "            df.unpersist()\r\n",
                "\r\n",
                "        spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
                "        #source_path = source_path.replace(':', '\\:')\r\n",
                "        print(f\"source_path is: {source_path}\")\r\n",
                "        streaming_df = spark.readStream.load(self.to_url(source_path), **options)\r\n",
                "        # for more info on append vs complete vs update modes for structured streaming: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#basic-concepts\r\n",
                "        query = streaming_df.writeStream.format('delta').outputMode('append').trigger(once=True).option('checkpointLocation', self.to_url(source_path) + '/_checkpoints').foreachBatch(wrapped_function).start()\r\n",
                "        query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
                "        number_of_new_inbound_rows = query.lastProgress[\"numInputRows\"]\r\n",
                "        logger.info(f'Number of new inbound rows processed: {number_of_new_inbound_rows}')\r\n",
                "        logger.debug(query.lastProgress)\r\n",
                "        return number_of_new_inbound_rows\r\n",
                "    \r\n",
                "    def return_pk_statement(self, pk_columns):\r\n",
                "        pk_statement = \"\"\r\n",
                "        for i, column in enumerate(pk_columns):\r\n",
                "            pk_statement += f\"sink.{column} = updates.{column}\"\r\n",
                "            if i < len(pk_columns) - 1:\r\n",
                "                pk_statement += \" AND \"\r\n",
                "        \r\n",
                "        return pk_statement\r\n",
                "\r\n",
                "    \r\n",
                "    def return_upsert_cols(self,\r\n",
                "                       columns, \r\n",
                "                       partitioning_cols, \r\n",
                "                       primary_key,\r\n",
                "                       upsert_type,\r\n",
                "                       skey = None):\r\n",
                "        if type(primary_key) == list:\r\n",
                "            pass\r\n",
                "        else:\r\n",
                "            primary_key = [primary_key]\r\n",
                "        \r\n",
                "        if upsert_type == 'update':\r\n",
                "            if skey is not None:\r\n",
                "                if type(skey) == list:\r\n",
                "                    pass\r\n",
                "                else:\r\n",
                "                    skey = [skey]\r\n",
                "                \r\n",
                "                iter_columns = list(set(columns) - set(partitioning_cols) - set(primary_key) - set(skey))\r\n",
                "            else:\r\n",
                "                iter_columns = list(set(columns) - set(partitioning_cols) - set(primary_key))\r\n",
                "            update_cols = dict()\r\n",
                "            for column in iter_columns:\r\n",
                "                update_cols[f\"sink.{column}\"] = f\"updates.{column}\"\r\n",
                "            return update_cols\r\n",
                "        \r\n",
                "        elif upsert_type == 'insert':\r\n",
                "            iter_columns = list(set(columns) - set(partitioning_cols))\r\n",
                "            insert_cols = dict()\r\n",
                "            for column in iter_columns:\r\n",
                "                insert_cols[f\"sink.{column}\"] = f\"updates.{column}\"\r\n",
                "            return insert_cols              \r\n",
                "\r\n",
                "    def upsert(self, df, destination_path, primary_key='id', partitioning=False, partitioning_cols = [], surrogate_key = False, overwrite = False):\r\n",
                "        # FIXME: Revert to original upsert functionality with DE-DUP + MERGE\r\n",
                "        # FIXME: Re-check Skey logic when maxSkey is None (not Int)\r\n",
                "        # FIXME: 2024-01-31: overwrite param introduced as a temp fix for stage 3 migration task\r\n",
                "        \"\"\" Upserts the data in the given dataframe into the specified destination using the given primary_key_column to identify the updates.\r\n",
                "            If there is no delta table found in the destination_path, one will be created.    \r\n",
                "        \"\"\"\r\n",
                "        skey = None\r\n",
                "        destination_url = self.to_url(destination_path)\r\n",
                "        df = self.fix_column_names(df)\r\n",
                "        if type(primary_key) == list:\r\n",
                "            pk_statement = self.return_pk_statement(primary_key)\r\n",
                "            skey = list()\r\n",
                "            for pk_component in primary_key:\r\n",
                "                sk_component = pk_component[:-4] + 'SKey'\r\n",
                "                skey.append(sk_component)\r\n",
                "        else:\r\n",
                "            skey = primary_key[:-4] + 'SKey'\r\n",
                "            if 'hkey' in primary_key.lower():\r\n",
                "                surrogate_key = True\r\n",
                "            pk_statement = self.return_pk_statement([primary_key])\r\n",
                "        \r\n",
                "        if surrogate_key:\r\n",
                "            if type(primary_key) == list:\r\n",
                "                for index, pk_component in enumerate(primary_key):\r\n",
                "                    # TODO: Revist logic and Optimize\r\n",
                "                    sk_component = skey[index]\r\n",
                "                    df = df.withColumn('row_id_label', (F.monotonically_increasing_id()))\r\n",
                "                    windowSpec = W.orderBy(\"row_id_label\")\r\n",
                "                    df = df.withColumn(\"row_id_label\", F.row_number().over(windowSpec))\r\n",
                "                    \r\n",
                "                    df = df.withColumn(sk_component, F.when((F.col(pk_component).isNull()) | (F.col(sk_component) == -1), -1).otherwise(F.col('row_id_label')))\r\n",
                "                    df = df.drop('row_id_label')\r\n",
                "            else:\r\n",
                "                # TODO: Revist logic and Optimize\r\n",
                "                df = df.withColumn('row_id_label', (F.monotonically_increasing_id()))\r\n",
                "                windowSpec = W.orderBy(\"row_id_label\")\r\n",
                "                df = df.withColumn(\"row_id_label\", F.row_number().over(windowSpec))\r\n",
                "                \r\n",
                "                df = df.withColumn(skey, F.when((F.col(primary_key).isNull()) | (F.col(skey) == -1), -1).otherwise(F.col('row_id_label')))\r\n",
                "                df = df.drop('row_id_label')\r\n",
                "            df_original = df\r\n",
                "        else:\r\n",
                "            # NOTE: Do not De-Duplicate when surrogate key is present\r\n",
                "            if '/dbo/Fact' in destination_path and 'stage3' in destination_path:\r\n",
                "                # FIXME: 2024-02-07 TEMP FIX to suspend dedup for fact tables\r\n",
                "                logger.info(\"De-Duplication suspended for fact tables in stage3\")\r\n",
                "            elif partitioning and type(primary_key) != list:\r\n",
                "                df = df.dropDuplicates([primary_key] + partitioning_cols)\r\n",
                "            elif partitioning and type(primary_key) == list:\r\n",
                "                df = df.dropDuplicates(primary_key + partitioning_cols)\r\n",
                "            elif not(partitioning) and type(primary_key) != list:\r\n",
                "                df = df.dropDuplicates([primary_key])\r\n",
                "            elif not(partitioning) and type(primary_key) == list:\r\n",
                "                df = df.dropDuplicates(primary_key)\r\n",
                "\r\n",
                "        if DeltaTable.isDeltaTable(spark, destination_url) and df.count() <= 0:\r\n",
                "            logger.info(\"No Ingress Records\")\r\n",
                "            return 0,0,0,0\r\n",
                "\r\n",
                "        if DeltaTable.isDeltaTable(spark, destination_url) and not overwrite:\r\n",
                "            delta_table_sink = DeltaTable.forPath(spark, destination_url)\r\n",
                "            if surrogate_key:\r\n",
                "                if type(primary_key) == list:\r\n",
                "                    for index, pk_component in enumerate(primary_key):\r\n",
                "                        sk_component = skey[index]\r\n",
                "                        sink_df = self.query(destination_path, f'select max({sk_component}) max_skey')\r\n",
                "                        max_skey = int(sink_df.first()['max_skey'])\r\n",
                "                        # df = df.withColumn(skey, F.col(skey) + max_skey)\r\n",
                "                        df = df.withColumn(sk_component, F.when(F.col(sk_component) == -1, F.col(sk_component)).otherwise(F.col(sk_component) + max_skey))\r\n",
                "                else:\r\n",
                "                    sink_df = self.query(destination_path, f'select max({skey}) max_skey')\r\n",
                "                    max_skey = int(sink_df.first()['max_skey'])\r\n",
                "                    # df = df.withColumn(skey, F.col(skey) + max_skey)\r\n",
                "                    df = df.withColumn(skey, F.when(F.col(skey) == -1, F.col(skey)).otherwise(F.col(skey) + max_skey))\r\n",
                "                    \r\n",
                "            if partitioning:\r\n",
                "                #TODO: Generalize for arbitrary partitioning columns\r\n",
                "                if (sorted(partitioning_cols) == ['DistrictId', 'SchoolYear']) or (len(partitioning_cols) == 0):\r\n",
                "                    # Assumption: Each DF should have constant DistrictId and SchoolYear per run\r\n",
                "                    partitioning_cols = ['DistrictId', 'SchoolYear']\r\n",
                "                    if (df.select('DistrictId').first() and df.select('SchoolYear').first()):\r\n",
                "                        DistrictId = df.select('DistrictId').first()[0]\r\n",
                "                        SchoolYear = df.select('SchoolYear').first()[0]\r\n",
                "                        destination_partition_url = self.to_url(f\"{destination_path}/DistrictId={DistrictId}/SchoolYear={SchoolYear}\")\r\n",
                "                        if DeltaTable.isDeltaTable(spark, destination_partition_url):\r\n",
                "                            logger.info('Upsert by Partitions + PK Cols')\r\n",
                "                            \r\n",
                "                            if surrogate_key:\r\n",
                "                                update_cols = self.return_upsert_cols(df.columns, partitioning_cols, primary_key, 'update', skey)\r\n",
                "                                insert_cols = self.return_upsert_cols(df.columns, partitioning_cols, primary_key, 'insert', None)\r\n",
                "\r\n",
                "                                delta_table_sink.alias('sink').merge(df.alias('updates'), pk_statement).whenMatchedUpdate(set = update_cols).whenNotMatchedInsert(values = insert_cols).execute()\r\n",
                "                            else:\r\n",
                "                                logger.info('TRUE UPSERT')\r\n",
                "                                delta_table_sink.alias('sink').merge(df.alias('updates'), pk_statement).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\r\n",
                "                    else:\r\n",
                "                        logger.info('Dynamically over-write the partition')\r\n",
                "                        spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\r\n",
                "                        df.write.format('delta').mode('overwrite').partitionBy(*partitioning_cols).save(destination_url)\r\n",
                "                else:\r\n",
                "                    logger.info('Dynamically over-write the partition')\r\n",
                "                    spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\r\n",
                "                    df.write.format('delta').mode('overwrite').partitionBy(*partitioning_cols).save(destination_url)\r\n",
                "        \r\n",
                "            else:\r\n",
                "                logger.info('Upsert by PK Cols')\r\n",
                "                if surrogate_key:\r\n",
                "                    update_cols = self.return_upsert_cols(df.columns, [], primary_key, 'update', skey)\r\n",
                "                    insert_cols = self.return_upsert_cols(df.columns, [], primary_key, 'insert', None)\r\n",
                "\r\n",
                "                    delta_table_sink.alias('sink').merge(df.alias('updates'), pk_statement).whenMatchedUpdate(set = update_cols).whenNotMatchedInsert(values = insert_cols).execute()\r\n",
                "                else:\r\n",
                "                    logger.info('TRUE UPSERT')\r\n",
                "                    delta_table_sink.alias('sink').merge(df.alias('updates'), pk_statement).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\r\n",
                "                    \r\n",
                "            first_row = delta_table_sink.history(1).select('operationMetrics.numOutputRows', \r\n",
                "                                                               'operationMetrics.numTargetRowsInserted', \r\n",
                "                                                               'operationMetrics.numTargetRowsUpdated').first()\r\n",
                "            numOutputRows = int(first_row[0]) if first_row[0] is not None else 0\r\n",
                "            numTargetRowsInserted = int(first_row[1]) if first_row[1] is not None else 0\r\n",
                "            numTargetRowsUpdated = int(first_row[2]) if first_row[2] is not None else 0\r\n",
                "\r\n",
                "            numInputRows = df.count() \r\n",
                "            return numInputRows, numOutputRows, numTargetRowsInserted, numTargetRowsUpdated\r\n",
                "\t\t\r\n",
                "        elif overwrite:\r\n",
                "            logger.info('Overwriting existing delta table found')\r\n",
                "            if not(partitioning):\r\n",
                "                logger.info('Writing unpartitioned delta lake')\r\n",
                "                df.write.format('delta').mode('overwrite').save(destination_url)\r\n",
                "            else:\r\n",
                "                if partitioning and len(partitioning_cols) == 0:\r\n",
                "                    logger.info('Partitioning columns absent - defaulting to DistrictId and SchoolYear as partitioning columns')\r\n",
                "                    df.write.format('delta').mode('overwrite').partitionBy('DistrictId', 'SchoolYear').save(destination_url)\r\n",
                "                else:\r\n",
                "                    partitioning_str = ', '.join(partitioning_cols)\r\n",
                "                    logger.info(f'Writing partitioned delta lake - partitioned by - {partitioning_str}')\r\n",
                "                    df.write.format('delta').mode('overwrite').partitionBy(*partitioning_cols).save(destination_url)\r\n",
                "\r\n",
                "            first_row = [None, None, None]\r\n",
                "            numOutputRows = int(first_row[0]) if first_row[0] is not None else df.count()\r\n",
                "            numTargetRowsInserted = int(first_row[1]) if first_row[1] is not None else df.count()\r\n",
                "            numTargetRowsUpdated = int(first_row[2]) if first_row[2] is not None else 0\r\n",
                "\r\n",
                "            numInputRows = df.count()\r\n",
                "            return numInputRows, numOutputRows, numTargetRowsInserted, numTargetRowsUpdated\r\n",
                "\r\n",
                "        else:\r\n",
                "            logger.debug('No existing delta table found. Creating delta table.')\r\n",
                "            if not(partitioning):\r\n",
                "                logger.info('Writing unpartitioned delta lake')\r\n",
                "                df.write.format('delta').save(destination_url)\r\n",
                "            else:\r\n",
                "                if partitioning and len(partitioning_cols) == 0:\r\n",
                "                    logger.info('Partitioning columns absent - defaulting to DistrictId and SchoolYear as partitioning columns')\r\n",
                "                    df.write.format('delta').partitionBy('DistrictId', 'SchoolYear').save(destination_url)\r\n",
                "                else:\r\n",
                "                    partitioning_str = ', '.join(partitioning_cols)\r\n",
                "                    logger.info(f'Writing partitioned delta lake - partitioned by - {partitioning_str}')\r\n",
                "                    df.write.format('delta').partitionBy(*partitioning_cols).save(destination_url)\r\n",
                "\r\n",
                "            first_row = [None, None, None]\r\n",
                "            numOutputRows = int(first_row[0]) if first_row[0] is not None else df.count()\r\n",
                "            numTargetRowsInserted = int(first_row[1]) if first_row[1] is not None else df.count()\r\n",
                "            numTargetRowsUpdated = int(first_row[2]) if first_row[2] is not None else 0\r\n",
                "\r\n",
                "            numInputRows = df.count()\r\n",
                "            return numInputRows, numOutputRows, numTargetRowsInserted, numTargetRowsUpdated\r\n",
                "\r\n",
                "\r\n",
                "    def delete_then_insert(self, df, destination_path, primary_key='id', partitioning=False, partitioning_cols = [], surrogate_key = False):\r\n",
                "        # TODO: Clean-up; S2R children and grand children tables to use this (maybe parent as well?)\r\n",
                "        # NOTE: This is a workaround to remediate the drawback of Delta MERGE that requires the merge key to be UNIQUE\r\n",
                "        \"\"\" Upserts the data in the given dataframe into the specified destination using the given primary_key_column to identify the updates.\r\n",
                "            If there is no delta table found in the destination_path, one will be created.    \r\n",
                "        \"\"\"\r\n",
                "        skey = None\r\n",
                "        destination_url = self.to_url(destination_path)\r\n",
                "        df = self.fix_column_names(df)\r\n",
                "        if type(primary_key) == list:\r\n",
                "            pk_statement = self.return_pk_statement(primary_key)\r\n",
                "        else:\r\n",
                "            skey = primary_key[:-4] + 'SKey'\r\n",
                "            if 'hkey' in primary_key.lower():\r\n",
                "                surrogate_key = True\r\n",
                "            pk_statement = self.return_pk_statement([primary_key])\r\n",
                "        \r\n",
                "        # print(pk_statement)\r\n",
                "        df_original = df\r\n",
                "        if surrogate_key:\r\n",
                "            if type(primary_key) == list:\r\n",
                "                pass\r\n",
                "            else:\r\n",
                "                # TODO: Revist logic and Optimize\r\n",
                "                df = df.withColumn('row_id_label', (F.monotonically_increasing_id()))\r\n",
                "                windowSpec = W.orderBy(\"row_id_label\")\r\n",
                "                df = df.withColumn(\"row_id_label\", F.row_number().over(windowSpec))\r\n",
                "                \r\n",
                "                df = df.withColumn(skey, F.when((F.col(primary_key).isNull()) | (F.col(skey) == -1), -1).otherwise(F.col('row_id_label')))\r\n",
                "                df = df.drop('row_id_label')\r\n",
                "            df_original = df\r\n",
                "        else:\r\n",
                "            pass\r\n",
                "            # NOTE: Do not De-Duplicate when surrogate key is present\r\n",
                "            # if partitioning and type(primary_key) != list:\r\n",
                "            #     df = df.dropDuplicates([primary_key] + partitioning_cols)\r\n",
                "            # elif partitioning and type(primary_key) == list:\r\n",
                "            #     df = df.dropDuplicates(primary_key + partitioning_cols)\r\n",
                "            # elif not(partitioning) and type(primary_key) != list:\r\n",
                "            #     df = df.dropDuplicates([primary_key])\r\n",
                "            # elif not(partitioning) and type(primary_key) == list:\r\n",
                "            #     df = df.dropDuplicates(primary_key)\r\n",
                "\r\n",
                "        if DeltaTable.isDeltaTable(spark, destination_url) and df.count() <= 0:\r\n",
                "            logger.info(\"No Ingress Records\")\r\n",
                "            return 0,0,0,0\r\n",
                "\r\n",
                "        if DeltaTable.isDeltaTable(spark, destination_url):\r\n",
                "            delta_table_sink = DeltaTable.forPath(spark, destination_url)\r\n",
                "            if surrogate_key:\r\n",
                "                if type(primary_key) == list:\r\n",
                "                    pass\r\n",
                "                else:\r\n",
                "                    sink_df = self.query(destination_path, f'select max({skey}) max_skey')\r\n",
                "                    max_skey = int(sink_df.first()['max_skey'])\r\n",
                "                    # df = df.withColumn(skey, F.col(skey) + max_skey)\r\n",
                "                    df = df.withColumn(skey, F.when(F.col(skey) == -1, F.col(skey)).otherwise(F.col(skey) + max_skey))\r\n",
                "                    \r\n",
                "            if partitioning:\r\n",
                "                #TODO: Generalize for arbitrary partitioning columns\r\n",
                "                if (sorted(partitioning_cols) == ['DistrictId', 'SchoolYear']) or (len(partitioning_cols) == 0):\r\n",
                "                    # Assumption: Each DF should have constant DistrictId and SchoolYear per run\r\n",
                "                    partitioning_cols = ['DistrictId', 'SchoolYear']\r\n",
                "                    if (df.select('DistrictId').first() and df.select('SchoolYear').first()):\r\n",
                "                        DistrictId = df.select('DistrictId').first()[0]\r\n",
                "                        SchoolYear = df.select('SchoolYear').first()[0]\r\n",
                "                        destination_partition_url = self.to_url(f\"{destination_path}/DistrictId={DistrictId}/SchoolYear={SchoolYear}\")\r\n",
                "                        if DeltaTable.isDeltaTable(spark, destination_partition_url):\r\n",
                "                            logger.info('Upsert by Partitions + PK Cols')\r\n",
                "                            \r\n",
                "                            if surrogate_key:\r\n",
                "                                update_cols = self.return_upsert_cols(df.columns, partitioning_cols, primary_key, 'update', skey)\r\n",
                "                                insert_cols = self.return_upsert_cols(df.columns, partitioning_cols, primary_key, 'insert', None)\r\n",
                "\r\n",
                "                                delta_table_sink.alias('sink').merge(df.alias('updates'), pk_statement).whenMatchedUpdate(set = update_cols).whenNotMatchedInsert(values = insert_cols).execute()\r\n",
                "                            else:\r\n",
                "                                # logger.info('Delta Merge')\r\n",
                "                                # delta_table_sink.alias('sink').merge(df.alias('updates'), f'sink.{primary_key} = updates.{primary_key}').whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\r\n",
                "                                logger.info('DELETE THEN INSERT')\r\n",
                "                                delta_table_sink.alias('sink').merge(df.alias('updates'), pk_statement).whenMatchedDelete().execute()#whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\r\n",
                "                                df_original.write.format('delta').mode('append').option(\"mergeSchema\", \"true\").partitionBy(*partitioning_cols).save(destination_url)\r\n",
                "                    else:\r\n",
                "                        # raise ValueError(\"Error\")\r\n",
                "                        #print(destination_url)\r\n",
                "                        logger.info('Dynamically over-write the partition')\r\n",
                "                        spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\r\n",
                "                        df.write.format('delta').mode('overwrite').partitionBy(*partitioning_cols).save(destination_url)\r\n",
                "                else:\r\n",
                "                    logger.info('Dynamically over-write the partition')\r\n",
                "                    spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\r\n",
                "                    df.write.format('delta').mode('overwrite').partitionBy(*partitioning_cols).save(destination_url)\r\n",
                "        \r\n",
                "            else:\r\n",
                "                logger.info('Upsert by PK Cols')\r\n",
                "                if surrogate_key:\r\n",
                "                    update_cols = self.return_upsert_cols(df.columns, [], primary_key, 'update', skey)\r\n",
                "                    insert_cols = self.return_upsert_cols(df.columns, [], primary_key, 'insert', None)\r\n",
                "\r\n",
                "                    # print(update_cols)\r\n",
                "                    # print(insert_cols)\r\n",
                "\r\n",
                "                    delta_table_sink.alias('sink').merge(df.alias('updates'), pk_statement).whenMatchedUpdate(set = update_cols).whenNotMatchedInsert(values = insert_cols).execute()\r\n",
                "                else:\r\n",
                "                    logger.info('DELETE THEN INSERT')\r\n",
                "                    delta_table_sink.alias('sink').merge(df.alias('updates'), pk_statement).whenMatchedDelete().execute()#whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\r\n",
                "                    df_original.write.format('delta').mode('append').option(\"mergeSchema\", \"true\").save(destination_url)\r\n",
                "            \r\n",
                "            first_row = delta_table_sink.history(1).select('operationMetrics.numOutputRows', \r\n",
                "                                                               'operationMetrics.numTargetRowsInserted', \r\n",
                "                                                               'operationMetrics.numTargetRowsUpdated').first()\r\n",
                "            numOutputRows = int(first_row[0]) if first_row[0] is not None else 0\r\n",
                "            numTargetRowsInserted = int(first_row[1]) if first_row[1] is not None else 0\r\n",
                "            numTargetRowsUpdated = int(first_row[2]) if first_row[2] is not None else 0\r\n",
                "\r\n",
                "            numInputRows = df.count() \r\n",
                "            return numInputRows, numOutputRows, numTargetRowsInserted, numTargetRowsUpdated\r\n",
                "\r\n",
                "        else:\r\n",
                "            logger.debug('No existing delta table found. Creating delta table.')\r\n",
                "            if not(partitioning):\r\n",
                "                logger.info('Writing unpartitioned delta lake')\r\n",
                "                df.write.format('delta').save(destination_url)\r\n",
                "            else:\r\n",
                "                if partitioning and len(partitioning_cols) == 0:\r\n",
                "                    logger.info('Partitioning columns absent - defaulting to DistrictId and SchoolYear as partitioning columns')\r\n",
                "                    df.write.format('delta').partitionBy('DistrictId', 'SchoolYear').save(destination_url)\r\n",
                "                else:\r\n",
                "                    partitioning_str = ', '.join(partitioning_cols)\r\n",
                "                    logger.info(f'Writing partitioned delta lake - partitioned by - {partitioning_str}')\r\n",
                "                    df.write.format('delta').partitionBy(*partitioning_cols).save(destination_url)\r\n",
                "\r\n",
                "            first_row = [None, None, None]\r\n",
                "            numOutputRows = int(first_row[0]) if first_row[0] is not None else df.count()\r\n",
                "            numTargetRowsInserted = int(first_row[1]) if first_row[1] is not None else df.count()\r\n",
                "            numTargetRowsUpdated = int(first_row[2]) if first_row[2] is not None else 0\r\n",
                "\r\n",
                "            numInputRows = df.count()\r\n",
                "\r\n",
                "            return numInputRows, numOutputRows, numTargetRowsInserted, numTargetRowsUpdated\r\n",
                "    \r\n",
                "    def overwrite(self, df, destination_path, primary_key='id', partitioning = False, partitioning_cols = []):\r\n",
                "        \"\"\" Overwrites the existing delta table with the given dataframe.\r\n",
                "            If there is no delta table found in the destination_path, one will be created.    \r\n",
                "        \"\"\"\r\n",
                "        destination_url = self.to_url(destination_path)\r\n",
                "        df = self.fix_column_names(df)\r\n",
                "        \r\n",
                "        # FIXME: 2024-02-13: de-duplication suspensed\r\n",
                "        # if partitioning: \r\n",
                "        #     df = df.dropDuplicates([primary_key] + partitioning_cols)\r\n",
                "        # else:\r\n",
                "        #     df = df.dropDuplicates([primary_key])\r\n",
                "        if not(partitioning):\r\n",
                "            logger.info('Writing unpartitioned delta lake')\r\n",
                "            df.write.format('delta').mode('overwrite').save(destination_url)\r\n",
                "        elif partitioning and len(partitioning_cols) == 0:\r\n",
                "            logger.info('Partitioning columns absent - defaulting to DistrictId and SchoolYear as partitioning columns')\r\n",
                "            df.write.format('delta').mode('overwrite').partitionBy('DistrictId', 'SchoolYear').save(destination_url)\r\n",
                "        else:\r\n",
                "            partitioning_str = ', '.join(partitioning_cols)\r\n",
                "            logger.info(f'Writing partitioned delta lake - partitioned by - {partitioning_str}')\r\n",
                "            df.write.format('delta').mode('overwrite').partitionBy(*partitioning_cols).save(destination_url)\r\n",
                "        \r\n",
                "        first_row = [None, None, None]\r\n",
                "        numOutputRows = int(first_row[0]) if first_row[0] is not None else df.count()#0\r\n",
                "        numTargetRowsInserted = int(first_row[1]) if first_row[1] is not None else df.count()#0\r\n",
                "        numTargetRowsUpdated = int(first_row[2]) if first_row[2] is not None else 0\r\n",
                "\r\n",
                "        numInputRows = df.count()\r\n",
                "\r\n",
                "        return numInputRows, numOutputRows, numTargetRowsInserted, numTargetRowsUpdated\r\n",
                "\r\n",
                "        \r\n",
                "    def append(self, df, destination_path, primary_key='id', partitioning = False, partitioning_cols = []):\r\n",
                "        \"\"\" Appends the given dataframe to the delta table in the specified destination.\r\n",
                "            If there is no delta table found in the destination_path, one will be created.    \r\n",
                "        \"\"\"\r\n",
                "        destination_url = self.to_url(destination_path)\r\n",
                "        df = self.fix_column_names(df)\r\n",
                "\r\n",
                "        if partitioning: \r\n",
                "            df = df.dropDuplicates([primary_key] + partitioning_cols)\r\n",
                "        else:\r\n",
                "            df = df.dropDuplicates([primary_key])\r\n",
                "\r\n",
                "        if DeltaTable.isDeltaTable(spark, destination_url):\r\n",
                "            df.write.format('delta').mode('append').save(destination_url)  # https://docs.delta.io/latest/delta-batch.html#append\r\n",
                "        else:\r\n",
                "            logger.debug('No existing delta table found. Creating delta table.')\r\n",
                "            if not(partitioning):\r\n",
                "                logger.info('Writing unpartitioned delta lake')\r\n",
                "                df.write.format('delta').save(destination_url)\r\n",
                "            elif partitioning and len(partitioning_cols) == 0:\r\n",
                "                logger.info('Partitioning columns absent - defaulting to DistrictId and SchoolYear as partitioning columns')\r\n",
                "                df.write.format('delta').partitionBy('DistrictId', 'SchoolYear').save(destination_url)\r\n",
                "            else:\r\n",
                "                partitioning_str = ', '.join(partitioning_cols)\r\n",
                "                logger.info(f'Writing partitioned delta lake - partitioned by - {partitioning_str}')\r\n",
                "                df.write.format('delta').partitionBy(*partitioning_cols).save(destination_url)\r\n",
                "    \r\n",
                "    def ingest(self, entity_path, primary_key='id', options={}):\r\n",
                "        \"\"\" Ingests the data for the entity in the given path.\r\n",
                "            CSV files are expected to have a header row by default, and JSON files are expected to have complete JSON docs on each row in the file.\r\n",
                "            To specify options that are different from these defaults, use the options param.\r\n",
                "            eg, ingest('contoso_sis/v0.1/students') # ingests all entities found in that path\r\n",
                "            eg, ingest('contoso_sis/v0.1/students', options={'header':False}) # for CSV files that don't have a header\r\n",
                "        \"\"\"\r\n",
                "        primary_key = self.fix_column_name(primary_key) # fix the column name, in case it has a space in it or some other invalid character\r\n",
                "        ingested_path = f'stage2/Ingested/{entity_path}'\r\n",
                "        raw_path = f'stage1/Transactional/{entity_path}'\r\n",
                "\r\n",
                "        if not self.path_exists(raw_path):\r\n",
                "            logger.error(f'Failed to ingest data because the given source data was not found where expected: {raw_path}')\r\n",
                "            return\r\n",
                "\r\n",
                "        batches = self.get_batch_info(raw_path)\r\n",
                "        number_of_inbound_changes = 0\r\n",
                "        for batch in batches:\r\n",
                "            batch_type = batch[0]\r\n",
                "            source_data_format = batch[1]\r\n",
                "            logger.info(f'Ingesting from: {raw_path}, batch type of: {batch_type}, source data format of: {source_data_format}')\r\n",
                "            source_url = self.to_url(f'{raw_path}/{batch_type}_batch_data')\r\n",
                "\r\n",
                "            if oea.get_folder_size(f'{source_url}/{self.get_latest_folder(source_url)}') > 0:\r\n",
                "                if batch_type == 'snapshot'or batch_type=='additive': source_url = f'{source_url}/{self.get_latest_folder(source_url)}' \r\n",
                "                    \r\n",
                "                logger.debug(f'Processing {batch_type} data from: {source_url} and writing out to: {ingested_path}')\r\n",
                "                if batch_type == 'snapshot':\r\n",
                "                    def batch_func(df, batch_id): self.overwrite(df, ingested_path, primary_key)\r\n",
                "                elif batch_type == 'additive':\r\n",
                "                    def batch_func(df, batch_id): self.append(df, ingested_path, primary_key)\r\n",
                "                elif batch_type == 'delta':\r\n",
                "                    def batch_func(df, batch_id): self.upsert(df, ingested_path, primary_key)\r\n",
                "                elif batch_type == \"delete\":\r\n",
                "                    def batch_func(df, batch_id): self.soft_delete_rows(df, ingested_path, primary_key, batch_id)\r\n",
                "                else:\r\n",
                "                    raise ValueError(\"No valid batch folder was found at that path (expected to find a single folder with one of the following names: snapshot_batch_data, additive_batch_data, or delta_batch_data). Are you sure you have the right path?\")                      \r\n",
                "\r\n",
                "                if options == None: options = {}\r\n",
                "                options['format'] = source_data_format # eg, 'csv', 'json'\r\n",
                "                if source_data_format == 'csv' and (not 'header' in options or options['header'] == None): options['header'] = True  # default to expecting a header in csv files\r\n",
                "                if source_data_format == 'json' and (not 'multiline' in options or options['multiline'] == None): options['multiline'] = True # default to expecting multiline formatted json data\r\n",
                "\r\n",
                "                number_of_new_inbound_rows = self.process(source_url, batch_func, batch_type,options)\r\n",
                "                if number_of_new_inbound_rows > 0:    \r\n",
                "                    self.add_to_lake_db(ingested_path, overwrite = True)\r\n",
                "                number_of_inbound_changes += number_of_new_inbound_rows\r\n",
                "        return number_of_inbound_changes\r\n",
                "    \r\n",
                "    def get_sink_general_sensitive_paths(self, source_path):\r\n",
                "        path_dict = self.parse_path(source_path)\r\n",
                "        \r\n",
                "        sink_general_path = path_dict['entity_parent_path'].replace('Ingested', 'Refined') + '/general/' + path_dict['entity']\r\n",
                "        sink_sensitive_path = path_dict['entity_parent_path'].replace('Ingested', 'Refined') + '/sensitive/' + path_dict['entity'] + '_lookup'\r\n",
                "\r\n",
                "        return sink_general_path, sink_sensitive_path\r\n",
                "\r\n",
                "    def refine(self, entity_path, metadata=None, primary_key='id'):\r\n",
                "        source_path = f'stage2/Ingested/{entity_path}'\r\n",
                "        primary_key = self.fix_column_name(primary_key) # fix the column name, in case it has a space in it or some other invalid character\r\n",
                "        sink_general_path, sink_sensitive_path = get_sink_general_sensitive_paths(source_path)\r\n",
                "\r\n",
                "        if not metadata:\r\n",
                "            all_metadata = self.get_metadata_from_path(path_dict['entity_parent_path'])\r\n",
                "            metadata = all_metadata[path_dict['entity']]\r\n",
                "        \r\n",
                "        df_changes = self.get_latest_changes(source_path, sink_general_path)\r\n",
                "        spark_schema = self.to_spark_schema(metadata)\r\n",
                "        df_changes = self.modify_schema(df_changes, spark_schema)        \r\n",
                "        if df_changes.count() > 0:\r\n",
                "            df_pseudo, df_lookup = self.pseudonymize(df_changes, metadata)\r\n",
                "            self.upsert(df_pseudo, sink_general_path, f'{primary_key}_pseudonym') # todo: remove this assumption that the primary key will always be hashed during pseduonymization\r\n",
                "            self.upsert(df_lookup, sink_sensitive_path, primary_key)    \r\n",
                "            self.add_to_lake_db(sink_general_path)\r\n",
                "            self.add_to_lake_db(sink_sensitive_path)\r\n",
                "            logger.info(f'Processed {df_changes.count()} updated rows from {source_path} into stage2/Refined')\r\n",
                "        else:\r\n",
                "            logger.info(f'No updated rows in {source_path} to process.')\r\n",
                "        \r\n",
                "        return df_changes.count()\r\n",
                "\r\n",
                "    def pseudonymize(self, df, metadata, transform_mode = False, debugging = True): #: list[list[str]]):\r\n",
                "        \"\"\" Performs pseudonymization of the given dataframe based on the provided metadata (in the OEA format).\r\n",
                "            For example, if the given df is for an entity called person, \r\n",
                "            2 dataframes will be returned, one called person that has hashed ids and masked fields, \r\n",
                "            and one called person_lookup that contains the original person_id, person_id_pseudo,\r\n",
                "            and the non-masked values for columns marked to be masked.           \r\n",
                "            The lookup table should be written to a \"sensitive\" folder in the data lake.\r\n",
                "            eg, df_pseudo, df_lookup = oea.pseudonymize(df, metadata)\r\n",
                "            [More info on this approach here: https://learn.microsoft.com/en-us/azure/databricks/security/privacy/gdpr-delta#pseudonymize-data]\r\n",
                "        \"\"\"\r\n",
                "        salt = self._get_salt()\r\n",
                "        df_pseudo = df\r\n",
                "        df_lookup = df\r\n",
                "        if transform_mode:\r\n",
                "            lookup_cols = ['DistrictId', 'SchoolYear']\r\n",
                "        else:\r\n",
                "            lookup_cols = []\r\n",
                "        if debugging:\r\n",
                "            col_name = 'id'\r\n",
                "            #df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\r\n",
                "            #df_lookup = df_lookup.withColumn(col_name + \"_pseudonym\", F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256))\r\n",
                "            \r\n",
                "            df_pseudo = df_pseudo.withColumnRenamed(col_name, col_name + \"_pseudonym\")\r\n",
                "            df_lookup = df_lookup.withColumn(col_name + \"_pseudonym\", F.col(col_name))\r\n",
                "            \r\n",
                "            lookup_cols.append(col_name)\r\n",
                "            lookup_cols.append(col_name + \"_pseudonym\")\r\n",
                "        else:\r\n",
                "            for row in metadata:\r\n",
                "                col_name = row[0]\r\n",
                "                dtype = row[1]\r\n",
                "                op = row[2]\r\n",
                "                if op == \"hash-no-lookup\" or op == \"hnl\":\r\n",
                "                    # This means that the lookup can be performed against a different table so no lookup is needed.\r\n",
                "                    df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\r\n",
                "                    df_lookup = df_lookup.drop(col_name)           \r\n",
                "                elif op == \"hash\" or op == 'h':\r\n",
                "                    df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\r\n",
                "                    df_lookup = df_lookup.withColumn(col_name + \"_pseudonym\", F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256))\r\n",
                "                    \r\n",
                "                    lookup_cols.append(col_name)\r\n",
                "                    lookup_cols.append(col_name + \"_pseudonym\")\r\n",
                "                \r\n",
                "                elif op == \"mask\" or op == 'm':\r\n",
                "                    df_pseudo = df_pseudo.withColumn(col_name, F.lit('*'))\r\n",
                "                elif op == \"partition-by\":\r\n",
                "                    pass # make no changes for this column so that it will be in both dataframes and can be used for partitioning\r\n",
                "                elif op == \"no-op\" or op == 'x':\r\n",
                "                    df_lookup = df_lookup.drop(col_name)\r\n",
                "\t\t\r\n",
                "        df_lookup = df_lookup.select(*lookup_cols)\r\n",
                "        return (df_pseudo, df_lookup)\r\n",
                "    \r\n",
                "    def add_to_lake_db(self, source_entity_path, overwrite = False, extension = None):\r\n",
                "        \"\"\" Adds the given entity as a table (if the table doesn't already exist) to the proper lake db based on the path.\r\n",
                "            This method will also create the lake db if it doesn't already exist.\r\n",
                "            eg: add_to_lake_db('stage2/Ingested/contoso_sis/v0.1/students')\r\n",
                "\r\n",
                "            Note that a spark db that points to source data in the delta format can't be queried via SQL serverless pool. More info here: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#delta-lake\r\n",
                "        \"\"\"\r\n",
                "        source_dict = self.parse_path(source_entity_path)\r\n",
                "        \r\n",
                "        db_name = source_dict['ldb_name']\r\n",
                "        if '/emptySchemas/' not in source_entity_path:\r\n",
                "            if extension is not None:\r\n",
                "                if not(extension.startswith('_')):\r\n",
                "                    extension = '_' + extension\r\n",
                "                source_dict['entity'] = source_dict['entity'] + str(extension)\r\n",
                "\r\n",
                "            spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\r\n",
                "            if overwrite:\r\n",
                "                spark.sql(f\"drop table if exists {db_name}.{source_dict['entity']}\")\r\n",
                "\r\n",
                "            spark.sql(f\"create table if not exists {db_name}.{source_dict['entity']} using DELTA location '{self.to_url(source_dict['entity_path'])}'\")\r\n",
                "        \r\n",
                "    def add_to_bucketed_lake_db(self, \r\n",
                "                                source_entity_path, \r\n",
                "                                df, \r\n",
                "                                primary_key,\r\n",
                "                                partitioning_cols,\r\n",
                "                                destination_url,\r\n",
                "                                num_buckets = 5,\r\n",
                "                                overwrite = False, \r\n",
                "                                extension = None):\r\n",
                "        \"\"\" Adds the given entity as a table (if the table doesn't already exist) to the proper lake db based on the path.\r\n",
                "            This method will also create the lake db if it doesn't already exist.\r\n",
                "            eg: add_to_lake_db('stage2/Ingested/contoso_sis/v0.1/students')\r\n",
                "\r\n",
                "            Note that a spark db that points to source data in the delta format can't be queried via SQL serverless pool. More info here: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#delta-lake\r\n",
                "        \"\"\"\r\n",
                "        source_dict = self.parse_path(source_entity_path)\r\n",
                "        \r\n",
                "        db_name = source_dict['ldb_name']\r\n",
                "        if extension is not None:\r\n",
                "            source_dict['entity'] = source_dict['entity'] + str(extension)\r\n",
                "\r\n",
                "        spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\r\n",
                "        if overwrite:\r\n",
                "            spark.sql(f\"drop table if exists {db_name}.{source_dict['entity']}\")\r\n",
                "        \r\n",
                "        df.write.format('delta').mode('append').option(\"mergeSchema\", \"true\").partitionBy(*partitioning_cols).bucketBy(num_buckets, primary_key).option(\"path\", destination_url).saveAsTable(f\"{db_name}.{source_dict['entity']}\")\r\n",
                "        #spark.sql(f\"create table if not exists {db_name}.{source_dict['entity']} using DELTA location '{self.to_url(source_dict['entity_path'])}'\")\r\n",
                "\r\n",
                "    def soft_delete_rows(self, df, destination_path, primary_key='id', batch_id=''):\r\n",
                "        \"\"\"Soft deletes the entities in the given dataframe in the specified destination Delta table\r\n",
                "            Marks the entities as deleted by setting a status column using MERGE INTO\r\n",
                "        \"\"\"\r\n",
                "        df = df.withColumn(\"rowIsActive\", F.lit(False))\r\n",
                "        ref = f'{destination_path.split(\"/\")[-1]}{batch_id}'\r\n",
                "        print(ref)\r\n",
                "        df.createOrReplaceGlobalTempView(ref)\r\n",
                "        source_dict = self.parse_path(destination_path)\r\n",
                "        db_name = source_dict['ldb_name']\r\n",
                "        entity_name = source_dict['entity']\r\n",
                "        \r\n",
                "        destination_url = self.to_url(destination_path)\r\n",
                "        rundate = datetime.now().replace(microsecond=0) # use UTC for the datetime because when parsing it out later, spark's to_timestamp() assumes the local machine's timezone, and the timezone for the spark cluster will be UTC\r\n",
                "\r\n",
                "        # Use MERGE INTO to update the Delta table based on the condition\r\n",
                "        if DeltaTable.isDeltaTable(spark, destination_url):\r\n",
                "            delta_table_sink = DeltaTable.forPath(spark, destination_url)\r\n",
                "            delta_table_sink.alias('sink').merge(df.alias('source'), f'sink.{primary_key} = source.{primary_key}') \\\r\n",
                "                .whenMatchedUpdate(set={\"rowIsActive\": \"false\", \"LastModifiedDate\": f\"'{rundate}'\"}) \\\r\n",
                "                .execute()\r\n",
                "\r\n",
                "        # Refresh the table to make the changes visible\r\n",
                "        spark.sql(f\"REFRESH TABLE {db_name}.{entity_name}\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Ed-Fi Client - API Client"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "class EdFiClient:\r\n",
                "    #The constructor\r\n",
                "    def __init__(self, workspace, kvName, moduleName, authUrl, dataManagementUrl, changeQueriesUrl, dependenciesUrl, apiVersion, batchLimit, minChangeVer=\"\", maxChangeVer=\"\", schoolYear=None, districtId=None, clientId = None, clientSecret = None):\r\n",
                "        self.workspace = workspace\r\n",
                "        self.keyvault_linked_service = 'LS_KeyVault'\r\n",
                "        if kvName is None:\r\n",
                "            kvName = oea.keyvault\r\n",
                "\r\n",
                "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\r\n",
                "        for handler in logging.getLogger().handlers:\r\n",
                "            handler.setFormatter(formatter)           \r\n",
                "        # Customize log level for all loggers\r\n",
                "        logging.getLogger().setLevel(logging.INFO)   \r\n",
                "        logger.info(f\"minChangeVersion={minChangeVer} and maxChangeVersion={maxChangeVer}\")\r\n",
                "\r\n",
                "        if not kvName and workspace == \"dev\":\r\n",
                "            logger.info(\"defaulting to test data\")\r\n",
                "            self.clientId = \"\"\r\n",
                "            self.clientSecret = \"\"\r\n",
                "        else:\r\n",
                "            try:\r\n",
                "                #try to get the credentials from keyvault\r\n",
                "                self.clientId = oea._get_secret(\"oea-edfi-api-client-id\") if clientId is None else clientId\r\n",
                "                self.clientSecret = oea._get_secret(\"oea-edfi-api-client-secret\") if clientSecret is None else clientSecret\r\n",
                "            except Exception as e:\r\n",
                "                #if there was an error getting the credentials\r\n",
                "                #if this is the dev instance proceed with test data, otherwise raise the Exception\r\n",
                "                logger.info(f\"failed to retrieve clientId and clientSecret from keyvault with exception: {str(e)}\")\r\n",
                "                if workspace == \"dev\":\r\n",
                "                    logger.info(\"defaulting to test data\")\r\n",
                "                    self.clientId = \"\"\r\n",
                "                    self.clientSecret = \"\"\r\n",
                "                else:\r\n",
                "                    raise\r\n",
                "        \r\n",
                "        self.authUrl = authUrl\r\n",
                "        self.dataManagementUrl = dataManagementUrl\r\n",
                "        self.changeQueriesUrl = changeQueriesUrl\r\n",
                "        self.dependenciesUrl = dependenciesUrl\r\n",
                "        from datetime import datetime\r\n",
                "        self.runDate = datetime.utcnow().strftime('%Y-%m-%d')\r\n",
                "        self.authTime = None\r\n",
                "        self.expiresIn = None\r\n",
                "        self.accessToken = None\r\n",
                "        districtPath = districtId if districtId != None else \"All\"\r\n",
                "        schoolYearPath = schoolYear if schoolYear != None else \"All\"\r\n",
                "        self.transactionalFolder = f\"Transactional/{moduleName}/{apiVersion}/DistrictId={districtPath}/SchoolYear={schoolYearPath}\"\r\n",
                "        self.batchLimit = batchLimit\r\n",
                "        self.minChangeVer = minChangeVer\r\n",
                "        self.maxChangeVer = maxChangeVer\r\n",
                "\r\n",
                "    #Method to get the access token for the test data set\r\n",
                "    def authenticateWithAuthorization(self):\r\n",
                "        #TODO: need to update this if we want it to work with other edfi provided test data set versions\r\n",
                "        result = requests.post(\"https://api.ed-fi.org/v5.2/api/oauth/token\",{\"grant_type\":\"client_credentials\"},headers={\"Authorization\":\"Basic UnZjb2hLejl6SEk0OkUxaUVGdXNhTmY4MXh6Q3h3SGZib2xrQw==\"})\r\n",
                "        return result\r\n",
                "\r\n",
                "    #Method to get the access token for a production system with basic auth\r\n",
                "    def authenticateWithBasic(self):\r\n",
                "        authHeader = HTTPBasicAuth(self.clientId, self.clientSecret)\r\n",
                "        result = requests.post(self.authUrl,{\"grant_type\":\"client_credentials\"},auth=authHeader)\r\n",
                "        return result\r\n",
                "\r\n",
                "    #This method orchestrates the authentication\r\n",
                "    def authenticate(self):\r\n",
                "        self.authTime = datetime.now()\r\n",
                "        if not self.clientId or not self.clientSecret: #self.workspace == \"dev\":\r\n",
                "            result = self.authenticateWithAuthorization().json()\r\n",
                "            logger.info(result)\r\n",
                "        else:\r\n",
                "            result = self.authenticateWithBasic().json()\r\n",
                "        self.expiresIn = result[\"expires_in\"]\r\n",
                "        self.accessToken = result[\"access_token\"]\r\n",
                "    \r\n",
                "    #This method manages the access token, refreshing it when required\r\n",
                "    def getAccessToken(self):\r\n",
                "        currentTime = datetime.now()\r\n",
                "        #Get a new access token if none exists, or if the expires time is within 5 minutes of expiry\r\n",
                "        if self.accessToken == None or (currentTime-self.authTime).total_seconds() > self.expiresIn - 300:\r\n",
                "            self.authenticate()\r\n",
                "            return self.accessToken\r\n",
                "        else:\r\n",
                "            return self.accessToken \r\n",
                "\r\n",
                "    def getChangeQueryVersion(self):\r\n",
                "        access_token = self.getAccessToken()\r\n",
                "        response = requests.get(changeQueriesUrl + \"/availableChangeVersions\", headers={\"Authorization\":\"Bearer \" + access_token})\r\n",
                "        return response.json(), response.status_code\r\n",
                "\r\n",
                "    def getEntities(self):\r\n",
                "        return requests.get(self.dependenciesUrl).json()\r\n",
                "\r\n",
                "    def getDeletes(self,resource, minChangeVersion, maxChangeVersion):\r\n",
                "        if minChangeVersion is None and maxChangeVersion is None:\r\n",
                "            url = f\"{self.dataManagementUrl}{resource}/deletes\"\r\n",
                "        else:\r\n",
                "            url = f\"{self.dataManagementUrl}{resource}/deletes?MinChangeVersion={minChangeVersion}&MaxChangeVersion={maxChangeVersion}\"\r\n",
                "        \r\n",
                "        result = requests.get(url,headers = {\"Authorization\": f\"Bearer {self.getAccessToken()}\"})\r\n",
                "        return result\r\n",
                "\r\n",
                "    def writeToDeletesFile(self, resource, deletes):\r\n",
                "        path = f\"stage1/{self.transactionalFolder}{resource}/delete_batch_data/rundate={self.runDate}/data{uuid.uuid4()}.json\"\r\n",
                "        mssparkutils.fs.put(oea.to_url(path),deletes.text)\r\n",
                "\r\n",
                "    def landEntities(self, entities = 'All', debugMode = False):\r\n",
                "        if entities == 'All':\r\n",
                "            entities = self.getEntities()\r\n",
                "        else:\r\n",
                "            entities = self.getSpecifiedEntities(entities)\r\n",
                "        changeVersion, changeVersionResponseStatus = self.getChangeQueryVersion()\r\n",
                "        if changeVersionResponseStatus < 400:\r\n",
                "            # FIXME - Temporary fix for oldestChangeVersion casing\r\n",
                "            try:\r\n",
                "                minChangeVersion = changeVersion['OldestChangeVersion'] if self.minChangeVer == None else int(self.minChangeVer)\r\n",
                "                maxChangeVersion = changeVersion['NewestChangeVersion']  if self.maxChangeVer == None else int(self.maxChangeVer)\r\n",
                "            except:\r\n",
                "                minChangeVersion = changeVersion['oldestChangeVersion'] if self.minChangeVer == None else int(self.minChangeVer)\r\n",
                "                maxChangeVersion = changeVersion['newestChangeVersion']  if self.maxChangeVer == None else int(self.maxChangeVer)\r\n",
                "        else:\r\n",
                "            minChangeVersion = None\r\n",
                "            maxChangeVersion = None\r\n",
                "        \r\n",
                "        for entity in entities:\r\n",
                "            resource = entity['resource']\r\n",
                "            resourceMinChangeVersion = self.getChangeVersion(resource, minChangeVersion) if self.minChangeVer == None else minChangeVersion\r\n",
                "\r\n",
                "            self.landEntity(resource, resourceMinChangeVersion, maxChangeVersion, debugMode)\r\n",
                "            deletes = self.getDeletes(resource,resourceMinChangeVersion,maxChangeVersion)\r\n",
                "            if len(deletes.json()):\r\n",
                "                logger.info(f\"DELETES: Writing deletes for the resource: {entity}\")\r\n",
                "                self.writeToDeletesFile(resource,deletes)\r\n",
                "    \r\n",
                "    def getChangeVersion(self, resource, default):\r\n",
                "        path = f\"stage1/{self.transactionalFolder}{resource}/changeFile.json\"\r\n",
                "        if mssparkutils.fs.exists(oea.to_url(path)):\r\n",
                "            return json.loads(mssparkutils.fs.head(oea.to_url(path)))['changeVersion']\r\n",
                "        else:\r\n",
                "            return default\r\n",
                "\r\n",
                "    def landEntity(self,resource,minChangeVersion,maxChangeVersion, debugMode = False):\r\n",
                "        logger.info(f\"initiating {resource}\")\r\n",
                "        if minChangeVersion is None and maxChangeVersion is None:\r\n",
                "            url = f\"{self.dataManagementUrl}{resource}?totalCount=true\"\r\n",
                "        else:\r\n",
                "            url = f\"{self.dataManagementUrl}{resource}?MinChangeVersion={minChangeVersion}&MaxChangeVersion={maxChangeVersion}&totalCount=true\"\r\n",
                "            \r\n",
                "        path = f\"stage1/{self.transactionalFolder}{resource}\"\r\n",
                "        total_count_response = requests.get(url, headers={\"Authorization\":f\"Bearer {self.getAccessToken()}\"})\r\n",
                "        try:\r\n",
                "            #Keyset pagination implementation: https://techdocs.ed-fi.org/display/ODSAPIS3V61/Improve+Paging+Performance+on+Large+API+Resources\r\n",
                "            \r\n",
                "            #split into the total number of partitions, and the range size\r\n",
                "            total_count = int(total_count_response.headers[\"Total-Count\"])\r\n",
                "            if debugMode:\r\n",
                "                logger.info(f\"--- Total Count         : {total_count}\")\r\n",
                "                logger.info(f\"--- Batch Size         : {self.batchLimit}\")\r\n",
                "\r\n",
                "            partitions = math.ceil(total_count / self.batchLimit)                \r\n",
                "\r\n",
                "            #raise(ValueError('ERROR'))\r\n",
                "            if(total_count == 0 and partitions == 0):\r\n",
                "                logger.info(f'No new / updated items b/w the following versions {minChangeVersion} and {maxChangeVersion}')\r\n",
                "            else:\r\n",
                "                range_size = math.ceil(maxChangeVersion / partitions)\r\n",
                "                for i in range(partitions + 1):\r\n",
                "                    #calculate the min and max change version for the partition\r\n",
                "                    partitionMinChangeVersion = i*range_size\r\n",
                "                    partitionMaxChangeVersion = min(maxChangeVersion, (i+1)*range_size)\r\n",
                "\r\n",
                "                    #Calculate the number of batches per partition\r\n",
                "                    partitionUrl=f\"{self.dataManagementUrl}{resource}?MinChangeVersion={partitionMinChangeVersion}&MaxChangeVersion={partitionMaxChangeVersion}&totalCount=true\"\r\n",
                "                    partition_count_response = requests.get(partitionUrl, headers={\"Authorization\":f\"Bearer {self.getAccessToken()}\"})\r\n",
                "                    partition_count = int(partition_count_response.headers[\"Total-Count\"])\r\n",
                "                    batches = partition_count // self.batchLimit\r\n",
                "\r\n",
                "                    if debugMode:\r\n",
                "                        logger.info(f\"--- Partition Number         : {i}\")\r\n",
                "                        logger.info(f\"--- Partition MinChangeVer   : {partitionMinChangeVersion}\")\r\n",
                "                        logger.info(f\"--- Partition MaxChangeVer   : {partitionMaxChangeVersion}\")\r\n",
                "                        logger.info(f\"--- Number of batches        : {batches}\", )\r\n",
                "                        logger.info(f\"--- Number of partitions     : {partition_count}\")\r\n",
                "\r\n",
                "                    for j in range(batches + 1):\r\n",
                "                        batchUrl=f\"{partitionUrl}&limit={self.batchLimit}&offset={(j)*self.batchLimit}\"\r\n",
                "                        data = requests.get(batchUrl, headers={\"Authorization\":f\"Bearer {self.getAccessToken()}\"}) \r\n",
                "                        if(data.status_code < 400):         \r\n",
                "                            filepath = f\"{path}/delta_batch_data/rundate={self.runDate}/data{uuid.uuid4()}.json\"\r\n",
                "                            output = json.loads(data.text)\r\n",
                "                            output_string = \"\"\r\n",
                "                            for line in output:\r\n",
                "                                output_string += json.dumps(line) + \"\\n\"\r\n",
                "                            mssparkutils.fs.put(oea.to_url(filepath),output_string)\r\n",
                "                        else:\r\n",
                "                            logger.info(f\"There was an error retrieving batch data for {resource}\")\r\n",
                "        except ZeroDivisionError as zero_error:\r\n",
                "            logger.error(f'Divide by Zero Error - {zero_error}; Landing Data of offset = 0')\r\n",
                "            data = requests.get(url, headers={\"Authorization\":f\"Bearer {self.getAccessToken()}\"})          \r\n",
                "            #print(data.text)\r\n",
                "            if(data.status_code < 400):         \r\n",
                "                filepath = f\"{path}/delta_batch_data/rundate={self.runDate}/data{uuid.uuid4()}.json\"\r\n",
                "                output = json.loads(data.text)\r\n",
                "                if(len(output) == 0):\r\n",
                "                    logger.info(f'No new / updated items b/w the following versions {minChangeVersion} and {maxChangeVersion}')\r\n",
                "                else:\r\n",
                "                    output_string = \"\"\r\n",
                "                    for line in output:\r\n",
                "                        output_string += json.dumps(line) + \"\\n\"\r\n",
                "                    mssparkutils.fs.put(oea.to_url(filepath),output_string)\r\n",
                "            else:\r\n",
                "                logger.info(f\"There was an error retrieving data for {resource}\")\r\n",
                "            \r\n",
                "        except Exception as error:\r\n",
                "            if resource == '/ed-fi/schoolYearTypes':\r\n",
                "                output = self.returnEntityData(resource = resource,\r\n",
                "                                               minChangeVersion=None,\r\n",
                "                                               maxChangeVersion=None)\r\n",
                "                filepath = f\"{path}/delta_batch_data/rundate={self.runDate}/data{uuid.uuid4()}.json\"\r\n",
                "                if(len(output) == 0):\r\n",
                "                    logger.info(f'No new / updated items b/w the following versions {minChangeVersion} and {maxChangeVersion}')\r\n",
                "                else:\r\n",
                "                    output_string = \"\"\r\n",
                "                    for line in output:\r\n",
                "                        output_string += json.dumps(line) + \"\\n\"\r\n",
                "                    mssparkutils.fs.put(oea.to_url(filepath),output_string)\r\n",
                "            else:\r\n",
                "                logger.info(f'An Error Occured - {error}; Landing of the resource - {resource} skipped')\r\n",
                "                return\r\n",
                "    \r\n",
                "        changeFilepath = f\"{path}/changeFile.json\"\r\n",
                "        changeData = {\"changeVersion\":maxChangeVersion}\r\n",
                "        mssparkutils.fs.put(oea.to_url(changeFilepath),json.dumps(changeData),True)\r\n",
                "        logging.info(f\"completed {resource}\")\r\n",
                "    \r\n",
                "    def parse_text_to_dataframe(self, text_content, delimiter=','):\r\n",
                "        csv_file = StringIO(text_content)\r\n",
                "        df = pd.read_csv(csv_file, delimiter=delimiter) \r\n",
                "        \r\n",
                "        return df\r\n",
                "\r\n",
                "    def extract_entities_for_etl(self, df):\r\n",
                "        concat_list = []\r\n",
                "        entity_names_list = []\r\n",
                "        \r\n",
                "        for index, row in df.iterrows():\r\n",
                "            entity_type = row['entity_type']\r\n",
                "            entity_name = row['entity_name']\r\n",
                "            \r\n",
                "            if entity_type != 'ed-fi':\r\n",
                "                concat_list.append(f'/{entity_type}/{entity_name}')\r\n",
                "            \r\n",
                "            concat_list.append(f'/ed-fi/{entity_name}')\r\n",
                "            entity_names_list.append(entity_name)\r\n",
                "        \r\n",
                "        return concat_list, list(set(entity_names_list))\r\n",
                "\r\n",
                "\r\n",
                "    def getSpecifiedEntities(self, entities_list):\r\n",
                "        data = self.getEntities()\r\n",
                "        entities = [item for item in data if item['resource'] in entities_list]\r\n",
                "        return entities\r\n",
                "\r\n",
                "    def listSpecifiedEntities(self, path): \r\n",
                "        fullpath = path + '/entities-to-extract.csv'\r\n",
                "        pathExists = oea.path_exists(fullpath)\r\n",
                "        if pathExists:\r\n",
                "            csv_str = oea.get_text_from_path(fullpath)\r\n",
                "            csv_pd_df = self.parse_text_to_dataframe(csv_str, delimiter=',')\r\n",
                "            api_entities, entities = self.extract_entities_for_etl(csv_pd_df)\r\n",
                "        else:\r\n",
                "            api_entities = list()\r\n",
                "            entities = list()\r\n",
                "        return api_entities, entities\r\n",
                "    \r\n",
                "    def returnEntityData(self,\r\n",
                "                         resource,\r\n",
                "                         minChangeVersion=None,\r\n",
                "                         maxChangeVersion=None):\r\n",
                "        offset = 0\r\n",
                "        logger.info(f\"initiating {resource}\")\r\n",
                "        try:\r\n",
                "            temp_output = \"PLACEHOLDER\"\r\n",
                "            while temp_output != []:\r\n",
                "                url = f\"{self.dataManagementUrl}{resource}?limit=100&offset={offset}\"\r\n",
                "                temp_access_token = self.getAccessToken()\r\n",
                "                data = requests.get(url, headers={\"Authorization\": f\"Bearer {temp_access_token}\"})\r\n",
                "                if data.status_code == 404:\r\n",
                "                    logger.info(\"RESOURCE NOT FOUND\")\r\n",
                "                    return None\r\n",
                "                if data.status_code < 400:\r\n",
                "                    if (temp_output == \"PLACEHOLDER\"):\r\n",
                "                        temp_output = json.loads(data.text)\r\n",
                "                        output = temp_output\r\n",
                "                    else:\r\n",
                "                        temp_output = json.loads(data.text)\r\n",
                "                        output = output + temp_output\r\n",
                "                else:\r\n",
                "                    logging.info(f'ERROR - {data.status_code}')\r\n",
                "                    return None\r\n",
                "                offset += 50\r\n",
                "        except Exception as e:\r\n",
                "            logging.error(f\"ERROR Occurred - {e}\")\r\n",
                "        return output\r\n",
                "\r\n",
                "    def fetch_descriptors(self, \r\n",
                "                          descriptor_col, \r\n",
                "                          entities_info, \r\n",
                "                          minChangeVersion = None, \r\n",
                "                          maxChangeVersion = None):\r\n",
                "        for entity_info in entities_info:\r\n",
                "            if entity_info['resource'] == f'/ed-fi/{descriptor_col}':\r\n",
                "                descriptor_col = f'/ed-fi/{descriptor_col}'\r\n",
                "                \r\n",
                "            elif entity_info['resource'] == f'/TX/{descriptor_col}':\r\n",
                "                descriptor_col = f'/TX/{descriptor_col}'\r\n",
                "            \r\n",
                "        if not descriptor_col.startswith('/'):\r\n",
                "            logger.info(\"No Such Entity\")\r\n",
                "            return None\r\n",
                "        \r\n",
                "        output = self.returnEntityData(resource = descriptor_col,\r\n",
                "                                       minChangeVersion = minChangeVersion, \r\n",
                "                                       maxChangeVersion = maxChangeVersion)\r\n",
                "        if output is not None:\r\n",
                "            spark_df = spark.createDataFrame(output)\r\n",
                "        else:\r\n",
                "            return output\r\n",
                "        return spark_df"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Ed-Fi Refinement"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "class EdFiRefine:\r\n",
                "    #The constructor\r\n",
                "    def __init__(self, \r\n",
                "                 workspace, \r\n",
                "                 oea, \r\n",
                "                 schema_gen,\r\n",
                "                 moduleName, \r\n",
                "                 authUrl,\r\n",
                "                 swaggerUrl, \r\n",
                "                 dataManagementUrl, \r\n",
                "                 changeQueriesUrl, \r\n",
                "                 dependenciesUrl, \r\n",
                "                 apiVersion, \r\n",
                "                 schoolYear, \r\n",
                "                 districtId,\r\n",
                "                 test_mode):\r\n",
                "        self.workspace = workspace\r\n",
                "        self.oea = oea\r\n",
                "        self.schema_gen = schema_gen\r\n",
                "\r\n",
                "        self.moduleName = moduleName\r\n",
                "        self.authUrl = authUrl\r\n",
                "        self.swaggerUrl = swaggerUrl\r\n",
                "        self.dataManagementUrl = dataManagementUrl\r\n",
                "        self.dependenciesUrl = dependenciesUrl\r\n",
                "        self.apiVersion = apiVersion\r\n",
                "        \r\n",
                "        self.schoolYear = schoolYear\r\n",
                "        self.districtId = districtId\r\n",
                "\r\n",
                "        self.schemas = self.schema_gen.create_spark_schemas()\r\n",
                "        self.primitive_datatypes = ['timestamp', 'date', 'decimal', 'boolean', 'integer', 'string', 'long']\r\n",
                "        self.test_mode = test_mode\r\n",
                "\r\n",
                "    def get_descriptor_schema(self, descriptor):\r\n",
                "        fields = []\r\n",
                "        fields.append(StructField('_etag',LongType(), True))\r\n",
                "        fields.append(StructField(f\"{descriptor[:-1]}Id\", IntegerType(), True))\r\n",
                "        fields.append(StructField('codeValue',StringType(), True))\r\n",
                "        fields.append(StructField('description',StringType(), True))\r\n",
                "        fields.append(StructField('id',StringType(), True))\r\n",
                "        fields.append(StructField('namespace',StringType(), True))\r\n",
                "        fields.append(StructField('shortDescription',StringType(), True))\r\n",
                "        return StructType(fields)\r\n",
                "\r\n",
                "    def get_descriptor_metadata(self, descriptor):\r\n",
                "        return [['_etag', 'long', 'no-op'],\r\n",
                "                [f\"{descriptor[:-1]}Id\", 'integer', 'hash'],\r\n",
                "                ['codeValue','string', 'no-op'],\r\n",
                "                ['description','string', 'no-op'],\r\n",
                "                ['id','string', 'no-op'],\r\n",
                "                ['namespace','string', 'no-op'],\r\n",
                "                ['shortDescription','string', 'no-op']]\r\n",
                "\r\n",
                "    def has_column(self, df, col):\r\n",
                "        try:\r\n",
                "            df[col]\r\n",
                "            return True\r\n",
                "        except AnalysisException:\r\n",
                "            return False\r\n",
                "\r\n",
                "    def modify_descriptor_value(self, df, col_name, districtId_col_name = 'DistrictId', schoolYear_col_name = 'SchoolYear'):\r\n",
                "        if col_name in df.columns:\r\n",
                "            # TODO: @Abhinav, I do not see where you made the changes to use the descriptorId instead of Namespace/CodeValue\r\n",
                "            df = df.withColumn(f\"{col_name}LakeId\", f.concat_ws('_', f.col(districtId_col_name), f.col(schoolYear_col_name), f.regexp_replace(col_name, '#', '_')))\r\n",
                "            df = df.drop(col_name)\r\n",
                "        else:\r\n",
                "            df = df.withColumn(f\"{col_name}LakeId\", f.lit(None).cast(\"String\"))\r\n",
                "\r\n",
                "        return df\r\n",
                "\r\n",
                "    def flatten_reference_col(self, df, target_col, districtId_col_name = 'DistrictId', schoolYear_col_name = 'SchoolYear'):\r\n",
                "        col_prefix = target_col.name.replace('Reference', '')\r\n",
                "        df = df.withColumn(f\"{col_prefix}LakeId\", f.when(f.col(target_col.name).isNotNull(), f.concat_ws('_', f.col(districtId_col_name), f.col(schoolYear_col_name), f.split(f.col(f'{target_col.name}.link.href'), '/').getItem(3))))\r\n",
                "        df = df.drop(target_col.name)\r\n",
                "        return df\r\n",
                "\r\n",
                "    def modify_references_and_descriptors(self, df, target_col, districtId_col_name = 'DistrictId', schoolYear_col_name = 'SchoolYear'):\r\n",
                "        for ref_col in [x for x in df.columns if re.search('Reference$', x) is not None]:\r\n",
                "            df = self.flatten_reference_col(df, target_col.dataType.elementType[ref_col], districtId_col_name, schoolYear_col_name)\r\n",
                "        for desc_col in [x for x in df.columns if re.search('Descriptor$', x) is not None]:\r\n",
                "            df = self.modify_descriptor_value(df, desc_col, districtId_col_name, schoolYear_col_name)\r\n",
                "        return df\r\n",
                "\r\n",
                "    def explode_arrays(self, \r\n",
                "                       df, \r\n",
                "                       sink_general_path, \r\n",
                "                       target_col, \r\n",
                "                       schema_name, \r\n",
                "                       table_name, \r\n",
                "                       extension = None, \r\n",
                "                       districtId_col_name = 'DistrictId',\r\n",
                "                       schoolYear_col_name = 'SchoolYear',\r\n",
                "                       parent_cols = ['lakeId', 'DistrictId', 'SchoolYear', 'LastModifiedDate', 'rowIsActive'],\r\n",
                "                       nonNull_count = 1):\r\n",
                "        cols = parent_cols#['lakeId', 'DistrictId', 'SchoolYear', 'LastModifiedDate']\r\n",
                "        child_url = oea.to_url(f\"{sink_general_path}_{target_col.name}\")\r\n",
                "        if nonNull_count <=0 and DeltaTable.isDeltaTable(spark, child_url):\r\n",
                "            logger.info('Child Table - No Ingress Records (Empty Schema Present)')\r\n",
                "        else:\r\n",
                "            child_df = df.select(cols + [target_col.name])\r\n",
                "            child_df = child_df.withColumn(\"exploded\", f.explode(target_col.name)).drop(target_col.name).select(cols + ['exploded.*'])\r\n",
                "            child_df_cached = child_df.cache()\r\n",
                "            child_df = child_df_cached\r\n",
                "\r\n",
                "            grand_child_df = None\r\n",
                "            grand_child_df_cached = None\r\n",
                "\r\n",
                "            # TODO: It looks like te {target_col.name}LakeId column is not addedd to the child entities\r\n",
                "            #       We should use LakeId suffix when using the \"id\" column from the parent and HKey suffix when creating a Hash Key based on composite key columns\r\n",
                "            identity_cols = [x.name for x in target_col.dataType.elementType.fields if 'x-Ed-Fi-isIdentity' in x.metadata].sort()\r\n",
                "            if(identity_cols is not None and len(identity_cols) > 0):\r\n",
                "                child_df = child_df.withColumn(f\"{target_col.name}LakeId\", f.concat(f.col(districtId_col_name), f.lit('_'), f.col(schoolYear_col_name), f.lit('_'), *[f.concat(f.col(x), f.lit('_')) for x in identity_cols]))\r\n",
                "            \r\n",
                "            # IMPORTANT: We must modify Reference and Descriptor columns for child columns \"first\". \r\n",
                "            # This must be done \"after\" the composite key from identity_cols has been created otherwise the columns are renamed and will not be found by identity_cols.\r\n",
                "            # This must be done \"before\" the grand_child is exploded below\r\n",
                "            child_df = self.modify_references_and_descriptors(child_df, target_col, districtId_col_name, schoolYear_col_name)\r\n",
                "\r\n",
                "            for array_sub_col in [x for x in target_col.dataType.elementType.fields if x.dataType.typeName() == 'array' ]:\r\n",
                "                grand_child_url = oea.to_url(f\"{sink_general_path}_{target_col.name}_{array_sub_col.name}\")\r\n",
                "                \r\n",
                "                # TODO: Experiment with Non Nulls and Optimization (Future)\r\n",
                "                # child_df_size = child_df.withColumn(\"size\", F.size(F.col(array_sub_col.name)))\r\n",
                "                # nonNull_count = child_df_size.filter(F.col(\"size\") >= 1).count()\r\n",
                "                nonNull_count = 1\r\n",
                "                \r\n",
                "                if nonNull_count <=0 and DeltaTable.isDeltaTable(spark, grand_child_url):\r\n",
                "                    logger.info('Grand Child Table - No Ingress Records (Empty Schema Present)')\r\n",
                "                else:\r\n",
                "                    grand_child_df = child_df.withColumn('exploded', f.explode(array_sub_col.name)).select(child_df.columns + ['exploded.*']).drop(array_sub_col.name)\r\n",
                "                    grand_child_df_cached = grand_child_df.cache()\r\n",
                "                    grand_child_df = grand_child_df_cached\r\n",
                "                    \r\n",
                "                    # Modifying Reference and Descriptor columns for the grand_child array\r\n",
                "                    grand_child_df = self.modify_references_and_descriptors(grand_child_df, array_sub_col, districtId_col_name, schoolYear_col_name)\r\n",
                "\r\n",
                "                    logger.info(f\"Writing Grand Child Table - {table_name}_{target_col.name}_{array_sub_col.name}\")\r\n",
                "                    # TODO: Review impact of changing from UPSERT to delete_then_insert\r\n",
                "                    self.oea.delete_then_insert(df = grand_child_df, \r\n",
                "                            destination_path = f\"{sink_general_path}_{target_col.name}_{array_sub_col.name}\", \r\n",
                "                            primary_key = 'lakeId',\r\n",
                "                            partitioning = True,\r\n",
                "                            partitioning_cols = [districtId_col_name, schoolYear_col_name]) \r\n",
                "                    self.oea.add_to_lake_db(source_entity_path = f\"{sink_general_path}_{target_col.name}_{array_sub_col.name}\", \r\n",
                "                                    overwrite = True,\r\n",
                "                                    extension = extension)\r\n",
                "                    #grand_child_df.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').save(oea.to_url(f\"{sink_general_path}_{target_col.name}_{array_sub_col.name}\"))\r\n",
                "\r\n",
                "            logger.info(f\"Writing Child Table - {table_name}_{target_col.name}\")\r\n",
                "            # TODO: Review impact of changing from UPSERT to delete_then_insert\r\n",
                "            # FIXME: assessments_periods Temporary Fix\r\n",
                "            child_destination_path = f\"{sink_general_path}_{target_col.name}\"\r\n",
                "            if ('/assessments_period' in child_destination_path) and ('/assessments_periods' not in child_destination_path):\r\n",
                "                child_destination_path = child_destination_path.replace('/assessments_period', '/assessments_periods')\r\n",
                "            \r\n",
                "            self.oea.delete_then_insert(df = child_df, \r\n",
                "                    destination_path = child_destination_path, \r\n",
                "                    primary_key = 'lakeId',\r\n",
                "                    partitioning = True,\r\n",
                "                    partitioning_cols = [districtId_col_name, schoolYear_col_name]) \r\n",
                "            \r\n",
                "            self.oea.add_to_lake_db(source_entity_path = child_destination_path,#f\"{sink_general_path}_{target_col.name}\",\r\n",
                "                            overwrite = True,\r\n",
                "                            extension = extension)\r\n",
                "            #child_df.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').save(oea.to_url(f\"{sink_general_path}_{target_col.name}\"))\r\n",
                "            child_df_cached.unpersist()\r\n",
                "            if grand_child_df:\r\n",
                "                grand_child_df_cached.unpersist()\r\n",
                "\r\n",
                "        # Drop array column from parent entity\r\n",
                "        df = df.drop(target_col.name)\r\n",
                "        return df\r\n",
                "\r\n",
                "    def transform(self,\r\n",
                "                df, \r\n",
                "                schema_name, \r\n",
                "                table_name, \r\n",
                "                primary_key,\r\n",
                "                ext_entity,\r\n",
                "                sink_general_path,\r\n",
                "                districtId_col_name = 'DistrictId', \r\n",
                "                schoolYear_col_name = 'SchoolYear'):\r\n",
                "        if re.search('Descriptors$', table_name) is None:\r\n",
                "            # Use Deep Copy otherwise the schemas object also gets modified every time target_schema is modified\r\n",
                "            target_schema = copy.deepcopy(self.schemas[table_name])\r\n",
                "            # Add primary key\r\n",
                "            if self.has_column(df, primary_key):\r\n",
                "                df = df.withColumn('lakeId', f.concat_ws('_', f.col(districtId_col_name), f.col(schoolYear_col_name), f.col(primary_key)).cast(\"String\"))\r\n",
                "            else:\r\n",
                "                df = df.withColumn('lakeId', f.lit(None).cast(\"String\"))\r\n",
                "        else:\r\n",
                "            target_schema = self.get_descriptor_schema(table_name)\r\n",
                "            # Add primary key\r\n",
                "            if self.has_column(df, 'codeValue') and self.has_column(df, 'namespace'):\r\n",
                "                # TODO: @Abhinav, I do not see where you made the changes to use the descriptorId instead of Namespace/CodeValue\r\n",
                "                df = df.withColumn('lakeId', f.concat_ws('_', f.col(districtId_col_name), f.col(schoolYear_col_name), f.col('namespace'), f.col('codeValue')).cast(\"String\"))\r\n",
                "            else:\r\n",
                "                df = df.withColumn('lakeId', f.lit(None).cast(\"String\"))\r\n",
                "\r\n",
                "        # FIXME schoolYearTypes TEMPORARY FIX\r\n",
                "        if table_name == 'schoolYearTypes':\r\n",
                "            target_schema = target_schema.add(StructField(districtId_col_name, StringType()))\\\r\n",
                "                                    .add(StructField('LastModifiedDate', TimestampType())) \\\r\n",
                "                                    .add(StructField('rowIsActive', BooleanType()))\r\n",
                "        else:\r\n",
                "            target_schema = target_schema.add(StructField(districtId_col_name, StringType()))\\\r\n",
                "                                        .add(StructField(schoolYear_col_name, StringType()))\\\r\n",
                "                                        .add(StructField('LastModifiedDate', TimestampType())) \\\r\n",
                "                                        .add(StructField('rowIsActive', BooleanType()))\r\n",
                "\r\n",
                "        df = self.transform_sub_module(df, \r\n",
                "                                       target_schema, \r\n",
                "                                       sink_general_path, \r\n",
                "                                       schema_name, \r\n",
                "                                       table_name,\r\n",
                "                                       extension = None, \r\n",
                "                                       districtId_col_name = districtId_col_name, \r\n",
                "                                       schoolYear_col_name = schoolYear_col_name)\r\n",
                "\r\n",
                "        # FIXME schoolYearTypes TEMPORARY FIX\r\n",
                "        if table_name == 'schoolYearTypes':\r\n",
                "            df = df.withColumnRenamed(\"schoolYear\", schoolYear_col_name)\r\n",
                "        \r\n",
                "        if self.test_mode:\r\n",
                "            return df\r\n",
                "\r\n",
                "        logger.info(f\"Writing Main Table - {table_name}\")\r\n",
                "        self.oea.upsert(df = df, \r\n",
                "                destination_path = f\"{sink_general_path}\", \r\n",
                "                primary_key = 'lakeId',\r\n",
                "                partitioning = True,\r\n",
                "                partitioning_cols = [districtId_col_name, schoolYear_col_name]) \r\n",
                "        self.oea.add_to_lake_db(source_entity_path = sink_general_path, \r\n",
                "                        overwrite = True,\r\n",
                "                        extension = None)\r\n",
                "\r\n",
                "        if '_ext' in df.columns:\r\n",
                "            target_schema = self.get_ext_entities_schemas(table_name = table_name,\r\n",
                "                                                    ext_column_name = '_ext',\r\n",
                "                                                    default_value = ext_entity)\r\n",
                "            df = self.flatten_ext_column(df = df, \r\n",
                "                                    table_name = table_name, \r\n",
                "                                    ext_col = '_ext', \r\n",
                "                                    inner_key = ext_entity,\r\n",
                "                                    ext_inner_cols = target_schema.fieldNames(),\r\n",
                "                                    base_cols = ['lakeId', districtId_col_name, 'LastModifiedDate',schoolYear_col_name, 'rowIsActive','id_pseudonym'])\r\n",
                "            sink_general_path = sink_general_path.replace('/ed-fi/', f'/{ext_entity.lower()}/')\r\n",
                "            df = self.transform_sub_module(df, \r\n",
                "                                    target_schema, \r\n",
                "                                    sink_general_path, \r\n",
                "                                    schema_name,\r\n",
                "                                    table_name,\r\n",
                "                                    extension = f\"_{ext_entity.lower()}\",\r\n",
                "                                    districtId_col_name = districtId_col_name,\r\n",
                "                                    schoolYear_col_name = schoolYear_col_name)\r\n",
                "\r\n",
                "            logger.info(f\"Writing EXT Table - {table_name}\")\r\n",
                "            self.oea.upsert(df = df, \r\n",
                "                    destination_path = f\"{sink_general_path}\", \r\n",
                "                    primary_key = 'lakeId',\r\n",
                "                    partitioning = True,\r\n",
                "                    partitioning_cols = [districtId_col_name, schoolYear_col_name]) \r\n",
                "            self.oea.add_to_lake_db(sink_general_path, \r\n",
                "                            overwrite = True,\r\n",
                "                            extension = f\"_{ext_entity.lower()}\")\r\n",
                "            return None\r\n",
                "            \r\n",
                "    def transform_sub_module(self, \r\n",
                "                             df, \r\n",
                "                             target_schema, \r\n",
                "                             sink_general_path, \r\n",
                "                             schema_name, \r\n",
                "                             table_name,\r\n",
                "                             extension = None,\r\n",
                "                             districtId_col_name = 'DistrictId',\r\n",
                "                             schoolYear_col_name = 'SchoolYear'):\r\n",
                "        # print(districtId_col_name)\r\n",
                "        for col_name in target_schema.fieldNames():\r\n",
                "            target_col = target_schema[col_name]\r\n",
                "            # If Primitive datatype, i.e String, Bool, Integer, etc.abs\r\n",
                "            # Note: Descriptor is a String therefore is a Primitive datatype\r\n",
                "            if target_col.dataType.typeName() in self.primitive_datatypes:\r\n",
                "                # If it is a Descriptor\r\n",
                "                if re.search('Descriptor$', col_name) is not None:\r\n",
                "                    df = self.modify_descriptor_value(df, col_name, districtId_col_name, schoolYear_col_name)\r\n",
                "                else:\r\n",
                "                    if col_name in df.columns:\r\n",
                "                        # Casting columns to primitive data types\r\n",
                "                        df = df.withColumn(col_name, f.col(col_name).cast(target_col.dataType))\r\n",
                "                    else:\r\n",
                "                        # If Column not present in dataframe, add column with None values.\r\n",
                "                        df = df.withColumn(col_name, f.lit(None).cast(target_col.dataType))\r\n",
                "            # If Complex datatype, i.e. Object, Array\r\n",
                "            else:\r\n",
                "                if col_name not in df.columns:\r\n",
                "                    df = df.withColumn(col_name, f.lit(None).cast(target_col.dataType))\r\n",
                "                else:\r\n",
                "                    # Generate JSON column as a Complex Type\r\n",
                "                    if (table_name.lower() == 'assessments' and col_name.lower() == 'period') and (target_col.dataType.typeName() != 'array'):\r\n",
                "                        # FIXME: Temporary Fix to deal with assessments_periods\r\n",
                "                        df = df.withColumn(col_name, f.array(f.col(col_name)))\r\n",
                "                        target_col.dataType = f.ArrayType(target_col.dataType)\r\n",
                "                    \r\n",
                "                    df = df.withColumn(f\"{col_name}_json\", f.to_json(f.col(col_name))) \\\r\n",
                "                        .withColumn(col_name, f.from_json(f.col(f\"{col_name}_json\"), target_col.dataType)) \\\r\n",
                "                        .drop(f\"{col_name}_json\")\r\n",
                "                \r\n",
                "                # Modify the links with surrogate keys\r\n",
                "                if re.search('Reference$', col_name) is not None:\r\n",
                "                    df = self.flatten_reference_col(df, target_col, districtId_col_name = districtId_col_name, schoolYear_col_name = schoolYear_col_name)\r\n",
                "                \r\n",
                "                if self.test_mode:\r\n",
                "                    return df\r\n",
                "        \r\n",
                "                if target_col.dataType.typeName() == 'array':\r\n",
                "                    # TODO: Experiment with Non Nulls and Optimization (Future)\r\n",
                "                    # df_size = df.withColumn(\"size\", F.size(F.col(target_col.name))).cache()\r\n",
                "                    # nonNull_count = df_size.filter(F.col(\"size\") >= 1).count()\r\n",
                "                    \r\n",
                "                    nonNull_count = 1\r\n",
                "                    df = self.explode_arrays(df, \r\n",
                "                                             sink_general_path,\r\n",
                "                                             target_col, \r\n",
                "                                             schema_name, \r\n",
                "                                             table_name, \r\n",
                "                                             extension = extension, \r\n",
                "                                             districtId_col_name = districtId_col_name,\r\n",
                "                                             schoolYear_col_name = schoolYear_col_name,\r\n",
                "                                             parent_cols = ['lakeId', districtId_col_name, schoolYear_col_name, 'LastModifiedDate', 'rowIsActive'],\r\n",
                "                                             nonNull_count = nonNull_count)\r\n",
                "                    #df_size.unpersist()\r\n",
                "        return df\r\n",
                "    def get_ext_entities_schemas(self,\r\n",
                "                                table_name = 'staffs',\r\n",
                "                                ext_column_name = '_ext',\r\n",
                "                                default_value = 'TPDM'):\r\n",
                "        target_schema = copy.deepcopy(self.schemas[table_name])\r\n",
                "        for col_name in target_schema.fieldNames():\r\n",
                "            target_col = target_schema[col_name]\r\n",
                "            if target_col.name == ext_column_name:\r\n",
                "                if target_col.dataType[0].name == default_value:\r\n",
                "                    return target_col.dataType[0].dataType         \r\n",
                "                    \r\n",
                "    def flatten_ext_column(self, \r\n",
                "                           df, \r\n",
                "                           table_name, \r\n",
                "                           ext_col, \r\n",
                "                           inner_key,\r\n",
                "                           ext_inner_cols,\r\n",
                "                           base_cols = ['lakeId', 'DistrictId', 'LastModifiedDate', 'rowIsActive','SchoolYear', 'id_pseudonym']):\r\n",
                "        cols = base_cols\r\n",
                "        flattened_cols = ext_inner_cols#[\"educatorPreparationPrograms\"] #_ext_TX_cols[table_name]\r\n",
                "        dict_col = F.col(ext_col)[inner_key]\r\n",
                "        complex_dtype_text = str(df.select('_ext').dtypes[0][1])\r\n",
                "\r\n",
                "        exprs = [dict_col.getItem(key).alias(key) for key in flattened_cols if str(key) in complex_dtype_text]\r\n",
                "        flattened_df = df.select(exprs + cols)\r\n",
                "        return flattened_df\r\n",
                "\r\n",
                "    def sink_path_cleanup(self,destination_path):\r\n",
                "        pattern = re.compile(r'DistrictId=.*?/|SchoolYear=.*?/')\r\n",
                "        destination_path = re.sub(pattern, '', destination_path)\r\n",
                "\r\n",
                "        return destination_path\r\n",
                "\r\n",
                "    def non_common_elements(self, list1, list2):\r\n",
                "        unique_in_list1 = set(list1) - set(list2)\r\n",
                "        unique_in_list2 = set(list2) - set(list1)\r\n",
                "        \r\n",
                "        result = list(unique_in_list1) + list(unique_in_list2)\r\n",
                "        return result\r\n",
                "    \r\n",
                "    def non_empty_elements(self, emptyList, nonEmptyList):\r\n",
                "        unique_in_list1 = set(emptyList) - set(nonEmptyList)        \r\n",
                "        result = list(unique_in_list1)\r\n",
                "        return result\r\n",
                "   \r\n",
                "    def return_non_ext_tables(self):\r\n",
                "        table_names = list(self.schemas.keys())\r\n",
                "        non_ext_table_names = []\r\n",
                "        for table_name in table_names:\r\n",
                "            if 'extension' not in table_name.lower():\r\n",
                "                non_ext_table_names.append(table_name)\r\n",
                "        return non_ext_table_names"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Error Logging"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "class ErrorLogging:\r\n",
                "    def __init__(self, spark, oea, logger):\r\n",
                "        self.spark = spark\r\n",
                "        self.oea = oea\r\n",
                "        self.logger = logger\r\n",
                "        self.pipeline_id = None\r\n",
                "        self.spark_session_id = spark.sparkContext.applicationId\r\n",
                "        self.test_mode = True\r\n",
                "        self.entity_logs = list()\r\n",
                "        self.stage_logs = list()\r\n",
                "        self.pipeline_logs = list()\r\n",
                "        self.etl_logs = None\r\n",
                "\r\n",
                "    # Helper function to generate a random alphanumeric string of specified length\r\n",
                "    def generate_random_alphanumeric(self, length):\r\n",
                "        return uuid.uuid4().hex[:length]\r\n",
                "\r\n",
                "    def set_logs_prefix(self):\r\n",
                "        workspace_name = self.oea.workspace\r\n",
                "        self.storage_account = self.oea.storage_account\r\n",
                "\r\n",
                "        if workspace_name == 'prod' or workspace_name == 'production':\r\n",
                "            self.etl_logs = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/SAP/etl-logs'\r\n",
                "            # self.etl_logs = 'abfss://etl-logs@' + self.storage_account + '.dfs.core.windows.net'\r\n",
                "        elif workspace_name == 'dev' or workspace_name == 'development':\r\n",
                "            self.etl_logs = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/dev/etl-logs'\r\n",
                "        else:\r\n",
                "            self.etl_logs = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/sandboxes/{workspace_name}/etl-logs'\r\n",
                "        \r\n",
                "    def to_logs_url(self, path):\r\n",
                "        if self.etl_logs is None:\r\n",
                "            self.set_logs_prefix()\r\n",
                "        if not path or path == '': raise ValueError('Specified path cannot be empty.')\r\n",
                "        if path.startswith('abfss://'): return path # if a url is given, just return that same url (allows to_url to be invoked just in case translation may be needed)\r\n",
                "        path_args = path.split('/')\r\n",
                "        stage = path_args.pop(0)\r\n",
                "        if stage == 'etl-logs': stage = self.etl_logs\r\n",
                "        else: raise ValueError(\"Logs Path must begin with 'etl-logs'\")\r\n",
                "        url = f\"{stage}/{'/'.join(path_args)}\"\r\n",
                "        logger.debug(f'to_url: {url}')\r\n",
                "        return url      \r\n",
                "\r\n",
                "    def create_log_dict(self, **kwargs):\r\n",
                "        return kwargs\r\n",
                "\r\n",
                "    def consolidate_logs(self, log_data, log_type):\r\n",
                "        if log_type == 'entity':\r\n",
                "            log_data['log_type'] = 'entity'\r\n",
                "            self.entity_logs.append(log_data)\r\n",
                "        elif log_type == 'stage':\r\n",
                "            log_data['log_type'] = 'stage'\r\n",
                "            self.stage_logs.append(log_data)\r\n",
                "        elif log_type == 'pipeline':\r\n",
                "            log_data['log_type'] = 'pipeline'\r\n",
                "            self.pipeline_logs.append(log_data)\r\n",
                "        else:\r\n",
                "            raise ValueError('Invalid Log Type')\r\n",
                "    def create_spark_df(self, log_type):\r\n",
                "        if log_type == 'entity':\r\n",
                "            df = self.spark.createDataFrame(self.entity_logs) \r\n",
                "        elif log_type == 'stage':\r\n",
                "            df = self.spark.createDataFrame(self.stage_logs) \r\n",
                "        elif log_type == 'pipeline':\r\n",
                "            df = self.spark.createDataFrame(self.pipeline_logs)\r\n",
                "        else:\r\n",
                "            raise ValueError('Invalid Log Type')\r\n",
                "        \r\n",
                "        return df\r\n",
                "    \r\n",
                "    def write_logs_to_delta_lake(self, df, log_type,destination_url, partitioning_cols = ['pipelineExecutionId']):\r\n",
                "        #TODO: Pending Edits\r\n",
                "        logger.info('Dynamically over-write the partition')\r\n",
                "        self.spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\r\n",
                "        \r\n",
                "        if log_type == 'entity':\r\n",
                "            df.write.format('delta').mode('overwrite').partitionBy(*partitioning_cols).save(destination_url)\r\n",
                "        if log_type == 'pipeline':\r\n",
                "            df.write.format('delta').mode('overwrite').partitionBy('pipelineExecutionId').save(destination_url)\r\n",
                "        if log_type == 'stage':\r\n",
                "            df.write.format('delta').mode('overwrite').partitionBy('stageName','pipelineExecutionId').save(destination_url)\r\n",
                "    \r\n",
                "    def add_etl_logs_to_lake_db(self, \r\n",
                "                                db_name, \r\n",
                "                                logs_base_path, \r\n",
                "                                log_type,\r\n",
                "                                overwrite = False):\r\n",
                "        logs_full_url = self.to_logs_url(f\"{logs_base_path}/log_type={log_type}\")\r\n",
                "        spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\r\n",
                "        if overwrite:\r\n",
                "            spark.sql(f'DROP TABLE IF EXISTS {db_name}.ETL{log_type}Logs')\r\n",
                "        spark.sql(f\"CREATE TABLE IF NOT EXISTS {db_name}.ETL{log_type}Logs using DELTA location '{logs_full_url}'\")\r\n",
                ""
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Entity Frequency Processor"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "class EntityFrequencyProcessor:\r\n",
                "    def __init__(self, oea, filepath, highFrequentDelta = 1, moderateFrequentDelta = 5, lowFrequentDelta = 10, descriptorsDelta = 360):\r\n",
                "        self.oea = oea\r\n",
                "        self.filepath = filepath\r\n",
                "        self.highFrequentDelta = timedelta(days = highFrequentDelta)\r\n",
                "        self.moderateFrequentDelta = timedelta(days = moderateFrequentDelta)\r\n",
                "        self.lowFrequentDelta = timedelta(days = lowFrequentDelta)\r\n",
                "        self.descriptorsDelta = timedelta(days = descriptorsDelta)\r\n",
                "        \r\n",
                "    def load_lookup_df(self): \r\n",
                "        text_data = self.oea.get_text_from_path(self.filepath)\r\n",
                "        self.entity_freq_lookup_df = pd.read_csv(StringIO(text_data))\r\n",
                "\r\n",
                "    def set_freq_fiter_maps(self):\r\n",
                "        self.highFrequentMap = (self.entity_freq_lookup_df['resource_frequency_code'] == 'high') & (self.entity_freq_lookup_df['temp_timedelta'] >= self.highFrequentDelta)\r\n",
                "        self.moderateFrequentMap = (self.entity_freq_lookup_df['resource_frequency_code'] == 'moderate') & (self.entity_freq_lookup_df['temp_timedelta'] >= self.moderateFrequentDelta)\r\n",
                "        self.lowFrequentMap = (self.entity_freq_lookup_df['resource_frequency_code'] == 'low') & (self.entity_freq_lookup_df['temp_timedelta'] >= self.lowFrequentDelta)\r\n",
                "        self.descriptorsMap = (self.entity_freq_lookup_df['resource_frequency_code'] == 'descriptor') & (self.entity_freq_lookup_df['temp_timedelta'] >= self.descriptorsDelta)\r\n",
                "    \r\n",
                "    def return_entities_to_etl(self):\r\n",
                "        today_date = datetime.today() #.date()\r\n",
                "        self.entity_freq_lookup_df['temp_timedelta'] = today_date - pd.to_datetime(self.entity_freq_lookup_df['lastrundatetime']) #.dt.date\r\n",
                "        self.set_freq_fiter_maps()\r\n",
                "        \r\n",
                "        entities_to_etl_filter_map = self.highFrequentMap | self.lowFrequentMap | self.moderateFrequentMap | self.descriptorsMap\r\n",
                "        entities_to_etl = list(self.entity_freq_lookup_df[entities_to_etl_filter_map]['resource_full_name'].values)\r\n",
                "        self.entity_freq_lookup_df.drop(['temp_timedelta'], axis = 1, inplace = True)\r\n",
                "\r\n",
                "        entities_to_etl_dict = self.entity_freq_lookup_df.loc[entities_to_etl_filter_map,['resource_domain', 'resource_sub_name']].groupby('resource_domain').aggregate(list).to_dict()['resource_sub_name']\r\n",
                "        return entities_to_etl, entities_to_etl_dict\r\n",
                "\r\n",
                "    def update_lookup_df(self):\r\n",
                "        # TODO: WIP\r\n",
                "        today_date = datetime.today().date()\r\n",
                "        self.entity_freq_lookup_df.loc[self.highFrequentMap | self.lowFrequentMap | self.moderateFrequentMap | self.descriptorsMap, 'lastrundatetime'] = datetime.today()\r\n",
                "        self.entity_freq_lookup_df.loc[self.highFrequentMap | self.lowFrequentMap | self.moderateFrequentMap | self.descriptorsMap, 'lastrundate'] = today_date\r\n",
                "    \r\n",
                "    def write_lookup_df(self, destination_path):\r\n",
                "        data_str = self.entity_freq_lookup_df.to_csv(index=False)  # You can customize options based on your needs\r\n",
                "        destination_url = self.oea.to_url(destination_path)\r\n",
                "        mssparkutils.fs.put(destination_url, data_str, True)\r\n",
                "        "
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "class EntityFrequencyProcessor:\r\n",
                "    def __init__(self, oea, filepath, highFrequentDelta = 1, moderateFrequentDelta = 5, lowFrequentDelta = 10, descriptorsDelta = 360):\r\n",
                "        self.oea = oea\r\n",
                "        self.filepath = filepath\r\n",
                "        self.highFrequentDelta = timedelta(days = highFrequentDelta)\r\n",
                "        self.moderateFrequentDelta = timedelta(days = moderateFrequentDelta)\r\n",
                "        self.lowFrequentDelta = timedelta(days = lowFrequentDelta)\r\n",
                "        self.descriptorsDelta = timedelta(days = descriptorsDelta)\r\n",
                "        \r\n",
                "    def load_lookup_df(self): \r\n",
                "        text_data = self.oea.get_text_from_path(self.filepath)\r\n",
                "        self.entity_freq_lookup_df = pd.read_csv(StringIO(text_data))\r\n",
                "\r\n",
                "    def set_freq_fiter_maps(self):\r\n",
                "        self.highFrequentMap = (self.entity_freq_lookup_df['resource_frequency_code'] == 'high') & (self.entity_freq_lookup_df['temp_timedelta'] >= self.highFrequentDelta)\r\n",
                "        self.moderateFrequentMap = (self.entity_freq_lookup_df['resource_frequency_code'] == 'moderate') & (self.entity_freq_lookup_df['temp_timedelta'] >= self.moderateFrequentDelta)\r\n",
                "        self.lowFrequentMap = (self.entity_freq_lookup_df['resource_frequency_code'] == 'low') & (self.entity_freq_lookup_df['temp_timedelta'] >= self.lowFrequentDelta)\r\n",
                "        self.descriptorsMap = (self.entity_freq_lookup_df['resource_frequency_code'] == 'descriptor') & (self.entity_freq_lookup_df['temp_timedelta'] >= self.descriptorsDelta)\r\n",
                "    \r\n",
                "    def return_entities_to_etl(self):\r\n",
                "        today_date = datetime.today() #.date()\r\n",
                "        self.entity_freq_lookup_df['temp_timedelta'] = today_date - pd.to_datetime(self.entity_freq_lookup_df['lastrundatetime']) #.dt.date\r\n",
                "        self.set_freq_fiter_maps()\r\n",
                "        \r\n",
                "        entities_to_etl_filter_map = self.highFrequentMap | self.lowFrequentMap | self.moderateFrequentMap | self.descriptorsMap\r\n",
                "        entities_to_etl = list(self.entity_freq_lookup_df[entities_to_etl_filter_map]['resource_full_name'].values)\r\n",
                "        self.entity_freq_lookup_df.drop(['temp_timedelta'], axis = 1, inplace = True)\r\n",
                "\r\n",
                "        entities_to_etl_dict = self.entity_freq_lookup_df.loc[entities_to_etl_filter_map,['resource_domain', 'resource_sub_name']].groupby('resource_domain').aggregate(list).to_dict()['resource_sub_name']\r\n",
                "        return entities_to_etl, entities_to_etl_dict\r\n",
                "\r\n",
                "    def edgraph_return_entities_to_etl(self):\r\n",
                "        today_date = datetime.today() #.date()\r\n",
                "        self.entity_freq_lookup_df['temp_timedelta'] = today_date - pd.to_datetime(self.entity_freq_lookup_df['lastrundatetime']) #.dt.date\r\n",
                "        self.set_freq_fiter_maps()\r\n",
                "        \r\n",
                "        entities_to_etl_filter_map = self.highFrequentMap | self.lowFrequentMap | self.moderateFrequentMap | self.descriptorsMap\r\n",
                "        entities_to_etl = list(self.entity_freq_lookup_df[entities_to_etl_filter_map]['resource_full_name'].values)\r\n",
                "        self.entity_freq_lookup_df.drop(['temp_timedelta'], axis = 1, inplace = True)\r\n",
                "\r\n",
                "        # entities_to_etl_dict = self.entity_freq_lookup_df.loc[entities_to_etl_filter_map,['resource_domain', 'resource_sub_name']].groupby('resource_domain').aggregate(list).to_dict()['resource_sub_name']\r\n",
                "        return entities_to_etl, None\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "    def update_lookup_df(self):\r\n",
                "        # TODO: WIP\r\n",
                "        today_date = datetime.today().date()\r\n",
                "        self.entity_freq_lookup_df.loc[self.highFrequentMap | self.lowFrequentMap | self.moderateFrequentMap | self.descriptorsMap, 'lastrundatetime'] = datetime.today()\r\n",
                "        self.entity_freq_lookup_df.loc[self.highFrequentMap | self.lowFrequentMap | self.moderateFrequentMap | self.descriptorsMap, 'lastrundate'] = today_date\r\n",
                "    \r\n",
                "    def write_lookup_df(self, destination_path):\r\n",
                "        data_str = self.entity_freq_lookup_df.to_csv(index=False)  # You can customize options based on your needs\r\n",
                "        destination_url = self.oea.to_url(destination_path)\r\n",
                "        mssparkutils.fs.put(destination_url, data_str, True)\r\n",
                "        "
            ],
            "outputs": []
        }
    ]
}