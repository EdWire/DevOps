{
    "nbformat": 4,
    "nbformat_minor": 2,
    "metadata": {
        "save_output": true,
        "kernelspec": {
            "name": "synapse_pyspark",
            "display_name": "Synapse PySpark"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "execution_count": 2,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "instance = instanceId = InstanceId\r\n",
                "apiUrl = ApiUrl\r\n",
                "schoolYear = SchoolYear\r\n",
                "DistrictId = DistrictID = districtId = districtID\r\n",
                "apiLimit = batchLimit\r\n",
                "\r\n",
                "prepareEdFiMetaData = prepareEdFiMetadata"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### URL Initializations"
            ],
            "outputs": []
        },
        {
            "execution_count": 2,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "%run OEA/modules/Ed-Fi/v0.7/src/utilities/edfi_v0_7_fetch_urls"
            ],
            "outputs": []
        },
        {
            "execution_count": 3,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "instance_id = instanceId\r\n",
                "school_year = schoolYear\r\n",
                "api_url = apiUrl\r\n",
                "\r\n",
                "edfi_api_manager = EdFiApiManager(api_url, instance_id, school_year)\r\n",
                "edfi_api_manager.update_urls()\r\n",
                "edfi_api_manager.set_other_metadata()\r\n",
                "\r\n",
                "dependenciesUrl = edfi_api_manager.dependencies_url\r\n",
                "openApiMetadataUrl = edfi_api_manager.openapi_metadata_url\r\n",
                "dataManagementUrl = edfi_api_manager.data_management_url\r\n",
                "authUrl = edfi_api_manager.auth_url\r\n",
                "\r\n",
                "changeQueriesUrl = edfi_api_manager.get_referenced_url('Change-Queries')\r\n",
                "changeQueriesUrl = changeQueriesUrl[:-13].replace('/metadata/', '/')\r\n",
                "swagger_url = swaggerUrl = edfi_api_manager.get_referenced_url('Resources')\r\n",
                "\r\n",
                "apiVersion = edfi_api_manager.api_version\r\n",
                "apiVersion = apiVersion[1:] if apiVersion.startswith('v') else apiVersion"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### OEA Initializations"
            ],
            "outputs": []
        },
        {
            "execution_count": 4,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "%run OEA/modules/Ed-Fi/v0.7/src/utilities/edfi_v0_7_edfi_py"
            ],
            "outputs": []
        },
        {
            "execution_count": 5,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "oea = EdFiOEAChild()   \r\n",
                "oea.set_workspace(workspace)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Error Logging Initializations"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "error_logger = ErrorLogging(spark = spark, \r\n",
                "                            oea = oea, \r\n",
                "                            logger = logger)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Main Code"
            ],
            "outputs": []
        },
        {
            "execution_count": 13,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from concurrent.futures import ThreadPoolExecutor\r\n",
                "\r\n",
                "\r\n",
                "def threaded_task(input_tuple):\r\n",
                "    item,full_source_path,tables_source = input_tuple\r\n",
                "    table_path = full_source_path +'/'+ item\r\n",
                "    options = {'source_format': 'json', 'multiline': False}\r\n",
                "    try:\r\n",
                "        entity_path = f\"{tables_source}/{item}\"\r\n",
                "        logger.info(oea.to_url(f'stage2/Ingested/{entity_path}'))\r\n",
                "        # continue\r\n",
                "        if item == 'metadata.csv' or item == 'parameterizedRunLogs':\r\n",
                "            logger.info('ignore metadata processing, since this is not a table to be ingested')\r\n",
                "        else:\r\n",
                "            start_time = datetime.now()\r\n",
                "            number_of_inbound_changes = 0\r\n",
                "            number_of_inbound_changes = oea.ingest(entity_path, \r\n",
                "                                                   primary_key='id', \r\n",
                "                                                   hashing = False,\r\n",
                "                                                   natural_key = None,\r\n",
                "                                                   landingDateTimeFormat = landingDateTimeFormat,\r\n",
                "                                                   ingestionHistoryMode = ingestionHistoryMode,\r\n",
                "                                                   options=options)\r\n",
                "            end_time = datetime.now()\r\n",
                "            log_data = error_logger.create_log_dict(uniqueId = error_logger.generate_random_alphanumeric(10), # Generate a random 10-character alphanumeric value\r\n",
                "                                                pipelineExecutionId = pipelineExecutionId,#'TEST_1234',#executionId,\r\n",
                "                                                sparkSessionId = spark.sparkContext.applicationId,\r\n",
                "                                                stageName = \"ed-fi: Ingestion\",\r\n",
                "                                                schemaFormat = 'ed-fi: nested',\r\n",
                "                                                entityType =  'ed-fi+tx',\r\n",
                "                                                entityName = item,\r\n",
                "                                                numInputRows = number_of_inbound_changes,\r\n",
                "                                                totalNumOutputRows = number_of_inbound_changes,\r\n",
                "                                                numTargetRowsInserted = number_of_inbound_changes,\r\n",
                "                                                numTargetRowsUpdated = 0,\r\n",
                "                                                numRecordsSkipped = 0,\r\n",
                "                                                numRecordsDeleted = 0,\r\n",
                "                                                start_time = start_time,\r\n",
                "                                                end_time = end_time,\r\n",
                "                                                insertionType = 'append' if ingestionHistoryMode else 'upsert')\r\n",
                "            error_logger.consolidate_logs(log_data,'entity')\r\n",
                "    \r\n",
                "    except AnalysisException as e:\r\n",
                "        # This means the table may have not been properly refined due to errors with the primary key not aligning with columns expected in the lookup table.\r\n",
                "        pass\r\n",
                "    except Exception as error:\r\n",
                "        # This means the table may have not been properly refined due to errors with the primary key not aligning with columns expected in the lookup table.\r\n",
                "        logger.error(error)\r\n",
                "\r\n",
                "def ingest_edfi_dataset(tables_source, \r\n",
                "                        items = 'All'):\r\n",
                "    full_source_path = f\"stage1/Transactional/{tables_source}\"\r\n",
                "    if items == 'All':\r\n",
                "        items = oea.get_folders(full_source_path) \r\n",
                "   \r\n",
                "    options = {'source_format': 'json', 'multiline': False}\r\n",
                "    #items = ['staffs', 'students']\r\n",
                "        \r\n",
                "    with ThreadPoolExecutor(max_workers=8) as tpe:\r\n",
                "        # Use map to pass each item as a tuple along with items to the task function\r\n",
                "        logger.info('[INGESTION THREAD] Entered Threadpool')\r\n",
                "        tpe.map(threaded_task,[(item,full_source_path,tables_source) for item in items])"
            ],
            "outputs": []
        },
        {
            "execution_count": 11,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from datetime import datetime\r\n",
                "import math\r\n",
                "source_path = f'stage1/Transactional/Ed-Fi/{apiVersion}/DistrictId={districtId}/SchoolYear={schoolYear}/metadata-assets/frequency_etl.csv'  \r\n",
                "destination_path = source_path #f'stage1/Transactional/Ed-Fi/{apiVersion}/DistrictId={districtId}/SchoolYear={schoolYear}/metadata-assets/frequency_based_etl.csv'  \r\n",
                "logs_path = f\"stage1/Transactional/Ed-Fi/{apiVersion}/DistrictId={districtId}/SchoolYear={schoolYear}/metadata-assets/_frequency_etl_logs/run_logs_{datetime.today().strftime('%Y-%m-%d')}.csv\"\r\n",
                "\r\n",
                "processor = EntityFrequencyProcessor(oea = oea, \r\n",
                "                                     filepath = source_path, \r\n",
                "                                     highFrequentDelta = highFrequentDelta,#0.005, \r\n",
                "                                     moderateFrequentDelta = moderateFrequentDelta, #5, \r\n",
                "                                     lowFrequentDelta = lowFrequentDelta, #10, \r\n",
                "                                     descriptorsDelta = descriptorsDelta) #360)\r\n",
                "\r\n",
                "processor.load_lookup_df()\r\n",
                "_, entities_to_etl = processor.return_entities_to_etl()\r\n",
                "\r\n",
                "edfiEntities = \"All\" #['schoolYearTypes']\r\n",
                "tpdmEntities = 'All'\r\n",
                "\r\n",
                "edfiEntities = entities_to_etl.get('ed-fi', [])\r\n",
                "tpdmEntities = entities_to_etl.get('tpdm', [])"
            ],
            "outputs": []
        },
        {
            "execution_count": 14,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from datetime import datetime\r\n",
                "ingest_edfi_dataset(f'{moduleName}/{apiVersion}/DistrictId={districtId}/SchoolYear={schoolYear}/ed-fi',\r\n",
                "                    edfiEntities)\r\n",
                "ingest_edfi_dataset(f'{moduleName}/{apiVersion}/DistrictId={districtId}/SchoolYear={schoolYear}/tpdm',\r\n",
                "                    tpdmEntities)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Writing Logs"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "if error_logger.entity_logs != []:\r\n",
                "    logger.info('[INGESTION ERROR LOGGING] Writing Entity Level Error Logs')\r\n",
                "    df = error_logger.create_spark_df('entity')\r\n",
                "    error_logger.write_logs_to_delta_lake(df = df, \r\n",
                "                                log_type = 'entity',\r\n",
                "                                destination_url = error_logger.to_logs_url('etl-logs/log_type=entity'))\r\n",
                "    error_logger.add_etl_logs_to_lake_db(db_name = f'ldb_{workspace}_edfi_etl_logs',\r\n",
                "                                        logs_base_path = 'etl-logs',\r\n",
                "                                        log_type = 'entity',\r\n",
                "                                        overwrite = True)"
            ],
            "outputs": []
        }
    ]
}