{
    "nbformat": 4,
    "nbformat_minor": 2,
    "metadata": {
        "save_output": true,
        "kernelspec": {
            "name": "synapse_pyspark",
            "display_name": "Synapse PySpark"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "execution_count": 16,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "import copy\r\n",
                "import pyspark.sql.functions as f\r\n",
                "from concurrent.futures import ThreadPoolExecutor\r\n",
                "from datetime import datetime\r\n",
                "from requests.adapters import HTTPAdapter\r\n",
                "from requests.packages.urllib3.util.retry import Retry"
            ],
            "outputs": []
        },
        {
            "execution_count": 17,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from notebookutils import mssparkutils\r\n",
                "import configparser\r\n",
                "\r\n",
                "config_path = \"/tmp/conf.ini\"\r\n",
                "def copy_config_to_temp():\r\n",
                "    file_path = \"abfss://oea@stoeadevusc.dfs.core.windows.net/sandboxes/configs/edfi-configs-2024-02-01.ini\" # oea.to_url(\"stage1/Transactional/SAP/metadata-assets/edfi-configs.ini\")\r\n",
                "    mssparkutils.fs.cp(file_path,\"file:/tmp/conf.ini\")\r\n",
                "\r\n",
                "def read_edfi_credentials(config_path):\r\n",
                "    config = configparser.ConfigParser()\r\n",
                "    config.read(config_path)\r\n",
                "\r\n",
                "    edfi_credentials = {}\r\n",
                "\r\n",
                "    if 'EdFi' in config:\r\n",
                "        edfi_credentials['client_id'] = config['EdFi'].get('client_id', '')\r\n",
                "        edfi_credentials['client_secret'] = config['EdFi'].get('client_secret', '')\r\n",
                "        edfi_credentials['instance_id'] = config['EdFi'].get('instance_id', '')\r\n",
                "\r\n",
                "    return edfi_credentials\r\n",
                "\r\n",
                "try:\r\n",
                "    copy_config_to_temp()\r\n",
                "    edfi_credentials = read_edfi_credentials(config_path)\r\n",
                "    client_id = edfi_credentials['client_id']\r\n",
                "    client_secret_id = edfi_credentials['client_secret']\r\n",
                "    instanceId = edfi_credentials['instance_id']\r\n",
                "except Exception as error:\r\n",
                "    print(f'Error Message - {error}')"
            ],
            "outputs": []
        },
        {
            "execution_count": 18,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "instance = InstanceId = instanceId\r\n",
                "ApiUrl = apiUrl\r\n",
                "SchoolYear = schoolYear\r\n",
                "DistrictId = DistrictID = districtID = districtId\r\n",
                "apiLimit = batchLimit\r\n",
                "\r\n",
                "prepareEdFiMetaData = prepareEdFiMetadata"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### URL Initializations"
            ],
            "outputs": []
        },
        {
            "execution_count": 19,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "%run OEA/modules/Ed-Fi/v0.7/src/utilities/edfi_v0_7_fetch_urls"
            ],
            "outputs": []
        },
        {
            "execution_count": 20,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "instance_id = instanceId\r\n",
                "school_year = schoolYear\r\n",
                "api_url = apiUrl\r\n",
                "\r\n",
                "edfi_api_manager = EdFiApiManager(api_url, instance_id, school_year)\r\n",
                "edfi_api_manager.update_urls()\r\n",
                "edfi_api_manager.set_other_metadata()\r\n",
                "\r\n",
                "dependenciesUrl = edfi_api_manager.dependencies_url\r\n",
                "openApiMetadataUrl = edfi_api_manager.openapi_metadata_url\r\n",
                "dataManagementUrl = edfi_api_manager.data_management_url\r\n",
                "authUrl = edfi_api_manager.auth_url\r\n",
                "\r\n",
                "changeQueriesUrl = edfi_api_manager.get_referenced_url('Change-Queries')\r\n",
                "changeQueriesUrl = changeQueriesUrl[:-13].replace('/metadata/', '/')\r\n",
                "swagger_url = swaggerUrl = edfi_api_manager.get_referenced_url('Resources')\r\n",
                "\r\n",
                "apiVersion = edfi_api_manager.api_version\r\n",
                "apiVersion = apiVersion[1:] if apiVersion.startswith('v') else apiVersion"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### OEA Initializations"
            ],
            "outputs": []
        },
        {
            "execution_count": 21,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "%run OEA/modules/Ed-Fi/v0.7/src/utilities/edfi_v0_7_edfi_py"
            ],
            "outputs": []
        },
        {
            "execution_count": 22,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# FIXME: 2024-02-12: ingestionHistoryMode Under Review\r\n",
                "oea = EdFiOEAChild()   \r\n",
                "oea.set_workspace(workspace)\r\n",
                "oea.ingestionHistoryMode = ingestionHistoryMode"
            ],
            "outputs": []
        },
        {
            "execution_count": 23,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# swagger_url = swaggerUrl = edfi_api_manager.get_referenced_url('Descriptors')\r\n",
                "oea_utils = schema_gen = OpenAPIUtil(swagger_url)\r\n",
                "oea_utils.create_definitions()\r\n",
                "schemas = schema_gen.create_spark_schemas()"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Error Logging Initializations"
            ],
            "outputs": []
        },
        {
            "execution_count": 24,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "error_logger = ErrorLogging(spark = spark, \r\n",
                "                            oea = oea, \r\n",
                "                            logger = logger)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Threading Utilities"
            ],
            "outputs": []
        },
        {
            "execution_count": 25,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "def upsert_data(df_changes, \r\n",
                "                metadata,\r\n",
                "                schema_name, \r\n",
                "                transform_mode,\r\n",
                "                table_name,\r\n",
                "                primary_key,\r\n",
                "                ext_entity,\r\n",
                "                sink_general_path,\r\n",
                "                sink_sensitive_path):\r\n",
                "        df_pseudo, df_lookup = oea.pseudonymize(df_changes, \r\n",
                "                                                metadata,\r\n",
                "                                                transform_mode,\r\n",
                "                                                True)            \r\n",
                "        edfiRefineAgent.transform(df = df_pseudo, \r\n",
                "                schema_name = schema_name, \r\n",
                "                table_name = table_name, \r\n",
                "                primary_key = 'id_pseudonym', \r\n",
                "                ext_entity = ext_entity, \r\n",
                "                sink_general_path = sink_general_path,\r\n",
                "                districtId_col_name = 'DistrictId', \r\n",
                "                schoolYear_col_name = 'SchoolYear')\r\n",
                "        if '/emptySchemas/' not in sink_sensitive_path:            \r\n",
                "                oea.upsert(df = df_lookup, \r\n",
                "                        destination_path = sink_sensitive_path, \r\n",
                "                        primary_key = 'id',\r\n",
                "                        partitioning = True,\r\n",
                "                        partitioning_cols = ['DistrictId', 'SchoolYear'])    \r\n",
                "                oea.add_to_lake_db(source_entity_path = sink_sensitive_path,\r\n",
                "                                overwrite = True,\r\n",
                "                                extension = None)"
            ],
            "outputs": []
        },
        {
            "execution_count": 26,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "def threaded_task_empty_schema(input_tuple):\r\n",
                "    # FIXME: 2024-02-07: Threading Under Dev\r\n",
                "    item, schema_name, s2r_path, ext_entity, metadata, transform_mode, districtId, schoolYear = input_tuple\r\n",
                "    \r\n",
                "    table_name = item #sap_to_edfi_complex[item]\r\n",
                "    try:\r\n",
                "        logger.info('[REFINEMENT EMPTY SCHEMA DUMPING THREAD] Path does not exist - attempting to create empty data frame')                        \r\n",
                "        sink_general_path = f'{s2r_path}/general/{schema_name}/{item}'\r\n",
                "        sink_sensitive_path = f'{s2r_path}/sensitive/{schema_name}/{item}_lookup'\r\n",
                "                        \r\n",
                "        sink_general_path = edfiRefineAgent.sink_path_cleanup(sink_general_path)\r\n",
                "        sink_sensitive_path = edfiRefineAgent.sink_path_cleanup(sink_sensitive_path)\r\n",
                "        if not oea.path_exists(sink_general_path):  \r\n",
                "            # FIXME: 2024-02-07 TEMP FIX       \r\n",
                "            target_schema = copy.deepcopy(edfiRefineAgent.schemas[table_name])    \r\n",
                "            df_changes = spark.createDataFrame(data = [],\r\n",
                "                                                schema = target_schema)\r\n",
                "            df_changes = df_changes.withColumn('DistrictId', F.lit(districtId))\r\n",
                "            df_changes = df_changes.withColumn('SchoolYear', F.lit(schoolYear))\r\n",
                "            \r\n",
                "            current_timestamp = datetime.now()\r\n",
                "            df_changes = df_changes.withColumn('LastModifiedDate', F.lit(current_timestamp))\r\n",
                "            df_changes = df_changes.withColumn('rowIsActive', F.lit(True))\r\n",
                "                            \r\n",
                "\r\n",
                "            if 'id' in df_changes.columns:\r\n",
                "                upsert_data(df_changes, \r\n",
                "                            metadata,\r\n",
                "                            schema_name, \r\n",
                "                            transform_mode,\r\n",
                "                            table_name,\r\n",
                "                            'id',\r\n",
                "                            ext_entity,\r\n",
                "                            sink_general_path,\r\n",
                "                            sink_sensitive_path)\r\n",
                "            else:\r\n",
                "                logger.info(f'[REFINEMENT EMPTY SCHEMA DUMPING THREAD] {item} does not have id as primary key - flagged for future')\r\n",
                "    except Exception as error:\r\n",
                "        logger.exception(f\"[REFINEMENT EMPTY SCHEMA DUMPING THREAD] {error}\")\r\n",
                "\r\n",
                "def dump_empty_schemas(schema_name, \r\n",
                "                       s2r_path,\r\n",
                "                       ext_entity,\r\n",
                "                       transform_mode, \r\n",
                "                       items = []):\r\n",
                "    global districtId,schoolYear, metadata\r\n",
                "    if schema_name is None:\r\n",
                "        schema_name = 'ed-fi'\r\n",
                "    \r\n",
                "    with ThreadPoolExecutor(max_workers=8) as tpe:\r\n",
                "        logger.info('[REFINEMENT EMPTY SCHEMA DUMPING] Entered Threadpool')\r\n",
                "        tpe.map(threaded_task_empty_schema,[(item,schema_name if not item.lower().endswith('exts') else 'tx',s2r_path,ext_entity,metadata,transform_mode,districtId,schoolYear) for item in items])\r\n",
                "    \r\n",
                "    \r\n",
                "def threaded_task(input_tuple):\r\n",
                "    item,schema_name,tables_source,ext_entity,metadata,transform_mode,test_mode = input_tuple\r\n",
                "    \r\n",
                "    # print('inside thread')\r\n",
                "    table_name = item #sap_to_edfi_complex[item]\r\n",
                "    table_path = f\"{tables_source}/{item}\"\r\n",
                "    logger.info(f\"[REFINEMENT ETL TABLE THREAD] Processing schema/table: {schema_name}/{table_name}\")\r\n",
                "    if item == 'metadata.csv':\r\n",
                "        logger.info('ignore metadata processing, since this is not a table to be ingested')\r\n",
                "    else: \r\n",
                "        try:\r\n",
                "            if not(oea.path_exists(f\"stage2/Ingested/{table_path}\")):\r\n",
                "                pass\r\n",
                "            else:\r\n",
                "                if not(transform_mode):\r\n",
                "                    df = oea.refine(table_path, \r\n",
                "                                    metadata = metadata[item], \r\n",
                "                                    primary_key = 'id')\r\n",
                "                if transform_mode:\r\n",
                "                    logger.info('[REFINEMENT ETL TABLE THREAD] Ed-Fi to Ed-Fi Relationship Model: ' + table_name)               \r\n",
                "                    source_path = f'stage2/Ingested/{table_path}'\r\n",
                "                    sink_general_path, sink_sensitive_path = oea.get_sink_general_sensitive_paths(source_path)\r\n",
                "                    \r\n",
                "                    sink_general_path = edfiRefineAgent.sink_path_cleanup(sink_general_path)\r\n",
                "                    sink_sensitive_path = edfiRefineAgent.sink_path_cleanup(sink_sensitive_path)\r\n",
                "                    df_changes = oea.get_latest_changes(source_path, sink_general_path, filtering_date = 'rundate')\r\n",
                "\r\n",
                "                    df_changes = df_changes.withColumn('DistrictId', F.lit(districtId))\r\n",
                "                    \r\n",
                "                    # FIXME TO BE REVISED\r\n",
                "                    if item != 'schoolYearTypes':\r\n",
                "                        df_changes = df_changes.withColumn('SchoolYear', F.lit(schoolYear))\r\n",
                "                    else:\r\n",
                "                        # df_changes = df_changes.withColumnRenamed(\"schoolYear\", \"SchoolYear\")\r\n",
                "                        pass\r\n",
                "                    \r\n",
                "                    current_timestamp = datetime.now()\r\n",
                "                    df_changes = df_changes.withColumn('LastModifiedDate', F.lit(current_timestamp))\r\n",
                "                    \r\n",
                "                    if df_changes.count() > 0:\r\n",
                "                        upsert_data(df_changes, \r\n",
                "                                    metadata,\r\n",
                "                                    schema_name, \r\n",
                "                                    transform_mode,\r\n",
                "                                    table_name,\r\n",
                "                                    'id_pseudonym',\r\n",
                "                                    ext_entity,\r\n",
                "                                    sink_general_path,\r\n",
                "                                    sink_sensitive_path)\r\n",
                "                    else:\r\n",
                "                        logger.info(f'[REFINEMENT ETL TABLE THREAD] No updated rows in {source_path} to process.')\r\n",
                "\r\n",
                "        except AnalysisException as e:\r\n",
                "            logger.info(F\"[REFINEMENT ETL TABLE THREAD] {e}\")\r\n",
                "        except Exception as e:\r\n",
                "            logger.info(F\"[REFINEMENT ETL TABLE THREAD] {e}\")\r\n",
                "\r\n",
                "def refine_and_explode_data(schema_name, \r\n",
                "                            tables_source,\r\n",
                "                            ext_entity,\r\n",
                "                            metadata, \r\n",
                "                            transform_mode, \r\n",
                "                            test_mode,\r\n",
                "                            items = []):\r\n",
                "    global districtId,schoolYear\r\n",
                "    if items == 'All':\r\n",
                "        items = oea.get_folders(f\"stage2/Ingested/{tables_source}\")\r\n",
                "        items.append('schoolYearTypes')\r\n",
                "    #items = ['accountCodes', 'accounts', 'grades', 'students', 'staffs']\r\n",
                "    with ThreadPoolExecutor(max_workers=8) as tpe:\r\n",
                "        logger.info('[REFINEMENT ETL TABLES] Entered Threadpool')\r\n",
                "        tpe.map(threaded_task,[(item,schema_name,tables_source,ext_entity,metadata,transform_mode,test_mode) for item in items])\r\n",
                "            \r\n",
                "\r\n",
                "def get_non_ext_entities(entities_meta_info):\r\n",
                "    non_ext_table_names = list()\r\n",
                "    for entity_meta_info in entities_meta_info:\r\n",
                "        non_ext_table_names.append(entity_meta_info['resource'].split('/')[-1])\r\n",
                "    return non_ext_table_names\r\n",
                "\r\n",
                "def add_all_empty_tables_to_lake_db(empty_tables_path, schema_name, emptyTables = None):\r\n",
                "    if emptyTables is None:\r\n",
                "        empty_tables_source = oea.to_url(empty_tables_path)\r\n",
                "        items = oea.get_folders(empty_tables_source)\r\n",
                "    else:\r\n",
                "        items = emptyTables\r\n",
                "    if schema_name == 'ed-fi':\r\n",
                "        extension = None\r\n",
                "    else:\r\n",
                "        extension = schema_name \r\n",
                "\r\n",
                "    with ThreadPoolExecutor(max_workers=8) as tpe:\r\n",
                "        logger.info('[REFINEMENT EMPTY SCHEMA ADD TO LAKE DB] Entered Threadpool')\r\n",
                "        for item in items:\r\n",
                "            source_entity_path = empty_tables_path + '/' + item \r\n",
                "            tpe.submit(add_empty_table_to_lake_db,source_entity_path,False,extension)\r\n",
                "     \r\n",
                "    # for item in items:\r\n",
                "    #     source_entity_path = empty_tables_path + '/' + item \r\n",
                "    #     add_empty_table_to_lake_db(source_entity_path, \r\n",
                "    #                               overwrite = False, \r\n",
                "    #                               extension = extension)\r\n",
                "\r\n",
                "def add_empty_table_to_lake_db(source_entity_path, overwrite = False, extension = None):\r\n",
                "        # FIXME: Temporary Fix for Empty Schemas\r\n",
                "        \"\"\" Adds the given entity as a table (if the table doesn't already exist) to the proper lake db based on the path.\r\n",
                "            This method will also create the lake db if it doesn't already exist.\r\n",
                "            eg: add_to_lake_db('stage2/Ingested/contoso_sis/v0.2/students')\r\n",
                "\r\n",
                "            Note that a spark db that points to source data in the delta format can't be queried via SQL serverless pool. More info here: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#delta-lake\r\n",
                "        \"\"\"\r\n",
                "        source_dict = oea.parse_path(source_entity_path)\r\n",
                "        if '/emptySchemas/' in source_entity_path:\r\n",
                "            try:\r\n",
                "                base_db_name = source_dict['ldb_name']\r\n",
                "                base_table_name = source_dict['entity']\r\n",
                "                for submission_type in ['']:     \r\n",
                "                    if extension is not None:\r\n",
                "                        if not(extension.startswith('_')):\r\n",
                "                            extension = '_' + extension\r\n",
                "                        source_dict['entity'] = base_table_name + str(extension)\r\n",
                "                    \r\n",
                "                    db_name = base_db_name + submission_type\r\n",
                "\r\n",
                "                    logger.info(f\"[REFINEMENT EMPTY SCHEMA ADD TO LAKE DB] Adding: Lake DB: {db_name}; Table: {source_dict['entity']}\")\r\n",
                "                    spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\r\n",
                "                    if overwrite:\r\n",
                "                        spark.sql(f\"drop table if exists {db_name}.{source_dict['entity']}\")\r\n",
                "\r\n",
                "                    spark.sql(f\"create table if not exists {db_name}.{source_dict['entity']} using DELTA location '{oea.to_url(source_dict['entity_path'])}'\")\r\n",
                "            except Exception as error:\r\n",
                "                logger.error(f'[REFINEMENT EMPTY SCHEMA ADD TO LAKE DB] {error}')"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Main Code"
            ],
            "outputs": []
        },
        {
            "execution_count": 27,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "edfiRefineAgent = EdFiRefine(workspace = workspace, \r\n",
                "                             oea = oea, \r\n",
                "                             spark = spark,\r\n",
                "                             schema_gen = schema_gen,\r\n",
                "                             moduleName = moduleName, \r\n",
                "                             authUrl = authUrl,\r\n",
                "                             swaggerUrl = swaggerUrl, \r\n",
                "                             dataManagementUrl = dataManagementUrl, \r\n",
                "                             changeQueriesUrl = changeQueriesUrl, \r\n",
                "                             dependenciesUrl = dependenciesUrl, \r\n",
                "                             apiVersion = apiVersion, \r\n",
                "                             schoolYear = schoolYear, \r\n",
                "                             districtId = districtId,\r\n",
                "                             pipelineExecutionId = pipelineExecutionId,\r\n",
                "                             error_logger = error_logger,\r\n",
                "                             test_mode = False)"
            ],
            "outputs": []
        },
        {
            "execution_count": 28,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from datetime import datetime\r\n",
                "import math\r\n",
                "source_path = f'stage1/Transactional/Ed-Fi/{apiVersion}/DistrictId={districtId}/SchoolYear={schoolYear}/metadata-assets/frequency_etl.csv'  \r\n",
                "destination_path = source_path #f'stage1/Transactional/Ed-Fi/{apiVersion}/DistrictId={districtId}/SchoolYear={schoolYear}/metadata-assets/frequency_based_etl.csv'  \r\n",
                "logs_path = f\"stage1/Transactional/Ed-Fi/{apiVersion}/DistrictId={districtId}/SchoolYear={schoolYear}/metadata-assets/_frequency_etl_logs/run_logs_{datetime.today().strftime('%Y-%m-%d')}.csv\"\r\n",
                "\r\n",
                "processor = EntityFrequencyProcessor(oea = oea, \r\n",
                "                                     filepath = source_path, \r\n",
                "                                     highFrequentDelta = highFrequentDelta,#0.005, \r\n",
                "                                     moderateFrequentDelta = moderateFrequentDelta, #5, \r\n",
                "                                     lowFrequentDelta = lowFrequentDelta, #10, \r\n",
                "                                     descriptorsDelta = descriptorsDelta) #360)\r\n",
                "\r\n",
                "processor.load_lookup_df()\r\n",
                "_, entities_to_etl = processor.return_entities_to_etl()\r\n",
                "\r\n",
                "edfiEntities = \"All\" #['schoolYearTypes']\r\n",
                "tpdmEntities = 'All'\r\n",
                "\r\n",
                "edfiEntities = entities_to_etl.get('ed-fi', [])\r\n",
                "tpdmEntities = entities_to_etl.get('tpdm', [])"
            ],
            "outputs": []
        },
        {
            "execution_count": 29,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from datetime import datetime\r\n",
                "schema_name = 'ed-fi'\r\n",
                "ext_entity = 'TPDM'\r\n",
                "test_mode = False\r\n",
                "transform_mode = True\r\n",
                "tables_source = f'{moduleName}/{apiVersion}/DistrictId={districtId}/SchoolYear={schoolYear}/{schema_name}'\r\n",
                "transform_items = edfiEntities #\"All#['staffs', 'students']#\" #non_ext_table_names#edfiEntities \r\n",
                "\r\n",
                "# Create or overwrite Metadata.csv\r\n",
                "metadataPath = f'stage1/Transactional/Ed-Fi/{apiVersion}/DistrictId={districtId}/SchoolYear={schoolYear}/metadata-assets'\r\n",
                "metadata = oea.get_metadata_from_path(metadataPath) # metadata = oea.get_metadata_from_url(metadataUrl)"
            ],
            "outputs": []
        },
        {
            "execution_count": 30,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "if etlProcessing:\r\n",
                "    df = refine_and_explode_data(schema_name, \r\n",
                "                            tables_source,\r\n",
                "                            ext_entity,\r\n",
                "                            metadata,\r\n",
                "                            transform_mode, \r\n",
                "                            test_mode,\r\n",
                "                            transform_items)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Empty Schemas"
            ],
            "outputs": []
        },
        {
            "execution_count": 31,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from datetime import datetime\r\n",
                "transform_mode = True\r\n",
                "\r\n",
                "if prepareEdFiMetaData:\r\n",
                "    retry_strategy = Retry(total = 3,\r\n",
                "                       backoff_factor = 1,\r\n",
                "                       status_forcelist = [429, 500, 502, 503, 504],\r\n",
                "                       allowed_methods = [\"HEAD\", \"GET\", \"OPTIONS\", \"POST\", \"DELETE\"])\r\n",
                "\r\n",
                "    edfiAPIClient = EdFiClient(workspace = workspace, \r\n",
                "                                    kvName = kvName, #NOTE: Default to None \r\n",
                "                                    moduleName = moduleName, \r\n",
                "                                    authUrl = authUrl, \r\n",
                "                                    dataManagementUrl = dataManagementUrl, \r\n",
                "                                    changeQueriesUrl = changeQueriesUrl, \r\n",
                "                                    dependenciesUrl = dependenciesUrl, \r\n",
                "                                    apiVersion = apiVersion, \r\n",
                "                                    batchLimit = batchLimit, \r\n",
                "                                    minChangeVer = minChangeVer, \r\n",
                "                                    maxChangeVer = maxChangeVer,\r\n",
                "                                    schoolYear = schoolYear,\r\n",
                "                                    districtId = districtId,\r\n",
                "                                    kvSecret_clientId = client_id,\r\n",
                "                                    kvSecret_clientSecret = client_secret_id,\r\n",
                "                                    retry_strategy = retry_strategy,\r\n",
                "                                    threadMode = True,\r\n",
                "                                    devMode = True)\r\n",
                "\r\n",
                "    entities_meta_info = edfiAPIClient.getEntities()#[0]['resource']\r\n",
                "    non_ext_table_names = get_non_ext_entities(entities_meta_info) #TODO: To Be Reviewed\r\n",
                "    non_ext_table_names = ['schoolYearTypes'] + non_ext_table_names\r\n",
                "\r\n",
                "    for swagger_resource_type in ['Resources', 'Descriptors']:\r\n",
                "        swagger_url = swaggerUrl = edfi_api_manager.get_referenced_url(swagger_resource_type)\r\n",
                "        oea_utils = schema_gen = OpenAPIUtil(swagger_url)\r\n",
                "        oea_utils.create_definitions()\r\n",
                "        schemas = schema_gen.create_spark_schemas()\r\n",
                "\r\n",
                "        \r\n",
                "        edfiRefineAgent = EdFiRefine(workspace = workspace, \r\n",
                "                             oea = oea, \r\n",
                "                             spark = spark,\r\n",
                "                             schema_gen = schema_gen,\r\n",
                "                             moduleName = moduleName, \r\n",
                "                             authUrl = authUrl,\r\n",
                "                             swaggerUrl = swaggerUrl, \r\n",
                "                             dataManagementUrl = dataManagementUrl, \r\n",
                "                             changeQueriesUrl = changeQueriesUrl, \r\n",
                "                             dependenciesUrl = dependenciesUrl, \r\n",
                "                             apiVersion = apiVersion, \r\n",
                "                             schoolYear = schoolYear, \r\n",
                "                             districtId = districtId,\r\n",
                "                             pipelineExecutionId = pipelineExecutionId,\r\n",
                "                             error_logger = error_logger,\r\n",
                "                             test_mode = False)\r\n",
                "\r\n",
                "        # non_ext_table_names = sap_to_edfi_client.return_non_ext_tables()  \r\n",
                "        if swagger_resource_type == 'Resources':\r\n",
                "            transform_items = [item for item in non_ext_table_names if not(item.lower().endswith('descriptors'))]\r\n",
                "        elif swagger_resource_type == 'Descriptors':\r\n",
                "            transform_items = [item for item in non_ext_table_names if item.lower().endswith('descriptors')]\r\n",
                "      \r\n",
                "        s2r_path = f'stage2/Refined/Ed-Fi/{apiVersion}/emptySchemas'\r\n",
                "\r\n",
                "        dump_empty_schemas(schema_name = schema_name , \r\n",
                "                         s2r_path = s2r_path,\r\n",
                "                         ext_entity = ext_entity,\r\n",
                "                         transform_mode = transform_mode, \r\n",
                "                         items = transform_items)"
            ],
            "outputs": []
        },
        {
            "execution_count": 32,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "if prepareEdFiMetadata:\r\n",
                "    tables_source = f'Ed-Fi/{apiVersion}/ed-fi'\r\n",
                "    mainTables = [item for item in oea.get_folders(f\"stage2/Refined/{tables_source}/general\") if item != 'descriptorTables']\r\n",
                "\r\n",
                "    tables_source = f'Ed-Fi/{apiVersion}/{ext_entity.lower()}'\r\n",
                "    extTables = [item for item in oea.get_folders(f\"stage2/Refined/{tables_source}/general\") if item != 'descriptorTables']\r\n",
                "    if extTables != []:\r\n",
                "        mainTables = mainTables + extTables\r\n",
                "    edfi_emptyTables = oea.get_folders(f'stage2/Refined/Ed-Fi/{apiVersion}/emptySchemas/general/ed-fi')\r\n",
                "    edfi_emptyTables = edfiRefineAgent.non_empty_elements(edfi_emptyTables, \r\n",
                "                                                             mainTables)\r\n",
                "    ext_emptyTables = oea.get_folders(f'stage2/Refined/Ed-Fi/{apiVersion}/emptySchemas/general/{ext_entity.lower()}')\r\n",
                "    ext_emptyTables = edfiRefineAgent.non_empty_elements(ext_emptyTables, \r\n",
                "                                                             mainTables)\r\n",
                "\r\n",
                "    emptyTables_path = f'stage2/Refined/Ed-Fi/{apiVersion}/emptySchemas/general/ed-fi'\r\n",
                "    if edfi_emptyTables != list():\r\n",
                "        add_all_empty_tables_to_lake_db(emptyTables_path, 'ed-fi', edfi_emptyTables)\r\n",
                "\r\n",
                "    emptyTables_path = f'stage2/Refined/Ed-Fi/{apiVersion}/emptySchemas/general/{ext_entity.lower()}'\r\n",
                "    if ext_emptyTables != list():\r\n",
                "        add_all_empty_tables_to_lake_db(emptyTables_path, 'tpdm', ext_emptyTables)"
            ],
            "outputs": []
        },
        {
            "execution_count": 482,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "if error_logger.entity_logs != []:\r\n",
                "    logger.info('[REFINEMENT ERROR LOGGING] Writing Entity Level Error Logs')\r\n",
                "    df = error_logger.create_spark_df('entity')\r\n",
                "    error_logger.write_logs_to_delta_lake(df = df, \r\n",
                "                                log_type = 'entity',\r\n",
                "                                destination_url = error_logger.to_logs_url('etl-logs/log_type=entity'))\r\n",
                "    error_logger.add_etl_logs_to_lake_db(db_name = f'ldb_{workspace}_edfi_etl_logs',\r\n",
                "                                        logs_base_path = 'etl-logs',\r\n",
                "                                        log_type = 'entity',\r\n",
                "                                        overwrite = True)"
            ],
            "outputs": []
        }
    ]
}