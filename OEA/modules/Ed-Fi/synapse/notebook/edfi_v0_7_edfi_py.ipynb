{
    "nbformat": 4,
    "nbformat_minor": 2,
    "metadata": {
        "save_output": true,
        "kernelspec": {
            "name": "synapse_pyspark",
            "display_name": "python"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "import requests\r\n",
                "import json\r\n",
                "import uuid\r\n",
                "from requests.auth import HTTPBasicAuth\r\n",
                "import logging\r\n",
                "import csv\r\n",
                "import pandas as pd\r\n",
                "from io import StringIO\r\n",
                "from pyspark.sql.window import Window as W\r\n",
                "\r\n",
                "from datetime import datetime, timedelta\r\n",
                "from notebookutils import mssparkutils\r\n",
                "import threading\r\n",
                "from requests.adapters import HTTPAdapter\r\n",
                "from requests.packages.urllib3.util.retry import Retry\r\n",
                "import concurrent.futures\r\n",
                "from itertools import repeat\r\n",
                "import time\r\n",
                "from pyspark.sql import Window\r\n",
                "from pyspark.sql.functions import row_number, desc\r\n",
                "\r\n",
                "logger = logging.getLogger('EdFiClient')"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "%run OEA/modules/Ed-Fi/v0.7/src/utilities/edfi_v0_7_oea_py"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### EdFi Extended Module of OEA"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "class EdFiOEAChild(OEA):\r\n",
                "    \"\"\" \r\n",
                "    NOTE: This class inherits features from the base class OEA and therefore,\r\n",
                "    should be created / executed after running the notebook OEA_py\r\n",
                "    \"\"\"\r\n",
                "    def __init__(self, workspace='dev', logging_level=logging.INFO, storage_account=None, keyvault=None, timezone=None):\r\n",
                "        # Call the base class constructor to initialize inherited attributes\r\n",
                "        super().__init__(workspace, logging_level, storage_account, keyvault, timezone)\r\n",
                "        self.ingestionHistoryMode = False\r\n",
                "        self.surrogateKeyMode = False\r\n",
                "    \r\n",
                "    def get_latest_changes(self, source_path, sink_path, filtering_date = 'LastModifiedDate',primary_key = ['id'],debugMode = False):\r\n",
                "        \"\"\" Returns a dataframe representing the changes in the source data based on the max rundate in the sink data. \r\n",
                "            If the sink path is not found, all of the data from the source_path is returned (the assumption is that the sink delta table is being created for the first time).\r\n",
                "            eg, get_latest_changes('stage2/Ingested/contoso/v0.1/students', 'stage2/Refined/contoso/v0.1/students')\r\n",
                "        \"\"\"   \r\n",
                "        maxdatetime = None\r\n",
                "        try:\r\n",
                "            sink_df = self.query(sink_path, f'select max({filtering_date}) maxdatetime')\r\n",
                "            maxdatetime = sink_df.first()['maxdatetime']\r\n",
                "        except AnalysisException as e:\r\n",
                "            # This means that there is no delta table at the sink_path yet.\r\n",
                "            # We'll assume that the sink delta table is being created for the first time, meaning that all of the source data should be returned.\r\n",
                "            pass\r\n",
                "\r\n",
                "        changes_df = self.load(source_path)\r\n",
                "        if maxdatetime and not(debugMode):\r\n",
                "            # filter the source table for the latest changes (using the max rundate in the destination table as the watermark)\r\n",
                "            changes_df = changes_df.where(f\"{filtering_date} > '{maxdatetime}'\")        \r\n",
                "        \r\n",
                "        if self.ingestionHistoryMode:\r\n",
                "            table_name = source_path.split('/')[-1]\r\n",
                "            # logger.info(f\"{table_name}: Before De-Duplication: {changes_df.count()}\")\r\n",
                "            changes_df = self.get_deduplicated_records_by_datetime(df = changes_df,\r\n",
                "                                                                    primary_key = primary_key,\r\n",
                "                                                                    date_col = filtering_date)\r\n",
                "            # logger.info(f\"{table_name}: After De-Duplication: {changes_df.count()}\")\r\n",
                "        return changes_df\r\n",
                "    \r\n",
                "    def get_deduplicated_records_by_datetime(self, df, primary_key = ['lakeId'], date_col = 'rundate'):\r\n",
                "        window = Window.partitionBy(*primary_key).orderBy(desc(date_col))\r\n",
                "        df = df.withColumn('row', f.row_number().over(window))\r\n",
                "        df = df.filter(f.col('row') == 1)\r\n",
                "        df = df.drop('row')\r\n",
                "        return df\r\n",
                "\r\n",
                "    def process(self, source_path,foreach_batch_function, batch_type, natural_key = None,landingDateTimeFormat = 'yyyyMMddHHmmss',options={}):\r\n",
                "        # FIXME: 2024-02-08 (Under Dev for high granularity and de-dup)\r\n",
                "        \"\"\" This simplifies the process of using structured streaming when processing transformations.\r\n",
                "            Provide a source_path and a function that receives a dataframe to work with (which will be a dataframe with data from the given source_path).\r\n",
                "            Use it like this...\r\n",
                "            def refine_contoso_dataset(df_source):\r\n",
                "                metadata = oea.get_metadata_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/gene/v0.7dev/modules/module_catalog/Student_and_School_Data_Systems/metadata.csv')\r\n",
                "                df_pseudo, df_lookup = oea.pseudonymize(df, metadata['studentattendance'])\r\n",
                "                oea.upsert(df_pseudo, 'stage2/Refined/contoso_sis/v0.1/studentattendance/general')\r\n",
                "                oea.upsert(df_lookup, 'stage2/Refined/contoso_sis/v0.1/studentattendance/sensitive')\r\n",
                "            oea.process('stage2/Ingested/contoso_sis/v0.1/studentattendance', refine_contoso_dataset)             \r\n",
                "        \"\"\"\r\n",
                "        if not self.path_exists(source_path):\r\n",
                "            raise ValueError(f'The given path does not exist: {source_path} (which resolves to: {self.to_url(source_path)})') \r\n",
                "\r\n",
                "        if natural_key is not None:\r\n",
                "            natural_key_expr = [f.col(key_component).cast('string') for key_component in natural_key]\r\n",
                "        \r\n",
                "        def wrapped_function(df, batch_id):\r\n",
                "            current_timestamp = datetime.now()\r\n",
                "            df = df.withColumn('LastModifiedDate', F.lit(current_timestamp))\r\n",
                "            df = df.withColumn(\"rundate\", F.to_timestamp(F.col(\"rundate\").cast('string'), landingDateTimeFormat))\r\n",
                "            # df = df.orderBy(F.col(\"rundate\").desc()).dropDuplicates([\"id\"])\r\n",
                "            if natural_key is not None:\r\n",
                "                df = df.withColumn(\"NATURAL_KEY_HASH\",F.sha2(F.concat(*[F.concat(F.coalesce(column, F.lit('')), F.lit('_')) for column in natural_key_expr]), 256))\r\n",
                "            if batch_type != 'delete':\r\n",
                "                df = df.withColumn(\"rowIsActive\", F.lit(True))\r\n",
                "            \r\n",
                "            df.persist() # cache the df so it doesn't get read in multiple times when we write to multiple destinations. See: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#foreachbatch\r\n",
                "            foreach_batch_function(df, batch_id)\r\n",
                "            df.unpersist()\r\n",
                "\r\n",
                "        spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
                "        #source_path = source_path.replace(':', '\\:')\r\n",
                "        print(f\"source_path is: {source_path}\")\r\n",
                "        streaming_df = spark.readStream.load(self.to_url(source_path), **options)\r\n",
                "        streaming_df = streaming_df.withColumn('stage1_source_url', F.input_file_name())\r\n",
                "\r\n",
                "        # for more info on append vs complete vs update modes for structured streaming: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#basic-concepts\r\n",
                "        query = streaming_df.writeStream.format('delta').outputMode('append').trigger(once=True).option('checkpointLocation', self.to_url(source_path) + '/_checkpoints').foreachBatch(wrapped_function).start()\r\n",
                "        query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
                "        number_of_new_inbound_rows = query.lastProgress[\"numInputRows\"]\r\n",
                "        logger.info(f'[EDFIOEACHILD INGESTION STRUCTURED STREAMING PROCESS]: Number of new inbound rows processed: {number_of_new_inbound_rows}')\r\n",
                "        logger.debug(query.lastProgress)\r\n",
                "        return number_of_new_inbound_rows\r\n",
                "    \r\n",
                "    def return_pk_statement(self, pk_columns):\r\n",
                "        pk_statement = \"\"\r\n",
                "        for i, column in enumerate(pk_columns):\r\n",
                "            pk_statement += f\"sink.{column} = updates.{column}\"\r\n",
                "            if i < len(pk_columns) - 1:\r\n",
                "                pk_statement += \" AND \"\r\n",
                "        \r\n",
                "        return pk_statement\r\n",
                "\r\n",
                "    def return_upsert_cols(self,\r\n",
                "                       columns, \r\n",
                "                       partitioning_cols, \r\n",
                "                       primary_key,\r\n",
                "                       upsert_type,\r\n",
                "                       skey = None):\r\n",
                "        if type(primary_key) == list:\r\n",
                "            pass\r\n",
                "        else:\r\n",
                "            primary_key = [primary_key]\r\n",
                "        \r\n",
                "        if upsert_type == 'update':\r\n",
                "            if skey is not None:\r\n",
                "                if type(skey) == list:\r\n",
                "                    pass\r\n",
                "                else:\r\n",
                "                    skey = [skey]\r\n",
                "                \r\n",
                "                iter_columns = list(set(columns) - set(partitioning_cols) - set(primary_key) - set(skey))\r\n",
                "            else:\r\n",
                "                iter_columns = list(set(columns) - set(partitioning_cols) - set(primary_key))\r\n",
                "            update_cols = dict()\r\n",
                "            for column in iter_columns:\r\n",
                "                update_cols[f\"sink.{column}\"] = f\"updates.{column}\"\r\n",
                "            return update_cols\r\n",
                "        \r\n",
                "        elif upsert_type == 'insert':\r\n",
                "            iter_columns = list(set(columns) - set(partitioning_cols))\r\n",
                "            insert_cols = dict()\r\n",
                "            for column in iter_columns:\r\n",
                "                insert_cols[f\"sink.{column}\"] = f\"updates.{column}\"\r\n",
                "            return insert_cols              \r\n",
                "\r\n",
                "    def return_upsert_metrics(self, numOutputRows, numTargetRowsInserted, numTargetRowsUpdated, **kwargs):\r\n",
                "        numInputRows = kwargs['df'].count()\r\n",
                "        if not(numOutputRows is None and numTargetRowsInserted is None and numTargetRowsUpdated is None):\r\n",
                "            # NOTE: For subsequent calls to upsert (not initial write)\r\n",
                "            first_row = kwargs['delta_table_sink'].history(1).select(\r\n",
                "                'operationMetrics.numOutputRows', \r\n",
                "                'operationMetrics.numTargetRowsInserted', \r\n",
                "                'operationMetrics.numTargetRowsUpdated'\r\n",
                "            ).first()\r\n",
                "\r\n",
                "            numOutputRows = int(first_row[0]) if first_row[0] is not None else 0\r\n",
                "            numTargetRowsInserted = int(first_row[1]) if first_row[1] is not None else 0\r\n",
                "            numTargetRowsUpdated = int(first_row[2]) if first_row[2] is not None else 0\r\n",
                "        else:\r\n",
                "            # NOTE: For First Run \r\n",
                "            numOutputRows = int(numOutputRows) if numOutputRows is not None else numInputRows\r\n",
                "            numTargetRowsInserted = int(numTargetRowsInserted) if numTargetRowsInserted is not None else numInputRows\r\n",
                "            numTargetRowsUpdated = int(numTargetRowsUpdated) if numTargetRowsUpdated is not None else 0\r\n",
                "\r\n",
                "        return numInputRows, numOutputRows, numTargetRowsInserted, numTargetRowsUpdated\r\n",
                "\r\n",
                "    def are_partition_cols_pure(self, df, partitioning_cols):\r\n",
                "        purity_indicator = 1\r\n",
                "        if df.count() == 0:\r\n",
                "            return purity_indicator == 1\r\n",
                "        \r\n",
                "        for partitioning_col in partitioning_cols:\r\n",
                "            distinct_count = df.select(partitioning_col).distinct().count()\r\n",
                "            purity_indicator *= int(distinct_count == 1)\r\n",
                "        return purity_indicator == 1\r\n",
                "    \r\n",
                "    def construct_partitioned_url(self, incomingPartitionsCache, partitioning_cols, destination_path):\r\n",
                "        # FIXME: 2024-02-07 Under Refactoring\r\n",
                "        partitioning_dict = dict()\r\n",
                "        for partitioning_col in partitioning_cols:\r\n",
                "            partitioning_dict[partitioning_col] = incomingPartitionsCache[partitioning_col]\r\n",
                "        destination_partition_path = destination_path\r\n",
                "        for component_key, component_value in partitioning_dict.items():\r\n",
                "            destination_partition_path = f\"{destination_path}/{component_key}={component_value}\"\r\n",
                "        return self.to_url(destination_partition_path)\r\n",
                "\r\n",
                "    def get_df_deDuplicated_records(self, df, partitioning, primary_key, partitioning_cols):\r\n",
                "        if partitioning and type(primary_key) != list:\r\n",
                "            df = df.dropDuplicates([primary_key] + partitioning_cols)\r\n",
                "        elif partitioning and type(primary_key) == list:\r\n",
                "            df = df.dropDuplicates(primary_key + partitioning_cols)\r\n",
                "        elif not(partitioning) and type(primary_key) != list:\r\n",
                "            df = df.dropDuplicates([primary_key])\r\n",
                "        elif not(partitioning) and type(primary_key) == list:\r\n",
                "            df = df.dropDuplicates(primary_key)\r\n",
                "        else:\r\n",
                "            df = df\r\n",
                "        return df\r\n",
                "\r\n",
                "    def get_df_latest_records_by_join(self, df, destination_path, primary_key = 'NATURAL_KEY_HASH',func_enabled = False):\r\n",
                "        if not func_enabled:\r\n",
                "            return df\r\n",
                "        else:\r\n",
                "            logger.info('[EDFIOEACHILD REFINEMENT RECORD HASHING] JOIN BASED COMPARSIONS / DELTA COMPARISONS BEFORE UPSERT IS ENABLED')\r\n",
                "            df = df.withColumnRenamed('RECORD_VERSION', 'RECORD_VERSION_LEFT')\r\n",
                "            \r\n",
                "            df_destination = self.load(destination_path)            \r\n",
                "            df.createOrReplaceTempView('temp_vw_df_source_table')\r\n",
                "            df_destination.createOrReplaceTempView('temp_vw_df_destination_table')\r\n",
                "\r\n",
                "            if type(primary_key) == str:\r\n",
                "                query = f\"\"\"SELECT temp_vw_df_source_table.*, \r\n",
                "                                temp_vw_df_destination_table.RECORD_VERSION\r\n",
                "                            FROM temp_vw_df_source_table \r\n",
                "                            LEFT JOIN temp_vw_df_destination_table \r\n",
                "                                ON temp_vw_df_source_table.{primary_key} = temp_vw_df_destination_table.{primary_key}\r\n",
                "                            WHERE (temp_vw_df_source_table.RECORD_HASH != temp_vw_df_destination_table.RECORD_HASH)\r\n",
                "                            OR (temp_vw_df_destination_table.RECORD_HASH IS NULL)\r\n",
                "                        \"\"\"\r\n",
                "            else:\r\n",
                "                # FIXME: 2024-02-22 To Be Dev\r\n",
                "                logger.info(f\"[EDFIOEACHILD REFINEMENT RECORD HASHING] Module does not support list as of now\")\r\n",
                "\r\n",
                "            df_joined = spark.sql(query)\r\n",
                "            df_joined = df_joined.withColumn('RECORD_VERSION', F.col('RECORD_VERSION') + 1)\r\n",
                "            df_joined = df_joined.drop('RECORD_VERSION_LEFT')\r\n",
                "\r\n",
                "            logger.info(f\"[EDFIOEACHILD REFINEMENT RECORD HASHING] --- NUM ROWS (SOURCE DELTA LAKE) - {df.count()}\")\r\n",
                "            logger.info(f\"[EDFIOEACHILD REFINEMENT RECORD HASHING] --- NUM ROWS (DESTINATION DELTA LAKE) - {df_destination.count()}\")\r\n",
                "            logger.info(f'[EDFIOEACHILD REFINEMENT RECORD HASHING] --- NUM ROWS (ACTUALLY MODIFIED) - {df_joined.count()}')\r\n",
                "            return df_joined\r\n",
                "    \r\n",
                "    def add_and_set_record_version1(self, df, etl_table):\r\n",
                "        # FIXME: 2024-01-25: Temporary fix - Need to be refactor\r\n",
                "        if etl_table:\r\n",
                "            df = df.withColumn('RECORD_VERSION', F.lit(1)) # FIXME: 2024-01-25: Under Review \r\n",
                "        return df\r\n",
                "    \r\n",
                "    def add_surrogate_key_to_df(self, df, primary_key, surrogate_key = False,func_enabled = False):\r\n",
                "        skey = None\r\n",
                "        if func_enabled:\r\n",
                "            if type(primary_key) == list:\r\n",
                "                pk_statement = self.return_pk_statement(primary_key)\r\n",
                "                skey = list()\r\n",
                "                for pk_component in primary_key:\r\n",
                "                    sk_component = pk_component[:-4] + 'SKey'\r\n",
                "                    skey.append(sk_component)\r\n",
                "            else:\r\n",
                "                skey = primary_key[:-4] + 'SKey'\r\n",
                "                if 'hkey' in primary_key.lower():\r\n",
                "                    surrogate_key = True\r\n",
                "                pk_statement = self.return_pk_statement([primary_key])\r\n",
                "            \r\n",
                "            if surrogate_key:\r\n",
                "                if type(primary_key) == list:\r\n",
                "                    for index, pk_component in enumerate(primary_key):\r\n",
                "                        sk_component = skey[index]\r\n",
                "                        df = df.withColumn('row_id_label', (F.monotonically_increasing_id()))\r\n",
                "                        windowSpec = W.orderBy(\"row_id_label\")\r\n",
                "                        df = df.withColumn(\"row_id_label\", F.row_number().over(windowSpec))\r\n",
                "                        \r\n",
                "                        df = df.withColumn(sk_component, F.when((F.col(pk_component).isNull()) | (F.col(sk_component) == -1), -1).otherwise(F.col('row_id_label')))\r\n",
                "                        df = df.drop('row_id_label')\r\n",
                "                else:\r\n",
                "                    df = df.withColumn('row_id_label', (F.monotonically_increasing_id()))\r\n",
                "                    windowSpec = W.orderBy(\"row_id_label\")\r\n",
                "                    df = df.withColumn(\"row_id_label\", F.row_number().over(windowSpec))\r\n",
                "                    \r\n",
                "                    df = df.withColumn(skey, F.when((F.col(primary_key).isNull()) | (F.col(skey) == -1), -1).otherwise(F.col('row_id_label')))\r\n",
                "                    df = df.drop('row_id_label')\r\n",
                "        else:\r\n",
                "            pass\r\n",
                "        return df, skey, surrogate_key\r\n",
                "\r\n",
                "    def upsert(self, \r\n",
                "               df, \r\n",
                "               destination_path, \r\n",
                "               primary_key='id', \r\n",
                "               partitioning=False, \r\n",
                "               partitioning_cols = [], \r\n",
                "               surrogate_key = False,\r\n",
                "               join_based_upsert = False, \r\n",
                "               de_duplicate = True):\r\n",
                "        # FIXME: Re-check Skey logic when maxSkey is None (not Int)\r\n",
                "        # FIXME: Current Implementation may not work when partitioned df has mix of partititions\r\n",
                "        \"\"\" Upserts the data in the given dataframe into the specified destination using the given primary_key_column to identify the updates.\r\n",
                "            If there is no delta table found in the destination_path, one will be created.    \r\n",
                "        \"\"\"\r\n",
                "        overwrite = True\r\n",
                "        if partitioning:\r\n",
                "            partitioning_cols = partitioning_cols\r\n",
                "        else:\r\n",
                "            partitioning_cols = []\r\n",
                "        \r\n",
                "        if not(self.are_partition_cols_pure(df, partitioning_cols)):\r\n",
                "            raise Exception(\"[EDFIOEACHILD UPSERT] Partition column with non-unique values is present - current implementation does not support this\")\r\n",
                "    \r\n",
                "        skey = None\r\n",
                "        destination_url = self.to_url(destination_path)\r\n",
                "        df = self.fix_column_names(df)\r\n",
                "        if type(primary_key) == list:\r\n",
                "            pk_statement = self.return_pk_statement(primary_key)\r\n",
                "        else:\r\n",
                "            pk_statement = self.return_pk_statement([primary_key])\r\n",
                "        if surrogate_key:\r\n",
                "            df, skey, surrogate_key = self.add_surrogate_key_to_df(df = df, \r\n",
                "                                                                   primary_key = primary_key, \r\n",
                "                                                                   surrogate_key = surrogate_key,\r\n",
                "                                                                   func_enabled = self.surrogateKeyMode)\r\n",
                "        else:\r\n",
                "            # NOTE: Do not De-Duplicate when surrogate key is present\r\n",
                "            if de_duplicate:\r\n",
                "                df = self.get_df_deDuplicated_records(df, partitioning, primary_key, partitioning_cols)\r\n",
                "\r\n",
                "        if DeltaTable.isDeltaTable(spark, destination_url) and df.count() <= 0:\r\n",
                "            logger.info(\"[EDFIOEACHILD UPSERT] No Ingress Records\")\r\n",
                "            numInputRows = numOutputRows = numTargetRowsInserted = numTargetRowsUpdated = 0\r\n",
                "        #FIX -2024-01-31 - 2024-03-07 Overwrite implementation\r\n",
                "        #elif DeltaTable.isDeltaTable(spark, destination_url) and df.count() > 0:\r\n",
                "        \r\n",
                "        elif DeltaTable.isDeltaTable(spark, destination_url) and not overwrite:\r\n",
                "            delta_table_sink = DeltaTable.forPath(spark, destination_url)\r\n",
                "            if surrogate_key:\r\n",
                "                if type(primary_key) == list:\r\n",
                "                    for index, pk_component in enumerate(primary_key):\r\n",
                "                        sk_component = skey[index]\r\n",
                "                        sink_df = self.query(destination_path, f'select max({sk_component}) max_skey')\r\n",
                "                        max_skey = int(sink_df.first()['max_skey'])\r\n",
                "                        # df = df.withColumn(skey, F.col(skey) + max_skey)\r\n",
                "                        df = df.withColumn(sk_component, F.when(F.col(sk_component) == -1, F.col(sk_component)).otherwise(F.col(sk_component) + max_skey))\r\n",
                "                else:\r\n",
                "                    sink_df = self.query(destination_path, f'select max({skey}) max_skey')\r\n",
                "                    max_skey = int(sink_df.first()['max_skey'])\r\n",
                "                    # df = df.withColumn(skey, F.col(skey) + max_skey)\r\n",
                "                    df = df.withColumn(skey, F.when(F.col(skey) == -1, F.col(skey)).otherwise(F.col(skey) + max_skey))\r\n",
                "            \r\n",
                "            # FIXME: 2024-02-07 Under Dev (Renewed Logic)\r\n",
                "            incomingPartitionsCache = df.select(*partitioning_cols).distinct().first()\r\n",
                "            destination_partition_url = self.construct_partitioned_url(incomingPartitionsCache,partitioning_cols, destination_path)\r\n",
                "            if DeltaTable.isDeltaTable(spark, destination_partition_url):\r\n",
                "                # NOTE: MERGE into a partition if it already exists\r\n",
                "                logger.info('[EDFIOEACHILD UPSERT] Upsert by Partitions + PK Cols')\r\n",
                "                if surrogate_key:\r\n",
                "                    update_cols = self.return_upsert_cols(df.columns, partitioning_cols, primary_key, 'update', skey)\r\n",
                "                    insert_cols = self.return_upsert_cols(df.columns, partitioning_cols, primary_key, 'insert', None)\r\n",
                "                    delta_table_sink.alias('sink').merge(df.alias('updates'), pk_statement).whenMatchedUpdate(set = update_cols).whenNotMatchedInsert(values = insert_cols).execute()\r\n",
                "                else:\r\n",
                "                    logger.info('[EDFIOEACHILD UPSERT] TRUE UPSERT')\r\n",
                "                    df = self.get_df_latest_records_by_join(df, destination_path, primary_key = primary_key, func_enabled = join_based_upsert)\r\n",
                "                    if df.count() > 0: \r\n",
                "                        delta_table_sink.alias('sink').merge(df.alias('updates'), pk_statement).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()    \r\n",
                "            else:\r\n",
                "                # NOTE: Dynamically create a new partition if it does not exist\r\n",
                "                # FIXME: ELSE CONDITION RELEVANCE AND EFFECT TO BE REVIEWED\r\n",
                "                # FIXME: If partitioning_dict does not have a valid a dictionary; this may execute and corrupt data by overwriting partitions\r\n",
                "                logger.info('[EDFIOEACHILD UPSERT] Dynamically over-write the partition')\r\n",
                "                spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\r\n",
                "                \r\n",
                "                # df = self.add_and_set_record_version1(df, etl_table = etl_table)\r\n",
                "                df.write.format('delta').mode('overwrite').partitionBy(*partitioning_cols).save(destination_url)\r\n",
                "            numInputRows, numOutputRows, numTargetRowsInserted, numTargetRowsUpdated = self.return_upsert_metrics(numOutputRows = 0, \r\n",
                "                                                                                                                  numTargetRowsInserted = 0, \r\n",
                "                                                                                                                  numTargetRowsUpdated = 0, \r\n",
                "                                                                                                                  df = df,\r\n",
                "                                                                                                                  delta_table_sink = delta_table_sink)\r\n",
                "\r\n",
                "        elif overwrite:\r\n",
                "            logger.info('Overwriting existing delta table found')\r\n",
                "            if not(partitioning):\r\n",
                "                logger.info('Writing unpartitioned delta lake')\r\n",
                "                df.write.format('delta').mode('overwrite').save(destination_url)\r\n",
                "            else:\r\n",
                "                if partitioning and len(partitioning_cols) == 0:\r\n",
                "                    logger.info('Partitioning columns absent - defaulting to DistrictId and SchoolYear as partitioning columns')\r\n",
                "                    df.write.format('delta').mode('overwrite').partitionBy('DistrictId', 'SchoolYear').save(destination_url)\r\n",
                "                else:\r\n",
                "                    partitioning_str = ', '.join(partitioning_cols)\r\n",
                "                    logger.info(f'Writing partitioned delta lake - partitioned by - {partitioning_str}')\r\n",
                "                    df.write.format('delta').mode('overwrite').partitionBy(*partitioning_cols).save(destination_url)\r\n",
                "\r\n",
                "            first_row = [None, None, None]\r\n",
                "            numOutputRows = int(first_row[0]) if first_row[0] is not None else df.count()\r\n",
                "            numTargetRowsInserted = int(first_row[1]) if first_row[1] is not None else df.count()\r\n",
                "            numTargetRowsUpdated = int(first_row[2]) if first_row[2] is not None else 0\r\n",
                "\r\n",
                "            numInputRows = df.count()\r\n",
                "            return numInputRows, numOutputRows, numTargetRowsInserted, numTargetRowsUpdated\r\n",
                "\r\n",
                "\r\n",
                "        else:\r\n",
                "            logger.debug('[EDFIOEACHILD UPSERT] No existing delta table found. Creating delta table.')\r\n",
                "            # df = self.add_and_set_record_version1(df, etl_table = etl_table)\r\n",
                "            if not(partitioning):\r\n",
                "                logger.info('[EDFIOEACHILD UPSERT] Writing unpartitioned delta lake')\r\n",
                "                df.write.format('delta').save(destination_url)\r\n",
                "            else:\r\n",
                "                if partitioning and len(partitioning_cols) == 0:\r\n",
                "                    logger.info('[EDFIOEACHILD UPSERT] Partitioning columns absent - defaulting to DistrictId and SchoolYear as partitioning columns')\r\n",
                "                    df.write.format('delta').partitionBy('DistrictId', 'SchoolYear').save(destination_url)\r\n",
                "                else:\r\n",
                "                    partitioning_str = ', '.join(partitioning_cols)\r\n",
                "                    logger.info(f'[EDFIOEACHILD UPSERT] Writing partitioned delta lake - partitioned by - {partitioning_str}')\r\n",
                "                    df.write.format('delta').partitionBy(*partitioning_cols).save(destination_url)\r\n",
                "\r\n",
                "            numInputRows, numOutputRows, numTargetRowsInserted, numTargetRowsUpdated = self.return_upsert_metrics(numOutputRows = None, \r\n",
                "                                                                                                                  numTargetRowsInserted = None, \r\n",
                "                                                                                                                  numTargetRowsUpdated = None, \r\n",
                "                                                                                                                  df = df)\r\n",
                "        return numInputRows, numOutputRows, numTargetRowsInserted, numTargetRowsUpdated\r\n",
                "\r\n",
                "    def delete_then_insert(self, df, destination_path, primary_key='id', partitioning=False, partitioning_cols = [], surrogate_key = False):\r\n",
                "        # FIXME: Re-check Skey logic when maxSkey is None (not Int)\r\n",
                "        \"\"\" Upserts the data in the given dataframe into the specified destination using the given primary_key_column to identify the updates.\r\n",
                "            If there is no delta table found in the destination_path, one will be created.    \r\n",
                "        \"\"\"\r\n",
                "        if partitioning:\r\n",
                "            partitioning_cols = partitioning_cols\r\n",
                "        else:\r\n",
                "            partitioning_cols = []\r\n",
                "        \r\n",
                "        skey = None\r\n",
                "        destination_url = self.to_url(destination_path)\r\n",
                "        df = self.fix_column_names(df)\r\n",
                "        if type(primary_key) == list:\r\n",
                "            pk_statement = self.return_pk_statement(primary_key)\r\n",
                "            skey = list()\r\n",
                "            for pk_component in primary_key:\r\n",
                "                sk_component = pk_component[:-4] + 'SKey'\r\n",
                "                skey.append(sk_component)\r\n",
                "        else:\r\n",
                "            skey = primary_key[:-4] + 'SKey'\r\n",
                "            if 'hkey' in primary_key.lower():\r\n",
                "                surrogate_key = True\r\n",
                "            pk_statement = self.return_pk_statement([primary_key])\r\n",
                "        \r\n",
                "        df_original = df\r\n",
                "        \r\n",
                "        if surrogate_key:\r\n",
                "            df, skey, surrogate_key = self.add_surrogate_key_to_df(df = df, \r\n",
                "                                                                   primary_key = primary_key, \r\n",
                "                                                                   surrogate_key = surrogate_key,\r\n",
                "                                                                   func_enabled = self.surrogateKeyMode)\r\n",
                "            df_original = df\r\n",
                "        else:\r\n",
                "            pass\r\n",
                "        \r\n",
                "        if DeltaTable.isDeltaTable(spark, destination_url) and df.count() <= 0:\r\n",
                "            logger.info(\"[EDFIOEACHILD DELETE THEN INSERT] No Ingress Records\")\r\n",
                "            numInputRows = numOutputRows = numTargetRowsInserted = numTargetRowsUpdated = 0\r\n",
                "\r\n",
                "        elif DeltaTable.isDeltaTable(spark, destination_url) and df.count() > 0:\r\n",
                "            delta_table_sink = DeltaTable.forPath(spark, destination_url)\r\n",
                "            if surrogate_key:\r\n",
                "                if type(primary_key) == list:\r\n",
                "                    for index, pk_component in enumerate(primary_key):\r\n",
                "                        sk_component = skey[index]\r\n",
                "                        sink_df = self.query(destination_path, f'select max({sk_component}) max_skey')\r\n",
                "                        max_skey = int(sink_df.first()['max_skey'])\r\n",
                "                        # df = df.withColumn(skey, F.col(skey) + max_skey)\r\n",
                "                        df = df.withColumn(sk_component, F.when(F.col(sk_component) == -1, F.col(sk_component)).otherwise(F.col(sk_component) + max_skey))\r\n",
                "                else:\r\n",
                "                    sink_df = self.query(destination_path, f'select max({skey}) max_skey')\r\n",
                "                    max_skey = int(sink_df.first()['max_skey'])\r\n",
                "                    # df = df.withColumn(skey, F.col(skey) + max_skey)\r\n",
                "                    df = df.withColumn(skey, F.when(F.col(skey) == -1, F.col(skey)).otherwise(F.col(skey) + max_skey))\r\n",
                "\r\n",
                "            # FIXME: 2024-02-07 Under Dev (Renewed Logic)\r\n",
                "            incomingPartitionsCache = df.select(*partitioning_cols).distinct().first()\r\n",
                "            destination_partition_url = self.construct_partitioned_url(incomingPartitionsCache,partitioning_cols, destination_path)\r\n",
                "            if DeltaTable.isDeltaTable(spark, destination_partition_url):\r\n",
                "                # NOTE: MERGE into a partition if it already exists\r\n",
                "                logger.info('[EDFIOEACHILD DELETE THEN INSERT] Upsert by Partitions + PK Cols')\r\n",
                "                if surrogate_key:\r\n",
                "                    update_cols = self.return_upsert_cols(df.columns, partitioning_cols, primary_key, 'update', skey)\r\n",
                "                    insert_cols = self.return_upsert_cols(df.columns, partitioning_cols, primary_key, 'insert', None)\r\n",
                "                    delta_table_sink.alias('sink').merge(df.alias('updates'), pk_statement).whenMatchedUpdate(set = update_cols).whenNotMatchedInsert(values = insert_cols).execute()\r\n",
                "                else:\r\n",
                "                    logger.info('[EDFIOEACHILD DELETE THEN INSERT] DELETE THEN INSERT')\r\n",
                "                    delta_table_sink.alias('sink').merge(df.alias('updates'), pk_statement).whenMatchedDelete().execute()#whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\r\n",
                "                    df_original.write.format('delta').mode('append').option(\"mergeSchema\", \"true\").partitionBy(*partitioning_cols).save(destination_url) \r\n",
                "            else:\r\n",
                "                # NOTE: Dynamically create a new partition if it does not exist\r\n",
                "                # FIXME: ELSE CONDITION RELEVANCE AND EFFECT TO BE REVIEWED\r\n",
                "                logger.info('[EDFIOEACHILD DELETE THEN INSERT] Dynamically over-write the partition')\r\n",
                "                spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\r\n",
                "                df.write.format('delta').mode('overwrite').partitionBy(*partitioning_cols).save(destination_url)\r\n",
                "            numInputRows, numOutputRows, numTargetRowsInserted, numTargetRowsUpdated = self.return_upsert_metrics(numOutputRows = 0, \r\n",
                "                                                                                                                  numTargetRowsInserted = 0, \r\n",
                "                                                                                                                  numTargetRowsUpdated = 0, \r\n",
                "                                                                                                                  df = df,\r\n",
                "                                                                                                                  delta_table_sink = delta_table_sink)\r\n",
                "        else:\r\n",
                "            logger.debug('[EDFIOEACHILD DELETE THEN INSERT] No existing delta table found. Creating delta table.')\r\n",
                "            if not(partitioning):\r\n",
                "                logger.info('[EDFIOEACHILD DELETE THEN INSERT] Writing unpartitioned delta lake')\r\n",
                "                df.write.format('delta').save(destination_url)\r\n",
                "            else:\r\n",
                "                if partitioning and len(partitioning_cols) == 0:\r\n",
                "                    logger.info('[EDFIOEACHILD DELETE THEN INSERT] Partitioning columns absent - defaulting to DistrictId and SchoolYear as partitioning columns')\r\n",
                "                    df.write.format('delta').partitionBy('DistrictId', 'SchoolYear').save(destination_url)\r\n",
                "                else:\r\n",
                "                    partitioning_str = ', '.join(partitioning_cols)\r\n",
                "                    logger.info(f'[EDFIOEACHILD DELETE THEN INSERT] Writing partitioned delta lake - partitioned by - {partitioning_str}')\r\n",
                "                    df.write.format('delta').partitionBy(*partitioning_cols).save(destination_url)\r\n",
                "\r\n",
                "            numInputRows, numOutputRows, numTargetRowsInserted, numTargetRowsUpdated = self.return_upsert_metrics(numOutputRows = None, \r\n",
                "                                                                                                                  numTargetRowsInserted = None, \r\n",
                "                                                                                                                  numTargetRowsUpdated = None, \r\n",
                "                                                                                                                  df = df)\r\n",
                "        return numInputRows, numOutputRows, numTargetRowsInserted, numTargetRowsUpdated\r\n",
                "\r\n",
                "    def overwrite(self, \r\n",
                "               df, \r\n",
                "               destination_path, \r\n",
                "               primary_key='id', \r\n",
                "               partitioning=False, \r\n",
                "               partitioning_cols = [], \r\n",
                "               surrogate_key = False, \r\n",
                "               de_duplicate = True):\r\n",
                "        \"\"\" Upserts the data in the given dataframe into the specified destination using the given primary_key_column to identify the updates.\r\n",
                "            If there is no delta table found in the destination_path, one will be created.    \r\n",
                "        \"\"\"\r\n",
                "        table_name = destination_path.split('/')[-1]\r\n",
                "        if partitioning:\r\n",
                "            partitioning_cols = partitioning_cols\r\n",
                "        else:\r\n",
                "            partitioning_cols = []\r\n",
                "        \r\n",
                "        if not(self.are_partition_cols_pure(df, partitioning_cols)):\r\n",
                "            raise Exception(f\"[EDFIOEACHILD OVERWRITE] {table_name} Partition column with non-unique values is present - current implementation does not support this\")\r\n",
                "    \r\n",
                "        skey = None\r\n",
                "        destination_url = self.to_url(destination_path)\r\n",
                "        df = self.fix_column_names(df)\r\n",
                "        if type(primary_key) == list:\r\n",
                "            pk_statement = self.return_pk_statement(primary_key)\r\n",
                "            skey = list()\r\n",
                "            for pk_component in primary_key:\r\n",
                "                sk_component = pk_component[:-4] + 'SKey'\r\n",
                "                skey.append(sk_component)\r\n",
                "        else:\r\n",
                "            skey = primary_key[:-4] + 'SKey'\r\n",
                "            if 'hkey' in primary_key.lower():\r\n",
                "                logger.info('[EDFIOEACHILD OVERWRITE] HKey Exists: Entering if control block')\r\n",
                "            pk_statement = self.return_pk_statement([primary_key])\r\n",
                "        #surrogate_key= False #Testing this out\r\n",
                "        if surrogate_key:\r\n",
                "            logger.info('[EDFIOEACHILD OVERWRITE] Trying to create surrogate key')\r\n",
                "            # df, skey, surrogate_key = self.add_surrogate_key_to_df(df = df, \r\n",
                "            #                                                        primary_key = primary_key, \r\n",
                "            #                                                        surrogate_key = surrogate_key,\r\n",
                "            #                                                        func_enabled = self.surrogateKeyMode)\r\n",
                "        else:\r\n",
                "            # NOTE: Do not De-Duplicate when surrogate key is present\r\n",
                "            if de_duplicate:\r\n",
                "                df = self.get_df_deDuplicated_records(df, partitioning, primary_key, partitioning_cols)\r\n",
                "\r\n",
                "        if DeltaTable.isDeltaTable(spark, destination_url) and df.count() <= 0:\r\n",
                "            logger.info(f\"[EDFIOEACHILD OVERWRITE] {table_name} No Ingress Records\")\r\n",
                "            numInputRows = numOutputRows = numTargetRowsInserted = numTargetRowsUpdated = 0\r\n",
                "        else:\r\n",
                "            logger.debug(f'[EDFIOEACHILD OVERWRITE] {table_name} Overwriting existing delta table found. Creating delta table.')\r\n",
                "            # df = self.add_and_set_record_version1(df, etl_table = etl_table)\r\n",
                "            if not(partitioning):\r\n",
                "                logger.info(f'[EDFIOEACHILD OVERWRITE] {table_name} Writing unpartitioned delta lake')\r\n",
                "                df.write.format('delta').mode('overwrite').save(destination_url)\r\n",
                "            else:\r\n",
                "                if partitioning and len(partitioning_cols) == 0:\r\n",
                "                    logger.info(f'[EDFIOEACHILD OVERWRITE] {table_name} Partitioning columns absent - defaulting to DistrictId and SchoolYear as partitioning columns')\r\n",
                "                    df.write.format('delta').mode('overwrite').partitionBy('DistrictId', 'SchoolYear').save(destination_url)\r\n",
                "                else:\r\n",
                "                    partitioning_str = ', '.join(partitioning_cols)\r\n",
                "                    logger.info(f'[EDFIOEACHILD OVERWRITE] {table_name} Writing partitioned delta lake - partitioned by - {partitioning_str}')\r\n",
                "                    df.write.format('delta').mode('overwrite').partitionBy(*partitioning_cols).save(destination_url)\r\n",
                "\r\n",
                "            numInputRows, numOutputRows, numTargetRowsInserted, numTargetRowsUpdated = self.return_upsert_metrics(numOutputRows = None, \r\n",
                "                                                                                                                  numTargetRowsInserted = None, \r\n",
                "                                                                                                                  numTargetRowsUpdated = None, \r\n",
                "                                                                                                                  df = df)\r\n",
                "        return numInputRows, numOutputRows, numTargetRowsInserted, numTargetRowsUpdated\r\n",
                "\r\n",
                "    \r\n",
                "    def append(self, df, destination_path, primary_key='id', partitioning = False, partitioning_cols = []):\r\n",
                "        # TODO: Edit similarly as above\r\n",
                "        \"\"\" Appends the given dataframe to the delta table in the specified destination.\r\n",
                "            If there is no delta table found in the destination_path, one will be created.    \r\n",
                "        \"\"\"\r\n",
                "        destination_url = self.to_url(destination_path)\r\n",
                "        df = self.fix_column_names(df)\r\n",
                "\r\n",
                "        if partitioning: \r\n",
                "            df = df.dropDuplicates([primary_key] + partitioning_cols)\r\n",
                "        else:\r\n",
                "            df = df.dropDuplicates([primary_key])\r\n",
                "\r\n",
                "        if DeltaTable.isDeltaTable(spark, destination_url):\r\n",
                "            df.write.format('delta').mode('append').save(destination_url)  # https://docs.delta.io/latest/delta-batch.html#append\r\n",
                "        else:\r\n",
                "            logger.debug('No existing delta table found. Creating delta table.')\r\n",
                "            if not(partitioning):\r\n",
                "                logger.info('[EDFIOEACHILD APPEND] Writing unpartitioned delta lake')\r\n",
                "                df.write.format('delta').save(destination_url)\r\n",
                "            elif partitioning and len(partitioning_cols) == 0:\r\n",
                "                logger.info('[EDFIOEACHILD APPEND] Partitioning columns absent - defaulting to DistrictId and SchoolYear as partitioning columns')\r\n",
                "                df.write.format('delta').partitionBy('DistrictId', 'SchoolYear').save(destination_url)\r\n",
                "            else:\r\n",
                "                partitioning_str = ', '.join(partitioning_cols)\r\n",
                "                logger.info(f'[EDFIOEACHILD APPEND] Writing partitioned delta lake - partitioned by - {partitioning_str}')\r\n",
                "                df.write.format('delta').partitionBy(*partitioning_cols).save(destination_url)\r\n",
                "    \r\n",
                "    def ingest(self, entity_path, primary_key='id', hashing = False,natural_key = None,landingDateTimeFormat = \"yyyyMMddHHmmss\",ingestionHistoryMode = False,options={}):\r\n",
                "        \"\"\" Ingests the data for the entity in the given path.\r\n",
                "            CSV files are expected to have a header row by default, and JSON files are expected to have complete JSON docs on each row in the file.\r\n",
                "            To specify options that are different from these defaults, use the options param.\r\n",
                "            eg, ingest('contoso_sis/v0.1/students') # ingests all entities found in that path\r\n",
                "            eg, ingest('contoso_sis/v0.1/students', options={'header':False}) # for CSV files that don't have a header\r\n",
                "        \"\"\"\r\n",
                "        self.ingestionHistoryMode = ingestionHistoryMode \r\n",
                "        if not(hashing):\r\n",
                "            natural_key = None\r\n",
                "        primary_key = self.fix_column_name(primary_key) # fix the column name, in case it has a space in it or some other invalid character\r\n",
                "        ingested_path = f'stage2/Ingested/{entity_path}'\r\n",
                "        raw_path = f'stage1/Transactional/{entity_path}'\r\n",
                "\r\n",
                "        if not self.path_exists(raw_path):\r\n",
                "            logger.error(f'[EDFIOEACHILD INGESTION] Failed to ingest data because the given source data was not found where expected: {raw_path}')\r\n",
                "            return\r\n",
                "\r\n",
                "        batches = self.get_batch_info(raw_path)\r\n",
                "        number_of_inbound_changes = 0\r\n",
                "        for batch in batches:\r\n",
                "            batch_type = batch[0]\r\n",
                "            source_data_format = batch[1]\r\n",
                "            logger.info(f'[EDFIOEACHILD INGESTION] Ingesting from: {raw_path}, batch type of: {batch_type}, source data format of: {source_data_format}')\r\n",
                "            source_url = self.to_url(f'{raw_path}/{batch_type}_batch_data')\r\n",
                "\r\n",
                "            if oea.get_folder_size(f'{source_url}/{self.get_latest_folder(source_url)}') > 0:\r\n",
                "                if batch_type == 'snapshot'or batch_type=='additive': source_url = f'{source_url}/{self.get_latest_folder(source_url)}' \r\n",
                "                    \r\n",
                "                logger.debug(f'Processing {batch_type} data from: {source_url} and writing out to: {ingested_path}')\r\n",
                "                if batch_type == 'snapshot':\r\n",
                "                    def batch_func(df, batch_id): self.overwrite(df, ingested_path, primary_key)\r\n",
                "                elif batch_type == 'additive':\r\n",
                "                    def batch_func(df, batch_id): self.append(df, ingested_path, primary_key)\r\n",
                "                elif batch_type == 'delta' and not ingestionHistoryMode:\r\n",
                "                    def batch_func(df, batch_id): self.upsert(df, ingested_path, primary_key)\r\n",
                "                elif batch_type == 'delta' and ingestionHistoryMode:\r\n",
                "                    # FIXME: 2024-02-09 De-Duplication Based on latest record testing (via strategy1 => ingested data is ingested_history data)\r\n",
                "                    #        This approach requires using append function\r\n",
                "                    def batch_func(df, batch_id): self.append(df, ingested_path, primary_key)\r\n",
                "                elif batch_type == \"delete\":\r\n",
                "                    # FIXME: 2024-02-09 Reverted to using delete_rows (instead of soft_delete_rows)\r\n",
                "                    #        Side Effects of the revert under review\r\n",
                "                    def batch_func(df, batch_id): self.delete_rows(df, ingested_path, primary_key, batch_id)\r\n",
                "                else:\r\n",
                "                    raise ValueError(\"No valid batch folder was found at that path (expected to find a single folder with one of the following names: snapshot_batch_data, additive_batch_data, or delta_batch_data). Are you sure you have the right path?\")                      \r\n",
                "\r\n",
                "                if options == None: options = {}\r\n",
                "                options['format'] = source_data_format # eg, 'csv', 'json'\r\n",
                "                if source_data_format == 'csv' and (not 'header' in options or options['header'] == None): options['header'] = True  # default to expecting a header in csv files\r\n",
                "                if source_data_format == 'json' and (not 'multiline' in options or options['multiline'] == None): options['multiline'] = True # default to expecting multiline formatted json data\r\n",
                "\r\n",
                "                number_of_new_inbound_rows = self.process(source_path = source_url, \r\n",
                "                                                          foreach_batch_function = batch_func, \r\n",
                "                                                          batch_type = batch_type, \r\n",
                "                                                          natural_key = natural_key,\r\n",
                "                                                          landingDateTimeFormat = landingDateTimeFormat, \r\n",
                "                                                          options = options)\r\n",
                "                if number_of_new_inbound_rows > 0:    \r\n",
                "                    self.add_to_lake_db(ingested_path, overwrite = True)\r\n",
                "                number_of_inbound_changes += number_of_new_inbound_rows\r\n",
                "        return number_of_inbound_changes\r\n",
                "    \r\n",
                "    def get_sink_general_sensitive_paths(self, source_path):\r\n",
                "        path_dict = self.parse_path(source_path)\r\n",
                "        \r\n",
                "        sink_general_path = path_dict['entity_parent_path'].replace('Ingested', 'Refined') + '/general/' + path_dict['entity']\r\n",
                "        sink_sensitive_path = path_dict['entity_parent_path'].replace('Ingested', 'Refined') + '/sensitive/' + path_dict['entity'] + '_lookup'\r\n",
                "\r\n",
                "        return sink_general_path, sink_sensitive_path\r\n",
                "\r\n",
                "    def refine(self, entity_path, metadata=None, primary_key='id'):\r\n",
                "        source_path = f'stage2/Ingested/{entity_path}'\r\n",
                "        primary_key = self.fix_column_name(primary_key) # fix the column name, in case it has a space in it or some other invalid character\r\n",
                "        sink_general_path, sink_sensitive_path = get_sink_general_sensitive_paths(source_path)\r\n",
                "\r\n",
                "        if not metadata:\r\n",
                "            all_metadata = self.get_metadata_from_path(path_dict['entity_parent_path'])\r\n",
                "            metadata = all_metadata[path_dict['entity']]\r\n",
                "        \r\n",
                "        df_changes = self.get_latest_changes(source_path, sink_general_path)\r\n",
                "        spark_schema = self.to_spark_schema(metadata)\r\n",
                "        df_changes = self.modify_schema(df_changes, spark_schema)        \r\n",
                "        if df_changes.count() > 0:\r\n",
                "            df_pseudo, df_lookup = self.pseudonymize(df_changes, metadata)\r\n",
                "            self.upsert(df_pseudo, sink_general_path, f'{primary_key}_pseudonym') # todo: remove this assumption that the primary key will always be hashed during pseduonymization\r\n",
                "            self.upsert(df_lookup, sink_sensitive_path, primary_key)    \r\n",
                "            self.add_to_lake_db(sink_general_path)\r\n",
                "            self.add_to_lake_db(sink_sensitive_path)\r\n",
                "            logger.info(f'[EDFIOEACHILD REFINEMENT] Processed {df_changes.count()} updated rows from {source_path} into stage2/Refined')\r\n",
                "        else:\r\n",
                "            logger.info(f'[EDFIOEACHILD REFINEMENT] No updated rows in {source_path} to process.')\r\n",
                "        \r\n",
                "        return df_changes.count()\r\n",
                "\r\n",
                "    def pseudonymize(self, df, metadata, transform_mode = False, debugging = True): #: list[list[str]]):\r\n",
                "        \"\"\" Performs pseudonymization of the given dataframe based on the provided metadata (in the OEA format).\r\n",
                "            For example, if the given df is for an entity called person, \r\n",
                "            2 dataframes will be returned, one called person that has hashed ids and masked fields, \r\n",
                "            and one called person_lookup that contains the original person_id, person_id_pseudo,\r\n",
                "            and the non-masked values for columns marked to be masked.           \r\n",
                "            The lookup table should be written to a \"sensitive\" folder in the data lake.\r\n",
                "            eg, df_pseudo, df_lookup = oea.pseudonymize(df, metadata)\r\n",
                "            [More info on this approach here: https://learn.microsoft.com/en-us/azure/databricks/security/privacy/gdpr-delta#pseudonymize-data]\r\n",
                "        \"\"\"\r\n",
                "        salt = self._get_salt()\r\n",
                "        df_pseudo = df\r\n",
                "        df_lookup = df\r\n",
                "        if transform_mode:\r\n",
                "            lookup_cols = ['DistrictId', 'SchoolYear']\r\n",
                "        else:\r\n",
                "            lookup_cols = []\r\n",
                "        if debugging:\r\n",
                "            col_name = 'id'\r\n",
                "            #df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\r\n",
                "            #df_lookup = df_lookup.withColumn(col_name + \"_pseudonym\", F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256))\r\n",
                "            \r\n",
                "            df_pseudo = df_pseudo.withColumnRenamed(col_name, col_name + \"_pseudonym\")\r\n",
                "            df_lookup = df_lookup.withColumn(col_name + \"_pseudonym\", F.col(col_name))\r\n",
                "            \r\n",
                "            lookup_cols.append(col_name)\r\n",
                "            lookup_cols.append(col_name + \"_pseudonym\")\r\n",
                "        else:\r\n",
                "            for row in metadata:\r\n",
                "                col_name = row[0]\r\n",
                "                dtype = row[1]\r\n",
                "                op = row[2]\r\n",
                "                if op == \"hash-no-lookup\" or op == \"hnl\":\r\n",
                "                    # This means that the lookup can be performed against a different table so no lookup is needed.\r\n",
                "                    df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\r\n",
                "                    df_lookup = df_lookup.drop(col_name)           \r\n",
                "                elif op == \"hash\" or op == 'h':\r\n",
                "                    df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\r\n",
                "                    df_lookup = df_lookup.withColumn(col_name + \"_pseudonym\", F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256))\r\n",
                "                    \r\n",
                "                    lookup_cols.append(col_name)\r\n",
                "                    lookup_cols.append(col_name + \"_pseudonym\")\r\n",
                "                \r\n",
                "                elif op == \"mask\" or op == 'm':\r\n",
                "                    df_pseudo = df_pseudo.withColumn(col_name, F.lit('*'))\r\n",
                "                elif op == \"partition-by\":\r\n",
                "                    pass # make no changes for this column so that it will be in both dataframes and can be used for partitioning\r\n",
                "                elif op == \"no-op\" or op == 'x':\r\n",
                "                    df_lookup = df_lookup.drop(col_name)\r\n",
                "\t\t\r\n",
                "        df_lookup = df_lookup.select(*lookup_cols)\r\n",
                "        return (df_pseudo, df_lookup)\r\n",
                "    \r\n",
                "    # def add_to_lake_db(self, source_entity_path, overwrite = False, extension = None):\r\n",
                "    #     \"\"\" Adds the given entity as a table (if the table doesn't already exist) to the proper lake db based on the path.\r\n",
                "    #         This method will also create the lake db if it doesn't already exist.\r\n",
                "    #         eg: add_to_lake_db('stage2/Ingested/contoso_sis/v0.1/students')\r\n",
                "\r\n",
                "    #         Note that a spark db that points to source data in the delta format can't be queried via SQL serverless pool. More info here: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#delta-lake\r\n",
                "    #     \"\"\"\r\n",
                "    #     source_dict = self.parse_path(source_entity_path)\r\n",
                "        \r\n",
                "    #     db_name = source_dict['ldb_name']\r\n",
                "    #     if extension is not None:\r\n",
                "    #         if not(extension.startswith('_')):\r\n",
                "    #             extension = '_' + extension\r\n",
                "    #         source_dict['entity'] = source_dict['entity'] + str(extension)\r\n",
                "\r\n",
                "    #     spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\r\n",
                "    #     if overwrite:\r\n",
                "    #         spark.sql(f\"drop table if exists {db_name}.{source_dict['entity']}\")\r\n",
                "\r\n",
                "    #     spark.sql(f\"create table if not exists {db_name}.{source_dict['entity']} using DELTA location '{self.to_url(source_dict['entity_path'])}'\")\r\n",
                "    \r\n",
                "    def add_to_lake_db(self, source_entity_path, overwrite = False, extension = None):\r\n",
                "        \"\"\" Adds the given entity as a table (if the table doesn't already exist) to the proper lake db based on the path.\r\n",
                "            This method will also create the lake db if it doesn't already exist.\r\n",
                "            eg: add_to_lake_db('stage2/Ingested/contoso_sis/v0.1/students')\r\n",
                "\r\n",
                "            Note that a spark db that points to source data in the delta format can't be queried via SQL serverless pool. More info here: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#delta-lake\r\n",
                "        \"\"\"\r\n",
                "        source_dict = self.parse_path(source_entity_path)\r\n",
                "        db_name = source_dict['ldb_name']\r\n",
                "        if '/emptySchemas/' not in source_entity_path:\r\n",
                "            if extension is not None:\r\n",
                "                if not(extension.startswith('_')):\r\n",
                "                    extension = '_' + extension\r\n",
                "                source_dict['entity'] = source_dict['entity'] + str(extension)\r\n",
                "            \r\n",
                "            spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\r\n",
                "            if overwrite:\r\n",
                "                spark.sql(f\"drop table if exists {db_name}.{source_dict['entity']}\")\r\n",
                "\r\n",
                "            spark.sql(f\"create table if not exists {db_name}.{source_dict['entity']} using DELTA location '{self.to_url(source_dict['entity_path'])}'\")\r\n",
                "        \r\n",
                "    def add_to_bucketed_lake_db(self, \r\n",
                "                                source_entity_path, \r\n",
                "                                df, \r\n",
                "                                primary_key,\r\n",
                "                                partitioning_cols,\r\n",
                "                                destination_url,\r\n",
                "                                num_buckets = 5,\r\n",
                "                                overwrite = False, \r\n",
                "                                extension = None):\r\n",
                "        \"\"\" Adds the given entity as a table (if the table doesn't already exist) to the proper lake db based on the path.\r\n",
                "            This method will also create the lake db if it doesn't already exist.\r\n",
                "            eg: add_to_lake_db('stage2/Ingested/contoso_sis/v0.1/students')\r\n",
                "\r\n",
                "            Note that a spark db that points to source data in the delta format can't be queried via SQL serverless pool. More info here: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#delta-lake\r\n",
                "        \"\"\"\r\n",
                "        source_dict = self.parse_path(source_entity_path)\r\n",
                "        \r\n",
                "        db_name = source_dict['ldb_name']\r\n",
                "        if extension is not None:\r\n",
                "            source_dict['entity'] = source_dict['entity'] + str(extension)\r\n",
                "\r\n",
                "        spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\r\n",
                "        if overwrite:\r\n",
                "            spark.sql(f\"drop table if exists {db_name}.{source_dict['entity']}\")\r\n",
                "        \r\n",
                "        df.write.format('delta').mode('append').option(\"mergeSchema\", \"true\").partitionBy(*partitioning_cols).bucketBy(num_buckets, primary_key).option(\"path\", destination_url).saveAsTable(f\"{db_name}.{source_dict['entity']}\")\r\n",
                "        #spark.sql(f\"create table if not exists {db_name}.{source_dict['entity']} using DELTA location '{self.to_url(source_dict['entity_path'])}'\")\r\n",
                "\r\n",
                "    def soft_delete_rows(self, df, destination_path, primary_key='id', batch_id=''):\r\n",
                "        \"\"\"Soft deletes the entities in the given dataframe in the specified destination Delta table\r\n",
                "            Marks the entities as deleted by setting a status column using MERGE INTO\r\n",
                "        \"\"\"\r\n",
                "        df = df.withColumn(\"rowIsActive\", F.lit(False))\r\n",
                "        ref = f'{destination_path.split(\"/\")[-1]}{batch_id}'\r\n",
                "        print(ref)\r\n",
                "        df.createOrReplaceGlobalTempView(ref)\r\n",
                "        source_dict = self.parse_path(destination_path)\r\n",
                "        db_name = source_dict['ldb_name']\r\n",
                "        entity_name = source_dict['entity']\r\n",
                "        \r\n",
                "        destination_url = self.to_url(destination_path)\r\n",
                "        rundate = datetime.now().replace(microsecond=0) # use UTC for the datetime because when parsing it out later, spark's to_timestamp() assumes the local machine's timezone, and the timezone for the spark cluster will be UTC\r\n",
                "\r\n",
                "        # Use MERGE INTO to update the Delta table based on the condition\r\n",
                "        if DeltaTable.isDeltaTable(spark, destination_url):\r\n",
                "            delta_table_sink = DeltaTable.forPath(spark, destination_url)\r\n",
                "            delta_table_sink.alias('sink').merge(df.alias('source'), f'sink.{primary_key} = source.{primary_key}') \\\r\n",
                "                .whenMatchedUpdate(set={\"rowIsActive\": \"false\", \"LastModifiedDate\": f\"'{rundate}'\"}) \\\r\n",
                "                .execute()\r\n",
                "\r\n",
                "        # Refresh the table to make the changes visible\r\n",
                "        spark.sql(f\"REFRESH TABLE {db_name}.{entity_name}\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Ed-Fi Client - API Client"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "class EdFiClient:\r\n",
                "    #The constructor\r\n",
                "    def __init__(self, workspace, kvName, moduleName, authUrl, dataManagementUrl, changeQueriesUrl, dependenciesUrl, apiVersion, batchLimit, minChangeVer=\"\", maxChangeVer=\"\", landingDateTimeFormat = \"yyyyMMddHHmmss\", schoolYear=None, districtId=None, kvSecret_clientId = None, kvSecret_clientSecret = None, retry_strategy = None, threadMode = False, devMode = False, error_logger = None):\r\n",
                "        self.workspace = workspace\r\n",
                "        if retry_strategy is None:\r\n",
                "            self.retry_strategy = Retry(total=3,\r\n",
                "                                        backoff_factor=1,\r\n",
                "                                        status_forcelist=[429, 500, 502, 503, 504],\r\n",
                "                                        allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\", \"POST\", \"DELETE\"])\r\n",
                "        else:\r\n",
                "            self.retry_strategy = retry_strategy \r\n",
                "        self.adapter = HTTPAdapter(max_retries=self.retry_strategy)\r\n",
                "        self.threadMode = threadMode\r\n",
                "        if self.threadMode:\r\n",
                "            self.thread_local = threading.local()\r\n",
                "        self.keyvault_linked_service = 'LS_KeyVault'\r\n",
                "        if kvName is None:\r\n",
                "            kvName = oea.keyvault\r\n",
                "\r\n",
                "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\r\n",
                "        for handler in logging.getLogger().handlers:\r\n",
                "            handler.setFormatter(formatter)           \r\n",
                "        # Customize log level for all loggers\r\n",
                "        logging.getLogger().setLevel(logging.INFO)   \r\n",
                "        logger.info(f\"[LANDING]: minChangeVersion={minChangeVer} and maxChangeVersion={maxChangeVer}\")\r\n",
                "\r\n",
                "        if not kvName and workspace == \"dev\":\r\n",
                "            logger.info(\"defaulting to test data\")\r\n",
                "            self.clientId = \"\"\r\n",
                "            self.clientSecret = \"\"\r\n",
                "        else:\r\n",
                "            try:\r\n",
                "                #try to get the credentials from keyvault\r\n",
                "                if devMode:\r\n",
                "                    self.clientId = kvSecret_clientId # oea._get_secret(kvSecret_clientId) if kvSecret_clientId is not None else kvSecret_clientId\r\n",
                "                    self.clientSecret = kvSecret_clientSecret # oea._get_secret(kvSecret_clientSecret) if kvSecret_clientSecret is not None else kvSecret_clientSecret\r\n",
                "                else:\r\n",
                "                    self.clientId = oea._get_secret(kvSecret_clientId)\r\n",
                "                    self.clientSecret = oea._get_secret(kvSecret_clientSecret)\r\n",
                "            except Exception as e:\r\n",
                "                #if there was an error getting the credentials\r\n",
                "                #if this is the dev instance proceed with test data, otherwise raise the Exception\r\n",
                "                logger.info(f\"failed to retrieve clientId and clientSecret from keyvault with exception: {str(e)}\")\r\n",
                "                if workspace == \"dev\":\r\n",
                "                    logger.info(\"defaulting to test data\")\r\n",
                "                    self.clientId = \"\"\r\n",
                "                    self.clientSecret = \"\"\r\n",
                "                else:\r\n",
                "                    raise\r\n",
                "        \r\n",
                "        self.authUrl = authUrl\r\n",
                "        self.dataManagementUrl = dataManagementUrl\r\n",
                "        self.changeQueriesUrl = changeQueriesUrl\r\n",
                "        self.dependenciesUrl = dependenciesUrl\r\n",
                "        from datetime import datetime\r\n",
                "        if landingDateTimeFormat == 'yyyyMMddHHmmss':\r\n",
                "            self.runDate = datetime.utcnow().strftime('%Y%m%d%H%M%S')\r\n",
                "        else:\r\n",
                "            self.runDate = datetime.utcnow().strftime(landingDateTimeFormat)\r\n",
                "        self.authTime = None\r\n",
                "        self.expiresIn = None\r\n",
                "        self.accessToken = None\r\n",
                "        districtPath = districtId if districtId != None else \"All\"\r\n",
                "        schoolYearPath = schoolYear if schoolYear != None else \"All\"\r\n",
                "        self.transactionalFolder = f\"Transactional/{moduleName}/{apiVersion}/DistrictId={districtPath}/SchoolYear={schoolYearPath}\"\r\n",
                "        self.batchLimit = batchLimit\r\n",
                "        self.minChangeVer = minChangeVer\r\n",
                "        self.maxChangeVer = maxChangeVer\r\n",
                "        self.error_logger = error_logger\r\n",
                "        self.failedBatchUrls = dict()\r\n",
                "        self.lock = threading.Lock()\r\n",
                "    \r\n",
                "    def init_thread_local_vars(self):\r\n",
                "        if self.threadMode:\r\n",
                "            if not hasattr(self.thread_local, \"authTime\"):\r\n",
                "                self.thread_local.authTime = None\r\n",
                "            if not hasattr(self.thread_local, \"expiresIn\"):\r\n",
                "                self.thread_local.expiresIn = None\r\n",
                "            if not hasattr(self.thread_local, \"accessToken\"):\r\n",
                "                self.thread_local.accessToken = None\r\n",
                "            if not hasattr(self.thread_local, \"session\"):\r\n",
                "                self.thread_local.session = requests.Session()\r\n",
                "                self.thread_local.session.mount(\"https://\", self.adapter)\r\n",
                "                self.thread_local.session.mount(\"http://\", self.adapter)\r\n",
                "\r\n",
                "    def getSession(self):\r\n",
                "        if self.threadMode:\r\n",
                "            if not hasattr(self.thread_local, \"session\"):\r\n",
                "                self.thread_local.session = requests.Session()\r\n",
                "                self.thread_local.session.mount(\"https://\", self.adapter)\r\n",
                "                self.thread_local.session.mount(\"http://\", self.adapter)\r\n",
                "            return self.thread_local.session\r\n",
                "        else:\r\n",
                "            if not hasattr(self, \"session\"):\r\n",
                "                self.session = requests.Session()\r\n",
                "                self.session.mount(\"https://\", self.adapter)\r\n",
                "                self.session.mount(\"http://\", self.adapter)\r\n",
                "            return self.session\r\n",
                "\r\n",
                "    #Method to get the access token for the test data set\r\n",
                "    def authenticateWithAuthorization(self):\r\n",
                "        #TODO: need to update this if we want it to work with other edfi provided test data set versions\r\n",
                "        result = requests.post(\"https://api.ed-fi.org/v5.2/api/oauth/token\",{\"grant_type\":\"client_credentials\"},headers={\"Authorization\":\"Basic UnZjb2hLejl6SEk0OkUxaUVGdXNhTmY4MXh6Q3h3SGZib2xrQw==\"})\r\n",
                "        return result\r\n",
                "\r\n",
                "    #Method to get the access token for a production system with basic auth\r\n",
                "    def authenticateWithBasic(self):\r\n",
                "        authHeader = HTTPBasicAuth(self.clientId, self.clientSecret)\r\n",
                "        result = requests.post(self.authUrl,{\"grant_type\":\"client_credentials\"},auth=authHeader)\r\n",
                "        return result\r\n",
                "\r\n",
                "    #This method orchestrates the authentication\r\n",
                "    def authenticate(self):\r\n",
                "        if self.threadMode:\r\n",
                "            self.thread_local.authTime = datetime.now()\r\n",
                "            if not self.clientId or not self.clientSecret: #self.workspace == \"dev\":\r\n",
                "                result = self.authenticateWithAuthorization().json()\r\n",
                "                logger.info(result)\r\n",
                "            else:\r\n",
                "                result = self.authenticateWithBasic().json()\r\n",
                "            self.thread_local.expiresIn = result[\"expires_in\"]\r\n",
                "            self.thread_local.accessToken = result[\"access_token\"]\r\n",
                "        else:\r\n",
                "            self.authTime = datetime.now()\r\n",
                "            if not self.clientId or not self.clientSecret: #self.workspace == \"dev\":\r\n",
                "                result = self.authenticateWithAuthorization().json()\r\n",
                "                logger.info(result)\r\n",
                "            else:\r\n",
                "                result = self.authenticateWithBasic().json()\r\n",
                "            self.expiresIn = result[\"expires_in\"]\r\n",
                "            self.accessToken = result[\"access_token\"]\r\n",
                "    \r\n",
                "    #This method manages the access token, refreshing it when required\r\n",
                "    def getAccessToken(self):\r\n",
                "        # IMPLEMENT THREADING\r\n",
                "        # Get a new access token if none exists, or if the expires time is within 5 minutes of expiry\r\n",
                "        currentTime = datetime.now()\r\n",
                "        if self.threadMode:\r\n",
                "             if self.thread_local.accessToken == None or (currentTime-self.thread_local.authTime).total_seconds() > self.thread_local.expiresIn - 300:\r\n",
                "                self.authenticate()\r\n",
                "                return self.thread_local.accessToken\r\n",
                "             else:\r\n",
                "                return self.thread_local.accessToken \r\n",
                "        else:\r\n",
                "            if self.accessToken == None or (currentTime-self.authTime).total_seconds() > self.expiresIn - 300:\r\n",
                "                self.authenticate()\r\n",
                "                return self.accessToken\r\n",
                "            else:\r\n",
                "                return self.accessToken \r\n",
                "\r\n",
                "    def getChangeQueryVersion(self):\r\n",
                "        access_token = self.getAccessToken()\r\n",
                "        requests_session = self.getSession()\r\n",
                "        response = requests_session.get(changeQueriesUrl + \"/availableChangeVersions\", headers={\"Authorization\":\"Bearer \" + access_token})\r\n",
                "        return response.json(), response.status_code\r\n",
                "\r\n",
                "    def getEntities(self):\r\n",
                "        requests_session = self.getSession()\r\n",
                "        return requests_session.get(self.dependenciesUrl).json()\r\n",
                "\r\n",
                "    def getDeletes(self,resource, minChangeVersion, maxChangeVersion):\r\n",
                "        if minChangeVersion is None and maxChangeVersion is None:\r\n",
                "            url = f\"{self.dataManagementUrl}{resource}/deletes\"\r\n",
                "        else:\r\n",
                "            url = f\"{self.dataManagementUrl}{resource}/deletes?MinChangeVersion={minChangeVersion}&MaxChangeVersion={maxChangeVersion}\"\r\n",
                "\r\n",
                "        requests_session = self.getSession()\r\n",
                "        result = requests_session.get(url,headers = {\"Authorization\": f\"Bearer {self.getAccessToken()}\"})\r\n",
                "        return result\r\n",
                "\r\n",
                "    def writeToDeletesFile(self, resource, deletes):\r\n",
                "        path = f\"stage1/{self.transactionalFolder}{resource}/delete_batch_data/rundate={self.runDate}/data{uuid.uuid4()}.json\"\r\n",
                "        mssparkutils.fs.put(oea.to_url(path),deletes.text)\r\n",
                "    \r\n",
                "    def landEntityWithChangeVersion(self, input_tuple):\r\n",
                "        self.init_thread_local_vars()\r\n",
                "        entity, minChangeVersion, maxChangeVersion, debugMode, fetchHistory, requestedUrls = input_tuple\r\n",
                "        resource = entity['resource']\r\n",
                "        resourceMinChangeVersion = self.getChangeVersion(resource, minChangeVersion) if self.minChangeVer is None else minChangeVersion\r\n",
                "        if fetchHistory:\r\n",
                "            resourceMinChangeVersion = \"0\"\r\n",
                "        self.landEntity(resource, resourceMinChangeVersion, maxChangeVersion, debugMode, requestedUrls)\r\n",
                "        deletes = self.getDeletes(resource, resourceMinChangeVersion, maxChangeVersion)\r\n",
                "        \r\n",
                "        if len(deletes.json()):\r\n",
                "            logger.info(f\"[LANDING WITH CHANGE VERSION] DELETES: Writing deletes for the resource: {entity}\")\r\n",
                "            self.writeToDeletesFile(resource, deletes)\r\n",
                "\r\n",
                "    def landEntities(self, entities = 'All', debugMode = False, fetchHistory = False, thread_max_workers = None, requestedUrls = dict()):\r\n",
                "        if entities == 'All':\r\n",
                "            entities = self.getEntities()\r\n",
                "        elif requestedUrls != dict():\r\n",
                "            entities = list(requestedUrls.keys())\r\n",
                "            entities = self.getSpecifiedEntities(entities)\r\n",
                "        else:\r\n",
                "            entities = self.getSpecifiedEntities(entities)\r\n",
                "\r\n",
                "        self.init_thread_local_vars()\r\n",
                "        changeVersion, changeVersionResponseStatus = self.getChangeQueryVersion()\r\n",
                "        if changeVersionResponseStatus < 400:\r\n",
                "            # FIXME - Temporary fix for oldestChangeVersion casing\r\n",
                "            try:\r\n",
                "                minChangeVersion = changeVersion['OldestChangeVersion'] if self.minChangeVer == None else int(self.minChangeVer)\r\n",
                "                maxChangeVersion = changeVersion['NewestChangeVersion']  if self.maxChangeVer == None else int(self.maxChangeVer)\r\n",
                "            except:\r\n",
                "                minChangeVersion = changeVersion['oldestChangeVersion'] if self.minChangeVer == None else int(self.minChangeVer)\r\n",
                "                maxChangeVersion = changeVersion['newestChangeVersion']  if self.maxChangeVer == None else int(self.maxChangeVer)\r\n",
                "        else:\r\n",
                "            minChangeVersion = None\r\n",
                "            maxChangeVersion = None\r\n",
                "        \r\n",
                "        if self.threadMode:\r\n",
                "            with concurrent.futures.ThreadPoolExecutor(max_workers=thread_max_workers) as tpe:\r\n",
                "                logger.info(\"[LANDING ENTITIES WITH THREADS] Entered Thread Pool\")\r\n",
                "                self.init_thread_local_vars()\r\n",
                "                for entity in entities:\r\n",
                "                    tpe.submit(self.landEntityWithChangeVersion,(entity, minChangeVersion, maxChangeVersion,debugMode, fetchHistory, requestedUrls.get(entity['resource'], [])))\r\n",
                "        else:\r\n",
                "            for entity in entities:\r\n",
                "                self.landEntityWithChangeVersion((entity, minChangeVersion, maxChangeVersion, debugMode, fetchHistory, requestedUrls.get(entity['resource'], [])))\r\n",
                "    \r\n",
                "    def getChangeVersion(self, resource, default):\r\n",
                "        path = f\"stage1/{self.transactionalFolder}{resource}/changeFile.json\"\r\n",
                "        if mssparkutils.fs.exists(oea.to_url(path)):\r\n",
                "            return json.loads(mssparkutils.fs.head(oea.to_url(path)))['changeVersion']\r\n",
                "        else:\r\n",
                "            return default\r\n",
                "\r\n",
                "    def landEntity(self,resource,minChangeVersion,maxChangeVersion, debugMode = False, requestedUrls = list()):\r\n",
                "        logger.info(f\"[LANDING ENTITY] initiating {resource}\")\r\n",
                "        if minChangeVersion is None and maxChangeVersion is None:\r\n",
                "            url = f\"{self.dataManagementUrl}{resource}?totalCount=true\"\r\n",
                "        else:\r\n",
                "            url = f\"{self.dataManagementUrl}{resource}?MinChangeVersion={minChangeVersion}&MaxChangeVersion={maxChangeVersion}&totalCount=true\"\r\n",
                "            \r\n",
                "        path = f\"stage1/{self.transactionalFolder}{resource}\"\r\n",
                "        requests_session = self.getSession()\r\n",
                "        total_count_response = requests_session.get(url, headers={\"Authorization\":f\"Bearer {self.getAccessToken()}\"})\r\n",
                "        failedBatchUrls = list()\r\n",
                "        total_data_size = 0\r\n",
                "        start_time = datetime.now()\r\n",
                "\r\n",
                "        # FIXME: 2024-02-15 Failed Urls Fetches\r\n",
                "        if requestedUrls is not None and len(requestedUrls) > 1:\r\n",
                "            logger.info(f\"[LANDING ENTITY]: Landing data for the urls - {len(requestedUrls)}\")\r\n",
                "            for url in requestedUrls:\r\n",
                "                data = requests_session.get(url, headers={\"Authorization\":f\"Bearer {self.getAccessToken()}\"}) \r\n",
                "                if(data.status_code < 400):  \r\n",
                "                    filepath = f\"{path}/delta_batch_data/rundate={self.runDate}/data{uuid.uuid4()}.json\"\r\n",
                "                    output = json.loads(data.text)\r\n",
                "\r\n",
                "                    partition_data_size = len(output)\r\n",
                "                    total_data_size = total_data_size + partition_data_size  \r\n",
                "                    total_count = total_data_size                       \r\n",
                "                    output_string = \"\"\r\n",
                "                    for line in output:\r\n",
                "                        output_string += json.dumps(line) + \"\\n\"\r\n",
                "                    mssparkutils.fs.put(oea.to_url(filepath),output_string)\r\n",
                "                    failedBatchUrls.append(url)\r\n",
                "                else:\r\n",
                "                    logger.error(f\"[LANDING ENTITY] There was an error retrieving batch data for {resource}\")\r\n",
                "                    failedBatchUrls.append(url)\r\n",
                "        else:\r\n",
                "            try:\r\n",
                "                total_count = int(total_count_response.headers[\"Total-Count\"])\r\n",
                "                logger.info(f'[LANDING ENTITY] {resource}: TOTAL RECORD COUNT - {total_count}') \r\n",
                "            except:\r\n",
                "                logger.error(F\"[LANDING ENTITY] {resource}: Total-Count not present\")\r\n",
                "            try:\r\n",
                "                #Keyset pagination implementation: https://techdocs.ed-fi.org/display/ODSAPIS3V61/Improve+Paging+Performance+on+Large+API+Resources\r\n",
                "                \r\n",
                "                #split into the total number of partitions, and the range size\r\n",
                "                total_count = int(total_count_response.headers[\"Total-Count\"])\r\n",
                "                if debugMode:\r\n",
                "                    logger.info(f\"[LANDING ENTITY] {resource}: --- Total Count         : {total_count}\")\r\n",
                "                    logger.info(f\"[LANDING ENTITY] {resource}: --- Batch Size         : {self.batchLimit}\")\r\n",
                "\r\n",
                "                partitions = math.ceil(total_count / self.batchLimit)                \r\n",
                "                if(total_count == 0 and partitions == 0):\r\n",
                "                    logger.info(f'[LANDING ENTITY] {resource}: No new / updated items b/w the following versions {minChangeVersion} and {maxChangeVersion}')\r\n",
                "                else:\r\n",
                "                    range_size = math.ceil(maxChangeVersion / partitions)\r\n",
                "                    for i in range(partitions + 1):\r\n",
                "                        #calculate the min and max change version for the partition\r\n",
                "                        partitionMinChangeVersion = i*range_size\r\n",
                "                        partitionMaxChangeVersion = min(maxChangeVersion, (i+1)*range_size)\r\n",
                "\r\n",
                "                        #Calculate the number of batches per partition\r\n",
                "                        partitionUrl=f\"{self.dataManagementUrl}{resource}?MinChangeVersion={partitionMinChangeVersion}&MaxChangeVersion={partitionMaxChangeVersion}&totalCount=true\"\r\n",
                "                        partition_count_response = requests_session.get(partitionUrl, headers={\"Authorization\":f\"Bearer {self.getAccessToken()}\"})\r\n",
                "                        partition_count = int(partition_count_response.headers[\"Total-Count\"])\r\n",
                "                        batches = partition_count // self.batchLimit\r\n",
                "\r\n",
                "                        if debugMode:\r\n",
                "                            logger.info(f\"[LANDING ENTITY] {resource}: --- Partition Number         : {i}\")\r\n",
                "                            logger.info(f\"[LANDING ENTITY] {resource}: --- Partition MinChangeVer   : {partitionMinChangeVersion}\")\r\n",
                "                            logger.info(f\"[LANDING ENTITY] {resource}: --- Partition MaxChangeVer   : {partitionMaxChangeVersion}\")\r\n",
                "                            logger.info(f\"[LANDING ENTITY] {resource}: --- Number of batches        : {batches}\", )\r\n",
                "                            logger.info(f\"[LANDING ENTITY] {resource}: --- Number of partitions     : {partition_count}\")\r\n",
                "\r\n",
                "                        for j in range(batches + 1):\r\n",
                "                            batchUrl=f\"{partitionUrl}&limit={self.batchLimit}&offset={(j)*self.batchLimit}\"\r\n",
                "                            data = requests_session.get(batchUrl, headers={\"Authorization\":f\"Bearer {self.getAccessToken()}\"}) \r\n",
                "                            if(data.status_code < 400):\r\n",
                "                                filepath = f\"{path}/delta_batch_data/rundate={self.runDate}/data{uuid.uuid4()}.json\"\r\n",
                "                                output = json.loads(data.text)\r\n",
                "\r\n",
                "                                partition_data_size = len(output)\r\n",
                "                                total_data_size = total_data_size + partition_data_size                  \r\n",
                "                                output_string = \"\"\r\n",
                "                                for line in output:\r\n",
                "                                    output_string += json.dumps(line) + \"\\n\"\r\n",
                "                                mssparkutils.fs.put(oea.to_url(filepath),output_string)\r\n",
                "                            else:\r\n",
                "                                logger.error(f\"[LANDING ENTITY] There was an error retrieving batch data for {resource}\")\r\n",
                "                                failedBatchUrls.append(batchUrl)\r\n",
                "            except ZeroDivisionError as zero_error:\r\n",
                "                logger.error(f'[LANDING ENTITY] Divide by Zero Error - {zero_error}; Landing Data of offset = 0')\r\n",
                "                requests_session = self.getSession()\r\n",
                "                data = requests_session.get(url, headers={\"Authorization\":f\"Bearer {self.getAccessToken()}\"})      \r\n",
                "                if(data.status_code < 400):\r\n",
                "                    filepath = f\"{path}/delta_batch_data/rundate={self.runDate}/data{uuid.uuid4()}.json\"\r\n",
                "                    output = json.loads(data.text)\r\n",
                "\r\n",
                "                    partition_data_size = len(output)\r\n",
                "                    total_data_size = total_data_size + partition_data_size                  \r\n",
                "                    if(len(output) == 0):\r\n",
                "                        logger.info(f'[LANDING ENTITY] {resource}: No new / updated items b/w the following versions {minChangeVersion} and {maxChangeVersion}')\r\n",
                "                    else:\r\n",
                "                        output_string = \"\"\r\n",
                "                        for line in output:\r\n",
                "                            output_string += json.dumps(line) + \"\\n\"\r\n",
                "                        mssparkutils.fs.put(oea.to_url(filepath),output_string)\r\n",
                "                else:\r\n",
                "                    logger.info(f\"[LANDING DIVIDE BY ZERO] There was an error retrieving data for {resource}\")\r\n",
                "                    failedBatchUrls.append(url)\r\n",
                "            except Exception as error:\r\n",
                "                if resource == '/ed-fi/schoolYearTypes':\r\n",
                "                    output, total_count, returned_failedBatchUrls = self.returnEntityData(resource = resource,\r\n",
                "                                                minChangeVersion=None,\r\n",
                "                                                maxChangeVersion=None)\r\n",
                "                    total_data_size = len(output)\r\n",
                "                    filepath = f\"{path}/delta_batch_data/rundate={self.runDate}/data{uuid.uuid4()}.json\"\r\n",
                "                    if(len(output) == 0):\r\n",
                "                        logger.info(f'[LANDING ENTITY] {resource}: No new / updated items b/w the following versions {minChangeVersion} and {maxChangeVersion}')\r\n",
                "                    else:\r\n",
                "                        output_string = \"\"\r\n",
                "                        for line in output:\r\n",
                "                            output_string += json.dumps(line) + \"\\n\"\r\n",
                "                        mssparkutils.fs.put(oea.to_url(filepath),output_string)\r\n",
                "                    failedBatchUrls = failedBatchUrls + returned_failedBatchUrls\r\n",
                "                else:\r\n",
                "                    logger.info(f'[LANDING ENTITY] An Error Occured - {error}; Landing of the resource - {resource} skipped')\r\n",
                "                    return\r\n",
                "        with self.lock:\r\n",
                "            self.failedBatchUrls[resource] = failedBatchUrls\r\n",
                "        \r\n",
                "        changeFilepath = f\"{path}/changeFile.json\"\r\n",
                "        changeData = {\"changeVersion\":maxChangeVersion}\r\n",
                "        mssparkutils.fs.put(oea.to_url(changeFilepath),json.dumps(changeData),True)\r\n",
                "        logger.info(f\"[LANDING ENTITY] Completed {resource}\")\r\n",
                "\r\n",
                "        end_time = datetime.now()\r\n",
                "        # FIXME 2024-02-20: Under Review - logging, numInputRows, numTargetRowsInserted\r\n",
                "        log_data = self.error_logger.create_log_dict(uniqueId = self.error_logger.generate_random_alphanumeric(10), # Generate a random 10-character alphanumeric value\r\n",
                "                                                pipelineExecutionId = pipelineExecutionId,#'TEST_1234',#executionId,\r\n",
                "                                                sparkSessionId = spark.sparkContext.applicationId,\r\n",
                "                                                stageName = \"ed-fi: Landing\",\r\n",
                "                                                schemaFormat = 'ed-fi: nested',\r\n",
                "                                                entityType =  resource.split('/')[1],\r\n",
                "                                                entityName = resource.split('/')[-1],\r\n",
                "                                                numInputRows = total_count,\r\n",
                "                                                totalNumOutputRows = total_data_size,\r\n",
                "                                                numTargetRowsInserted = total_data_size,\r\n",
                "                                                numTargetRowsUpdated = 0,\r\n",
                "                                                numRecordsSkipped = 0,\r\n",
                "                                                numRecordsDeleted = 0,\r\n",
                "                                                start_time = start_time,\r\n",
                "                                                end_time = end_time,\r\n",
                "                                                insertionType = 'NA/Invalid')\r\n",
                "        with self.lock:\r\n",
                "            self.error_logger.consolidate_logs(log_data,'entity')\r\n",
                "    \r\n",
                "    def parse_text_to_dataframe(self, text_content, delimiter=','):\r\n",
                "        csv_file = StringIO(text_content)\r\n",
                "        df = pd.read_csv(csv_file, delimiter=delimiter) \r\n",
                "        \r\n",
                "        return df\r\n",
                "\r\n",
                "    def extract_entities_for_etl(self, df):\r\n",
                "        concat_list = []\r\n",
                "        entity_names_list = []\r\n",
                "        \r\n",
                "        for index, row in df.iterrows():\r\n",
                "            entity_type = row['entity_type']\r\n",
                "            entity_name = row['entity_name']\r\n",
                "            \r\n",
                "            if entity_type != 'ed-fi':\r\n",
                "                concat_list.append(f'/{entity_type}/{entity_name}')\r\n",
                "            \r\n",
                "            concat_list.append(f'/ed-fi/{entity_name}')\r\n",
                "            entity_names_list.append(entity_name)\r\n",
                "        \r\n",
                "        return concat_list, list(set(entity_names_list))\r\n",
                "\r\n",
                "\r\n",
                "    def getSpecifiedEntities(self, entities_list):\r\n",
                "        data = self.getEntities()\r\n",
                "        entities = [item for item in data if item['resource'] in entities_list]\r\n",
                "        return entities\r\n",
                "\r\n",
                "    def listSpecifiedEntities(self, path): \r\n",
                "        fullpath = path + '/entities-to-extract.csv'\r\n",
                "        pathExists = oea.path_exists(fullpath)\r\n",
                "        if pathExists:\r\n",
                "            csv_str = oea.get_text_from_path(fullpath)\r\n",
                "            csv_pd_df = self.parse_text_to_dataframe(csv_str, delimiter=',')\r\n",
                "            api_entities, entities = self.extract_entities_for_etl(csv_pd_df)\r\n",
                "        else:\r\n",
                "            api_entities = list()\r\n",
                "            entities = list()\r\n",
                "        return api_entities, entities\r\n",
                "\r\n",
                "    def returnEntityData(self,\r\n",
                "                         resource,\r\n",
                "                         minChangeVersion=None,\r\n",
                "                         maxChangeVersion=None,\r\n",
                "                         increment = 50):\r\n",
                "        self.init_thread_local_vars()\r\n",
                "        offset = 0\r\n",
                "        total_count = None\r\n",
                "        logger.info(f\"[LANDING W/O CHANGE VERSION] initiating {resource}\")\r\n",
                "        failedBatchUrls = list()\r\n",
                "        try:\r\n",
                "            temp_output = \"PLACEHOLDER\"\r\n",
                "            while temp_output != []:\r\n",
                "                url = f\"{self.dataManagementUrl}{resource}?limit={self.batchLimit}&offset={offset}&totalCount=true\"\r\n",
                "                temp_access_token = self.getAccessToken()\r\n",
                "                requests_session = self.getSession()\r\n",
                "                data = requests_session.get(url, headers={\"Authorization\": f\"Bearer {temp_access_token}\"})\r\n",
                "                if data.status_code == 404:\r\n",
                "                    logger.info(\"[LANDING W/O CHANGE VERSION] RESOURCE NOT FOUND\")\r\n",
                "                    return None, None, None\r\n",
                "                if data.status_code < 400:\r\n",
                "                    if (temp_output == \"PLACEHOLDER\"):\r\n",
                "                        temp_output = json.loads(data.text)\r\n",
                "                        output = temp_output\r\n",
                "                        if total_count is None:\r\n",
                "                            total_count = int(data.headers[\"Total-Count\"])\r\n",
                "                    else:\r\n",
                "                        temp_output = json.loads(data.text)\r\n",
                "                        output = output + temp_output\r\n",
                "                else:\r\n",
                "                    logger.error(f'[LANDING W/O CHANGE VERSION] ERROR - {data.status_code}')\r\n",
                "                    failedBatchUrls.append(url)\r\n",
                "                    return None, None, None\r\n",
                "                offset += increment\r\n",
                "        except Exception as e:\r\n",
                "            logger.error(f\"[LANDING W/O CHANGE VERSION] ERROR Occurred - {e}\")\r\n",
                "        return output, total_count, failedBatchUrls\r\n",
                "\r\n",
                "    def fetch_descriptors(self, \r\n",
                "                          descriptor_col, \r\n",
                "                          entities_info, \r\n",
                "                          minChangeVersion = None, \r\n",
                "                          maxChangeVersion = None):\r\n",
                "        for entity_info in entities_info:\r\n",
                "            if entity_info['resource'] == f'/ed-fi/{descriptor_col}':\r\n",
                "                descriptor_col = f'/ed-fi/{descriptor_col}'\r\n",
                "                \r\n",
                "            elif entity_info['resource'] == f'/TX/{descriptor_col}':\r\n",
                "                descriptor_col = f'/TX/{descriptor_col}'\r\n",
                "            \r\n",
                "        if not descriptor_col.startswith('/'):\r\n",
                "            logger.info(\"[FETCH DESCRIPTOR] No Such Entity\")\r\n",
                "            return None\r\n",
                "        \r\n",
                "        output, _ = self.returnEntityData(resource = descriptor_col,\r\n",
                "                                       minChangeVersion = minChangeVersion, \r\n",
                "                                       maxChangeVersion = maxChangeVersion)\r\n",
                "        if output is not None:\r\n",
                "            spark_df = spark.createDataFrame(output)\r\n",
                "        else:\r\n",
                "            return output\r\n",
                "        return spark_df"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Ed-Fi Refinement"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "class EdFiRefine:\r\n",
                "    #The constructor\r\n",
                "    def __init__(self, \r\n",
                "                 workspace, \r\n",
                "                 oea, \r\n",
                "                 spark,\r\n",
                "                 schema_gen,\r\n",
                "                 moduleName, \r\n",
                "                 authUrl,\r\n",
                "                 swaggerUrl, \r\n",
                "                 dataManagementUrl, \r\n",
                "                 changeQueriesUrl, \r\n",
                "                 dependenciesUrl, \r\n",
                "                 apiVersion, \r\n",
                "                 schoolYear, \r\n",
                "                 districtId,\r\n",
                "                 pipelineExecutionId,\r\n",
                "                 error_logger,\r\n",
                "                 test_mode):\r\n",
                "        self.workspace = workspace\r\n",
                "        self.oea = oea\r\n",
                "        self.spark = spark\r\n",
                "        self.schema_gen = schema_gen\r\n",
                "\r\n",
                "        self.moduleName = moduleName\r\n",
                "        self.authUrl = authUrl\r\n",
                "        self.swaggerUrl = swaggerUrl\r\n",
                "        self.dataManagementUrl = dataManagementUrl\r\n",
                "        self.dependenciesUrl = dependenciesUrl\r\n",
                "        self.apiVersion = apiVersion\r\n",
                "        \r\n",
                "        self.schoolYear = schoolYear\r\n",
                "        self.districtId = districtId\r\n",
                "        self.districtId_col_name = 'DistrictId'\r\n",
                "        self.schoolYear_col_name = 'SchoolYear'\r\n",
                "\r\n",
                "        self.schemas = self.schema_gen.create_spark_schemas()\r\n",
                "        self.primitive_datatypes = ['timestamp', 'date', 'decimal', 'boolean', 'integer', 'string', 'long']\r\n",
                "        self.test_mode = test_mode\r\n",
                "        self.thread_local = threading.local()\r\n",
                "        self.pipelineExecutionId = pipelineExecutionId\r\n",
                "        self.error_logger = error_logger\r\n",
                "    \r\n",
                "    def store_start_time(self, table_name):\r\n",
                "        if not hasattr(self.thread_local, \"start_times\"):\r\n",
                "            self.thread_local.start_times = dict()\r\n",
                "            self.thread_local.start_times[table_name] = datetime.now()\r\n",
                "        else:\r\n",
                "            self.thread_local.start_times[table_name] = datetime.now()\r\n",
                "        \r\n",
                "    def get_descriptor_schema(self, descriptor):\r\n",
                "        fields = []\r\n",
                "        fields.append(StructField('_etag',LongType(), True))\r\n",
                "        fields.append(StructField(f\"{descriptor[:-1]}Id\", IntegerType(), True))\r\n",
                "        fields.append(StructField('codeValue',StringType(), True))\r\n",
                "        fields.append(StructField('description',StringType(), True))\r\n",
                "        fields.append(StructField('id',StringType(), True))\r\n",
                "        fields.append(StructField('namespace',StringType(), True))\r\n",
                "        fields.append(StructField('shortDescription',StringType(), True))\r\n",
                "        return StructType(fields)\r\n",
                "\r\n",
                "    def get_descriptor_metadata(self, descriptor):\r\n",
                "        return [['_etag', 'long', 'no-op'],\r\n",
                "                [f\"{descriptor[:-1]}Id\", 'integer', 'hash'],\r\n",
                "                ['codeValue','string', 'no-op'],\r\n",
                "                ['description','string', 'no-op'],\r\n",
                "                ['id','string', 'no-op'],\r\n",
                "                ['namespace','string', 'no-op'],\r\n",
                "                ['shortDescription','string', 'no-op']]\r\n",
                "\r\n",
                "    def has_column(self, df, col):\r\n",
                "        try:\r\n",
                "            df[col]\r\n",
                "            return True\r\n",
                "        except AnalysisException:\r\n",
                "            return False\r\n",
                "\r\n",
                "    def modify_descriptor_value(self, df, col_name, districtId_col_name = 'DistrictId', schoolYear_col_name = 'SchoolYear'):\r\n",
                "        if col_name in df.columns:\r\n",
                "            # TODO: @Abhinav, I do not see where you made the changes to use the descriptorId instead of Namespace/CodeValue\r\n",
                "            df = df.withColumn(f\"{col_name}LakeId\", f.concat_ws('_', f.col(districtId_col_name), f.col(schoolYear_col_name), f.regexp_replace(col_name, '#', '_')))\r\n",
                "            df = df.drop(col_name)\r\n",
                "        else:\r\n",
                "            df = df.withColumn(f\"{col_name}LakeId\", f.lit(None).cast(\"String\"))\r\n",
                "\r\n",
                "        return df\r\n",
                "\r\n",
                "    def flatten_reference_col(self, df, target_col, districtId_col_name = 'DistrictId', schoolYear_col_name = 'SchoolYear'):\r\n",
                "        col_prefix = target_col.name.replace('Reference', '')\r\n",
                "        df = df.withColumn(f\"{col_prefix}LakeId\", f.when(f.col(target_col.name).isNotNull(), f.concat_ws('_', f.col(districtId_col_name), f.col(schoolYear_col_name), f.split(f.col(f'{target_col.name}.link.href'), '/').getItem(3))))\r\n",
                "        df = df.drop(target_col.name)\r\n",
                "        return df\r\n",
                "\r\n",
                "    def modify_references_and_descriptors(self, df, target_col, districtId_col_name = 'DistrictId', schoolYear_col_name = 'SchoolYear'):\r\n",
                "        for ref_col in [x for x in df.columns if re.search('Reference$', x) is not None]:\r\n",
                "            df = self.flatten_reference_col(df, target_col.dataType.elementType[ref_col], districtId_col_name, schoolYear_col_name)\r\n",
                "        for desc_col in [x for x in df.columns if re.search('Descriptor$', x) is not None]:\r\n",
                "            df = self.modify_descriptor_value(df, desc_col, districtId_col_name, schoolYear_col_name)\r\n",
                "        return df\r\n",
                "    \r\n",
                "    def upsert_with_logging(self, \r\n",
                "                            df, \r\n",
                "                            destination_path, \r\n",
                "                            primary_key, \r\n",
                "                            table_name,\r\n",
                "                            ext_entity,\r\n",
                "                            parent = True):\r\n",
                "        # TODO: 2024-02-12: Edit the logging capabilities\r\n",
                "        start_time = self.thread_local.start_times.get(table_name, datetime.now())\r\n",
                "        logger.info(f\"[REFINEMENT UPSERT WITH LOGGING] {table_name}: {destination_path}\")\r\n",
                "        if parent:\r\n",
                "            numInputRows, numOutputRows, numTargetRowsInserted, numTargetRowsUpdated = self.oea.upsert(df = df, \r\n",
                "                                                                    destination_path = destination_path,\r\n",
                "                                                                    primary_key = primary_key,#['RECORD', 'DistrictId', 'SchoolYear'],\r\n",
                "                                                                    partitioning = True,\r\n",
                "                                                                    partitioning_cols = [self.districtId_col_name, self.schoolYear_col_name],\r\n",
                "                                                                    surrogate_key = False)   \r\n",
                "        else:\r\n",
                "            numInputRows, numOutputRows, numTargetRowsInserted, numTargetRowsUpdated = self.oea.delete_then_insert(df = df, \r\n",
                "                                                                    destination_path = destination_path,\r\n",
                "                                                                    primary_key = primary_key,#['RECORD', 'DistrictId', 'SchoolYear'],\r\n",
                "                                                                    partitioning = True,\r\n",
                "                                                                    partitioning_cols = [self.districtId_col_name, self.schoolYear_col_name],\r\n",
                "                                                                    surrogate_key = False)                                    \r\n",
                "        end_time = datetime.now()\r\n",
                "        log_data = self.error_logger.create_log_dict(uniqueId = self.error_logger.generate_random_alphanumeric(10), # Generate a random 10-character alphanumeric value\r\n",
                "                                                pipelineExecutionId = self.pipelineExecutionId,#'TEST_1234',#executionId,\r\n",
                "                                                sparkSessionId = self.spark.sparkContext.applicationId,\r\n",
                "                                                stageName = \"ed-fi: Refinement\",\r\n",
                "                                                schemaFormat = 'ed-fi: exploded',\r\n",
                "                                                entityType =  ext_entity.lower(),\r\n",
                "                                                entityName = table_name,\r\n",
                "                                                numInputRows = numInputRows,\r\n",
                "                                                totalNumOutputRows = numOutputRows,\r\n",
                "                                                numTargetRowsInserted = numTargetRowsInserted,\r\n",
                "                                                numTargetRowsUpdated = numTargetRowsUpdated,\r\n",
                "                                                numRecordsSkipped = 0,\r\n",
                "                                                numRecordsDeleted = 0,\r\n",
                "                                                start_time = start_time,\r\n",
                "                                                end_time = end_time,\r\n",
                "                                                insertionType = 'upsert')\r\n",
                "        self.error_logger.consolidate_logs(log_data,'entity')\r\n",
                "\r\n",
                "    def explode_arrays(self, \r\n",
                "                       df, \r\n",
                "                       sink_general_path, \r\n",
                "                       target_col, \r\n",
                "                       schema_name, \r\n",
                "                       table_name, \r\n",
                "                       extension = None,\r\n",
                "                       ext_entity = None, \r\n",
                "                       districtId_col_name = 'DistrictId',\r\n",
                "                       schoolYear_col_name = 'SchoolYear',\r\n",
                "                       parent_cols = ['lakeId', 'DistrictId', 'SchoolYear', 'LastModifiedDate', 'rowIsActive'],\r\n",
                "                       nonNull_count = 1):\r\n",
                "        cols = parent_cols#['lakeId', 'DistrictId', 'SchoolYear', 'LastModifiedDate']\r\n",
                "        child_url = oea.to_url(f\"{sink_general_path}_{target_col.name}\")\r\n",
                "        child_name = f\"{table_name}_{target_col.name}\"\r\n",
                "        self.store_start_time(child_name)\r\n",
                "        if nonNull_count <=0 and DeltaTable.isDeltaTable(spark, child_url):\r\n",
                "            logger.info('[REFINEMENT EXPLOSION]: Child Table - No Ingress Records (Empty Schema Present)')\r\n",
                "        else:\r\n",
                "            child_df = df.select(cols + [target_col.name])\r\n",
                "            child_df = child_df.withColumn(\"exploded\", f.explode(target_col.name)).drop(target_col.name).select(cols + ['exploded.*'])\r\n",
                "            child_df_cached = child_df.cache()\r\n",
                "            child_df = child_df_cached\r\n",
                "\r\n",
                "            grand_child_df = None\r\n",
                "            grand_child_df_cached = None\r\n",
                "\r\n",
                "            # TODO: It looks like te {target_col.name}LakeId column is not addedd to the child entities\r\n",
                "            #       We should use LakeId suffix when using the \"id\" column from the parent and HKey suffix when creating a Hash Key based on composite key columns\r\n",
                "            identity_cols = [x.name for x in target_col.dataType.elementType.fields if 'x-Ed-Fi-isIdentity' in x.metadata].sort()\r\n",
                "            if(identity_cols is not None and len(identity_cols) > 0):\r\n",
                "                child_df = child_df.withColumn(f\"{target_col.name}LakeId\", f.concat(f.col(districtId_col_name), f.lit('_'), f.col(schoolYear_col_name), f.lit('_'), *[f.concat(f.col(x), f.lit('_')) for x in identity_cols]))\r\n",
                "            \r\n",
                "            # IMPORTANT: We must modify Reference and Descriptor columns for child columns \"first\". \r\n",
                "            # This must be done \"after\" the composite key from identity_cols has been created otherwise the columns are renamed and will not be found by identity_cols.\r\n",
                "            # This must be done \"before\" the grand_child is exploded below\r\n",
                "            child_df = self.modify_references_and_descriptors(child_df, target_col, districtId_col_name, schoolYear_col_name)\r\n",
                "\r\n",
                "            for array_sub_col in [x for x in target_col.dataType.elementType.fields if x.dataType.typeName() == 'array' ]:\r\n",
                "                grand_child_url = oea.to_url(f\"{sink_general_path}_{target_col.name}_{array_sub_col.name}\")\r\n",
                "                grand_child_name = f\"{table_name}_{target_col.name}_{array_sub_col.name}\"\r\n",
                "                self.store_start_time(grand_child_name)\r\n",
                "                \r\n",
                "                # TODO: Experiment with Non Nulls and Optimization (Future)\r\n",
                "                # child_df_size = child_df.withColumn(\"size\", F.size(F.col(array_sub_col.name)))\r\n",
                "                # nonNull_count = child_df_size.filter(F.col(\"size\") >= 1).count()\r\n",
                "                nonNull_count = 1\r\n",
                "                \r\n",
                "                if nonNull_count <=0 and DeltaTable.isDeltaTable(spark, grand_child_url):\r\n",
                "                    logger.info('[REFINEMENT EXPLOSION] Grand Child Table - No Ingress Records (Empty Schema Present)')\r\n",
                "                else:\r\n",
                "                    grand_child_df = child_df.withColumn('exploded', f.explode(array_sub_col.name)).select(child_df.columns + ['exploded.*']).drop(array_sub_col.name)\r\n",
                "                    grand_child_df_cached = grand_child_df.cache()\r\n",
                "                    grand_child_df = grand_child_df_cached\r\n",
                "                    \r\n",
                "                    # Modifying Reference and Descriptor columns for the grand_child array\r\n",
                "                    grand_child_df = self.modify_references_and_descriptors(grand_child_df, array_sub_col, districtId_col_name, schoolYear_col_name)\r\n",
                "\r\n",
                "                    logger.info(f\"[REFINEMENT EXPLOSION] Writing Grand Child Table - {table_name}_{target_col.name}_{array_sub_col.name}\")\r\n",
                "                    # TODO: Review impact of changing from UPSERT to delete_then_insert\r\n",
                "                    self.upsert_with_logging(df = grand_child_df, \r\n",
                "                                    destination_path = f\"{sink_general_path}_{target_col.name}_{array_sub_col.name}\", \r\n",
                "                                    primary_key = 'lakeId', \r\n",
                "                                    table_name = grand_child_name,\r\n",
                "                                    ext_entity = ext_entity,\r\n",
                "                                    parent = False)\r\n",
                "                    self.oea.add_to_lake_db(source_entity_path = f\"{sink_general_path}_{target_col.name}_{array_sub_col.name}\", \r\n",
                "                                    overwrite = True,\r\n",
                "                                    extension = extension)\r\n",
                "                    #grand_child_df.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').save(oea.to_url(f\"{sink_general_path}_{target_col.name}_{array_sub_col.name}\"))\r\n",
                "\r\n",
                "            logger.info(f\"[REFINEMENT EXPLOSION] Writing Child Table - {table_name}_{target_col.name}\")\r\n",
                "            # TODO: Review impact of changing from UPSERT to delete_then_insert\r\n",
                "            # FIXME: assessments_periods Temporary Fix\r\n",
                "            child_destination_path = f\"{sink_general_path}_{target_col.name}\"\r\n",
                "            if ('/assessments_period' in child_destination_path) and ('/assessments_periods' not in child_destination_path):\r\n",
                "                child_destination_path = child_destination_path.replace('/assessments_period', '/assessments_periods')\r\n",
                "            self.upsert_with_logging(df = child_df, \r\n",
                "                                    destination_path = child_destination_path, \r\n",
                "                                    primary_key = 'lakeId', \r\n",
                "                                    table_name = child_name,\r\n",
                "                                    ext_entity = ext_entity,\r\n",
                "                                    parent = False)\r\n",
                "            self.oea.add_to_lake_db(source_entity_path = child_destination_path,#f\"{sink_general_path}_{target_col.name}\",\r\n",
                "                            overwrite = True,\r\n",
                "                            extension = extension)\r\n",
                "            #child_df.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').save(oea.to_url(f\"{sink_general_path}_{target_col.name}\"))\r\n",
                "            child_df_cached.unpersist()\r\n",
                "            if grand_child_df:\r\n",
                "                grand_child_df_cached.unpersist()\r\n",
                "\r\n",
                "        # Drop array column from parent entity\r\n",
                "        df = df.drop(target_col.name)\r\n",
                "        return df\r\n",
                "\r\n",
                "    def transform(self,\r\n",
                "                df, \r\n",
                "                schema_name, \r\n",
                "                table_name, \r\n",
                "                primary_key,\r\n",
                "                ext_entity,\r\n",
                "                sink_general_path,\r\n",
                "                districtId_col_name = 'DistrictId', \r\n",
                "                schoolYear_col_name = 'SchoolYear'):\r\n",
                "        self.store_start_time(table_name)            \r\n",
                "        if re.search('Descriptors$', table_name) is None:\r\n",
                "            # Use Deep Copy otherwise the schemas object also gets modified every time target_schema is modified\r\n",
                "            target_schema = copy.deepcopy(self.schemas[table_name])\r\n",
                "            # Add primary key\r\n",
                "            if self.has_column(df, primary_key):\r\n",
                "                df = df.withColumn('lakeId', f.concat_ws('_', f.col(districtId_col_name), f.col(schoolYear_col_name), f.col(primary_key)).cast(\"String\"))\r\n",
                "            else:\r\n",
                "                df = df.withColumn('lakeId', f.lit(None).cast(\"String\"))\r\n",
                "        else:\r\n",
                "            target_schema = self.get_descriptor_schema(table_name)\r\n",
                "            # Add primary key\r\n",
                "            if self.has_column(df, 'codeValue') and self.has_column(df, 'namespace'):\r\n",
                "                # TODO: @Abhinav, I do not see where you made the changes to use the descriptorId instead of Namespace/CodeValue\r\n",
                "                df = df.withColumn('lakeId', f.concat_ws('_', f.col(districtId_col_name), f.col(schoolYear_col_name), f.col('namespace'), f.col('codeValue')).cast(\"String\"))\r\n",
                "            else:\r\n",
                "                df = df.withColumn('lakeId', f.lit(None).cast(\"String\"))\r\n",
                "\r\n",
                "        # FIXME: Temporary Fix\r\n",
                "        if table_name.lower().endswith('exts'):\r\n",
                "            ext_entity_flag = ext_entity\r\n",
                "        else:\r\n",
                "            ext_entity_flag = None\r\n",
                "\r\n",
                "        # FIXME schoolYearTypes TEMPORARY FIX\r\n",
                "        if table_name == 'schoolYearTypes':\r\n",
                "            target_schema = target_schema.add(StructField(districtId_col_name, StringType()))\\\r\n",
                "                                    .add(StructField('LastModifiedDate', TimestampType())) \\\r\n",
                "                                    .add(StructField('rowIsActive', BooleanType()))\r\n",
                "        else:\r\n",
                "            target_schema = target_schema.add(StructField(districtId_col_name, StringType()))\\\r\n",
                "                                        .add(StructField(schoolYear_col_name, StringType()))\\\r\n",
                "                                        .add(StructField('LastModifiedDate', TimestampType())) \\\r\n",
                "                                        .add(StructField('rowIsActive', BooleanType()))\r\n",
                "\r\n",
                "        df = self.transform_sub_module(df, \r\n",
                "                                       target_schema, \r\n",
                "                                       sink_general_path, \r\n",
                "                                       schema_name, \r\n",
                "                                       table_name,\r\n",
                "                                       extension = None, \r\n",
                "                                       ext_entity = 'ed-fi' if ext_entity_flag is None else ext_entity_flag,\r\n",
                "                                       districtId_col_name = districtId_col_name, \r\n",
                "                                       schoolYear_col_name = schoolYear_col_name)\r\n",
                "\r\n",
                "        # FIXME schoolYearTypes TEMPORARY FIX\r\n",
                "        if table_name == 'schoolYearTypes':\r\n",
                "            df = df.withColumnRenamed(\"schoolYear\", schoolYear_col_name)\r\n",
                "        \r\n",
                "        if self.test_mode:\r\n",
                "            return df\r\n",
                "\r\n",
                "        logger.info(f\"[REFINEMENT TRANSFORM] Writing Main Table - {table_name}\")\r\n",
                "        self.upsert_with_logging(df = df, \r\n",
                "                                    destination_path = f\"{sink_general_path}\", \r\n",
                "                                    primary_key = 'lakeId', \r\n",
                "                                    table_name = table_name,\r\n",
                "                                    ext_entity = 'ed-fi' if ext_entity_flag is None else ext_entity_flag,\r\n",
                "                                    parent = True)\r\n",
                "\r\n",
                "        self.oea.add_to_lake_db(source_entity_path = sink_general_path, \r\n",
                "                        overwrite = True,\r\n",
                "                        extension = None)\r\n",
                "\r\n",
                "        if '_ext' in df.columns:\r\n",
                "            target_schema = self.get_ext_entities_schemas(table_name = table_name,\r\n",
                "                                                    ext_column_name = '_ext',\r\n",
                "                                                    default_value = ext_entity)\r\n",
                "            df = self.flatten_ext_column(df = df, \r\n",
                "                                    table_name = table_name, \r\n",
                "                                    ext_col = '_ext', \r\n",
                "                                    inner_key = ext_entity,\r\n",
                "                                    ext_inner_cols = target_schema.fieldNames(),\r\n",
                "                                    base_cols = ['lakeId', districtId_col_name, 'LastModifiedDate',schoolYear_col_name, 'rowIsActive','id_pseudonym'])\r\n",
                "            sink_general_path = sink_general_path.replace('/ed-fi/', f'/{ext_entity.lower()}/')\r\n",
                "            df = self.transform_sub_module(df, \r\n",
                "                                    target_schema, \r\n",
                "                                    sink_general_path, \r\n",
                "                                    schema_name,\r\n",
                "                                    table_name,\r\n",
                "                                    extension = f\"_{ext_entity.lower()}\",\r\n",
                "                                    ext_entity = ext_entity,\r\n",
                "                                    districtId_col_name = districtId_col_name,\r\n",
                "                                    schoolYear_col_name = schoolYear_col_name)\r\n",
                "\r\n",
                "            logger.info(f\"[REFINEMENT TRANSFORM] Writing EXT Table - {table_name}\")\r\n",
                "            self.upsert_with_logging(df = df, \r\n",
                "                                    destination_path = f\"{sink_general_path}\", \r\n",
                "                                    primary_key = 'lakeId', \r\n",
                "                                    table_name = table_name,\r\n",
                "                                    ext_entity = ext_entity,\r\n",
                "                                    parent = True)\r\n",
                "\r\n",
                "            self.oea.add_to_lake_db(sink_general_path, \r\n",
                "                            overwrite = True,\r\n",
                "                            extension = f\"_{ext_entity.lower()}\")\r\n",
                "            return None\r\n",
                "            \r\n",
                "    def transform_sub_module(self, \r\n",
                "                             df, \r\n",
                "                             target_schema, \r\n",
                "                             sink_general_path, \r\n",
                "                             schema_name, \r\n",
                "                             table_name,\r\n",
                "                             extension = None,\r\n",
                "                             ext_entity = None,\r\n",
                "                             districtId_col_name = 'DistrictId',\r\n",
                "                             schoolYear_col_name = 'SchoolYear'):\r\n",
                "        # print(districtId_col_name)\r\n",
                "        for col_name in target_schema.fieldNames():\r\n",
                "            target_col = target_schema[col_name]\r\n",
                "            # If Primitive datatype, i.e String, Bool, Integer, etc.abs\r\n",
                "            # Note: Descriptor is a String therefore is a Primitive datatype\r\n",
                "            if target_col.dataType.typeName() in self.primitive_datatypes:\r\n",
                "                # If it is a Descriptor\r\n",
                "                if re.search('Descriptor$', col_name) is not None:\r\n",
                "                    df = self.modify_descriptor_value(df, col_name, districtId_col_name, schoolYear_col_name)\r\n",
                "                else:\r\n",
                "                    if col_name in df.columns:\r\n",
                "                        # Casting columns to primitive data types\r\n",
                "                        df = df.withColumn(col_name, f.col(col_name).cast(target_col.dataType))\r\n",
                "                    else:\r\n",
                "                        # If Column not present in dataframe, add column with None values.\r\n",
                "                        df = df.withColumn(col_name, f.lit(None).cast(target_col.dataType))\r\n",
                "            # If Complex datatype, i.e. Object, Array\r\n",
                "            else:\r\n",
                "                if col_name not in df.columns:\r\n",
                "                    df = df.withColumn(col_name, f.lit(None).cast(target_col.dataType))\r\n",
                "                else:\r\n",
                "                    # Generate JSON column as a Complex Type\r\n",
                "                    if (table_name.lower() == 'assessments' and col_name.lower() == 'period') and (target_col.dataType.typeName() != 'array'):\r\n",
                "                        # FIXME: Temporary Fix to deal with assessments_periods\r\n",
                "                        df = df.withColumn(col_name, f.array(f.col(col_name)))\r\n",
                "                        target_col.dataType = f.ArrayType(target_col.dataType)\r\n",
                "                    \r\n",
                "                    df = df.withColumn(f\"{col_name}_json\", f.to_json(f.col(col_name))) \\\r\n",
                "                        .withColumn(col_name, f.from_json(f.col(f\"{col_name}_json\"), target_col.dataType)) \\\r\n",
                "                        .drop(f\"{col_name}_json\")\r\n",
                "                \r\n",
                "                # Modify the links with surrogate keys\r\n",
                "                if re.search('Reference$', col_name) is not None:\r\n",
                "                    df = self.flatten_reference_col(df, target_col, districtId_col_name = districtId_col_name, schoolYear_col_name = schoolYear_col_name)\r\n",
                "                \r\n",
                "                if self.test_mode:\r\n",
                "                    return df\r\n",
                "        \r\n",
                "                if target_col.dataType.typeName() == 'array':\r\n",
                "                    # TODO: Experiment with Non Nulls and Optimization (Future)\r\n",
                "                    # df_size = df.withColumn(\"size\", F.size(F.col(target_col.name))).cache()\r\n",
                "                    # nonNull_count = df_size.filter(F.col(\"size\") >= 1).count()\r\n",
                "                    \r\n",
                "                    nonNull_count = 1\r\n",
                "                    df = self.explode_arrays(df, \r\n",
                "                                             sink_general_path,\r\n",
                "                                             target_col, \r\n",
                "                                             schema_name, \r\n",
                "                                             table_name, \r\n",
                "                                             extension = extension, \r\n",
                "                                             ext_entity = ext_entity,\r\n",
                "                                             districtId_col_name = districtId_col_name,\r\n",
                "                                             schoolYear_col_name = schoolYear_col_name,\r\n",
                "                                             parent_cols = ['lakeId', districtId_col_name, schoolYear_col_name, 'LastModifiedDate', 'rowIsActive'],\r\n",
                "                                             nonNull_count = nonNull_count)\r\n",
                "                    #df_size.unpersist()\r\n",
                "        return df\r\n",
                "    def get_ext_entities_schemas(self,\r\n",
                "                                table_name = 'staffs',\r\n",
                "                                ext_column_name = '_ext',\r\n",
                "                                default_value = 'TPDM'):\r\n",
                "        target_schema = copy.deepcopy(self.schemas[table_name])\r\n",
                "        for col_name in target_schema.fieldNames():\r\n",
                "            target_col = target_schema[col_name]\r\n",
                "            if target_col.name == ext_column_name:\r\n",
                "                if target_col.dataType[0].name == default_value:\r\n",
                "                    return target_col.dataType[0].dataType         \r\n",
                "                    \r\n",
                "    def flatten_ext_column(self, \r\n",
                "                           df, \r\n",
                "                           table_name, \r\n",
                "                           ext_col, \r\n",
                "                           inner_key,\r\n",
                "                           ext_inner_cols,\r\n",
                "                           base_cols = ['lakeId', 'DistrictId', 'LastModifiedDate', 'rowIsActive','SchoolYear', 'id_pseudonym']):\r\n",
                "        cols = base_cols\r\n",
                "        flattened_cols = ext_inner_cols#[\"educatorPreparationPrograms\"] #_ext_TX_cols[table_name]\r\n",
                "        dict_col = F.col(ext_col)[inner_key]\r\n",
                "        complex_dtype_text = str(df.select('_ext').dtypes[0][1])\r\n",
                "\r\n",
                "        exprs = [dict_col.getItem(key).alias(key) for key in flattened_cols if str(key) in complex_dtype_text]\r\n",
                "        flattened_df = df.select(exprs + cols)\r\n",
                "        return flattened_df\r\n",
                "\r\n",
                "    def sink_path_cleanup(self,destination_path):\r\n",
                "        pattern = re.compile(r'DistrictId=.*?/|SchoolYear=.*?/')\r\n",
                "        destination_path = re.sub(pattern, '', destination_path)\r\n",
                "\r\n",
                "        return destination_path\r\n",
                "\r\n",
                "    def non_common_elements(self, list1, list2):\r\n",
                "        unique_in_list1 = set(list1) - set(list2)\r\n",
                "        unique_in_list2 = set(list2) - set(list1)\r\n",
                "        \r\n",
                "        result = list(unique_in_list1) + list(unique_in_list2)\r\n",
                "        return result\r\n",
                "    \r\n",
                "    def non_empty_elements(self, emptyList, nonEmptyList):\r\n",
                "        unique_in_list1 = set(emptyList) - set(nonEmptyList)        \r\n",
                "        result = list(unique_in_list1)\r\n",
                "        return result\r\n",
                "   \r\n",
                "    def return_non_ext_tables(self):\r\n",
                "        table_names = list(self.schemas.keys())\r\n",
                "        non_ext_table_names = []\r\n",
                "        for table_name in table_names:\r\n",
                "            if 'extension' not in table_name.lower():\r\n",
                "                non_ext_table_names.append(table_name)\r\n",
                "        return non_ext_table_names"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Error Logging"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "class ErrorLogging:\r\n",
                "    def __init__(self, spark, oea, logger):\r\n",
                "        self.spark = spark\r\n",
                "        self.oea = oea\r\n",
                "        self.logger = logger\r\n",
                "        self.pipeline_id = None\r\n",
                "        self.spark_session_id = spark.sparkContext.applicationId\r\n",
                "        self.test_mode = True\r\n",
                "        self.entity_logs = list()\r\n",
                "        self.stage_logs = list()\r\n",
                "        self.pipeline_logs = list()\r\n",
                "        self.etl_logs = None\r\n",
                "\r\n",
                "    # Helper function to generate a random alphanumeric string of specified length\r\n",
                "    def generate_random_alphanumeric(self, length):\r\n",
                "        return uuid.uuid4().hex[:length]\r\n",
                "\r\n",
                "    def set_logs_prefix(self):\r\n",
                "        workspace_name = self.oea.workspace\r\n",
                "        self.storage_account = self.oea.storage_account\r\n",
                "\r\n",
                "        if workspace_name == 'prod' or workspace_name == 'production':\r\n",
                "            self.etl_logs = f'abfss://stage1@{self.storage_account}.dfs.core.windows.net/Transactional/SAP/etl-logs'\r\n",
                "            # self.etl_logs = 'abfss://etl-logs@' + self.storage_account + '.dfs.core.windows.net'\r\n",
                "        elif workspace_name == 'dev' or workspace_name == 'development':\r\n",
                "            self.etl_logs = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/dev/etl-logs'\r\n",
                "        else:\r\n",
                "            self.etl_logs = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/sandboxes/{workspace_name}/etl-logs'\r\n",
                "        \r\n",
                "    def to_logs_url(self, path):\r\n",
                "        if self.etl_logs is None:\r\n",
                "            self.set_logs_prefix()\r\n",
                "        if not path or path == '': raise ValueError('Specified path cannot be empty.')\r\n",
                "        if path.startswith('abfss://'): return path # if a url is given, just return that same url (allows to_url to be invoked just in case translation may be needed)\r\n",
                "        path_args = path.split('/')\r\n",
                "        stage = path_args.pop(0)\r\n",
                "        if stage == 'etl-logs': stage = self.etl_logs\r\n",
                "        else: raise ValueError(\"Logs Path must begin with 'etl-logs'\")\r\n",
                "        url = f\"{stage}/{'/'.join(path_args)}\"\r\n",
                "        logger.debug(f'to_url: {url}')\r\n",
                "        return url      \r\n",
                "\r\n",
                "    def create_log_dict(self, **kwargs):\r\n",
                "        return kwargs\r\n",
                "\r\n",
                "    def consolidate_logs(self, log_data, log_type):\r\n",
                "        if log_type == 'entity':\r\n",
                "            log_data['log_type'] = 'entity'\r\n",
                "            self.entity_logs.append(log_data)\r\n",
                "        elif log_type == 'stage':\r\n",
                "            log_data['log_type'] = 'stage'\r\n",
                "            self.stage_logs.append(log_data)\r\n",
                "        elif log_type == 'pipeline':\r\n",
                "            log_data['log_type'] = 'pipeline'\r\n",
                "            self.pipeline_logs.append(log_data)\r\n",
                "        else:\r\n",
                "            raise ValueError('Invalid Log Type')\r\n",
                "    def create_spark_df(self, log_type):\r\n",
                "        if log_type == 'entity':\r\n",
                "            df = self.spark.createDataFrame(self.entity_logs) \r\n",
                "        elif log_type == 'stage':\r\n",
                "            df = self.spark.createDataFrame(self.stage_logs) \r\n",
                "        elif log_type == 'pipeline':\r\n",
                "            df = self.spark.createDataFrame(self.pipeline_logs)\r\n",
                "        else:\r\n",
                "            raise ValueError('Invalid Log Type')\r\n",
                "        \r\n",
                "        return df\r\n",
                "    \r\n",
                "    def write_logs_to_delta_lake(self, df, log_type,destination_url, partitioning_cols = ['pipelineExecutionId']):\r\n",
                "        #TODO: Pending Edits\r\n",
                "        # logger.info('Dynamically over-write the partition')\r\n",
                "        self.spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\r\n",
                "        if log_type == 'entity':\r\n",
                "            primary_key = partitioning_cols = ['entityName','entityType', 'stageName','pipelineExecutionId']\r\n",
                "        if log_type == 'pipeline':\r\n",
                "            primary_key = partitioning_cols = ['pipelineExecutionId']\r\n",
                "        if log_type == 'stage':\r\n",
                "            primary_key = partitioning_cols = ['stageName','pipelineExecutionId']\r\n",
                "        \r\n",
                "        self.oea.upsert(df = df, \r\n",
                "                        destination_path = destination_url,\r\n",
                "                        primary_key = primary_key,#['RECORD', 'DistrictId', 'SchoolYear'],\r\n",
                "                        partitioning = False,\r\n",
                "                        partitioning_cols = [],\r\n",
                "                        surrogate_key = False)\r\n",
                "    \r\n",
                "    def add_etl_logs_to_lake_db(self, \r\n",
                "                                db_name, \r\n",
                "                                logs_base_path, \r\n",
                "                                log_type,\r\n",
                "                                overwrite = False):\r\n",
                "        logs_full_url = self.to_logs_url(f\"{logs_base_path}/log_type={log_type}\")\r\n",
                "        spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\r\n",
                "        if overwrite:\r\n",
                "            spark.sql(f'DROP TABLE IF EXISTS {db_name}.ETL{log_type}Logs')\r\n",
                "        spark.sql(f\"CREATE TABLE IF NOT EXISTS {db_name}.ETL{log_type}Logs using DELTA location '{logs_full_url}'\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Entity Frequency Processor"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "class EntityFrequencyProcessor:\r\n",
                "    def __init__(self, oea, filepath, highFrequentDelta = 1, moderateFrequentDelta = 5, lowFrequentDelta = 10, descriptorsDelta = 360):\r\n",
                "        self.oea = oea\r\n",
                "        self.filepath = filepath\r\n",
                "        self.highFrequentDelta = timedelta(days = highFrequentDelta)\r\n",
                "        self.moderateFrequentDelta = timedelta(days = moderateFrequentDelta)\r\n",
                "        self.lowFrequentDelta = timedelta(days = lowFrequentDelta)\r\n",
                "        self.descriptorsDelta = timedelta(days = descriptorsDelta)\r\n",
                "        \r\n",
                "    def load_lookup_df(self): \r\n",
                "        text_data = self.oea.get_text_from_path(self.filepath)\r\n",
                "        self.entity_freq_lookup_df = pd.read_csv(StringIO(text_data))\r\n",
                "\r\n",
                "    def set_freq_fiter_maps(self):\r\n",
                "        self.highFrequentMap = (self.entity_freq_lookup_df['resource_frequency_code'] == 'high') & (self.entity_freq_lookup_df['temp_timedelta'] >= self.highFrequentDelta)\r\n",
                "        self.moderateFrequentMap = (self.entity_freq_lookup_df['resource_frequency_code'] == 'moderate') & (self.entity_freq_lookup_df['temp_timedelta'] >= self.moderateFrequentDelta)\r\n",
                "        self.lowFrequentMap = (self.entity_freq_lookup_df['resource_frequency_code'] == 'low') & (self.entity_freq_lookup_df['temp_timedelta'] >= self.lowFrequentDelta)\r\n",
                "        self.descriptorsMap = (self.entity_freq_lookup_df['resource_frequency_code'] == 'descriptor') & (self.entity_freq_lookup_df['temp_timedelta'] >= self.descriptorsDelta)\r\n",
                "    \r\n",
                "    def return_entities_to_etl(self):\r\n",
                "        today_date = datetime.today() #.date()\r\n",
                "        self.entity_freq_lookup_df['temp_timedelta'] = today_date - pd.to_datetime(self.entity_freq_lookup_df['lastrundatetime']) #.dt.date\r\n",
                "        self.set_freq_fiter_maps()\r\n",
                "        \r\n",
                "        entities_to_etl_filter_map = self.highFrequentMap | self.lowFrequentMap | self.moderateFrequentMap | self.descriptorsMap\r\n",
                "        entities_to_etl = list(self.entity_freq_lookup_df[entities_to_etl_filter_map]['resource_full_name'].values)\r\n",
                "        self.entity_freq_lookup_df.drop(['temp_timedelta'], axis = 1, inplace = True)\r\n",
                "\r\n",
                "        entities_to_etl_dict = self.entity_freq_lookup_df.loc[entities_to_etl_filter_map,['resource_domain', 'resource_sub_name']].groupby('resource_domain').aggregate(list).to_dict()['resource_sub_name']\r\n",
                "        return entities_to_etl, entities_to_etl_dict\r\n",
                "\r\n",
                "    def update_lookup_df(self):\r\n",
                "        # TODO: WIP\r\n",
                "        today_date = datetime.today().date()\r\n",
                "        self.entity_freq_lookup_df.loc[self.highFrequentMap | self.lowFrequentMap | self.moderateFrequentMap | self.descriptorsMap, 'lastrundatetime'] = datetime.today()\r\n",
                "        self.entity_freq_lookup_df.loc[self.highFrequentMap | self.lowFrequentMap | self.moderateFrequentMap | self.descriptorsMap, 'lastrundate'] = today_date\r\n",
                "    \r\n",
                "    def write_lookup_df(self, destination_path):\r\n",
                "        data_str = self.entity_freq_lookup_df.to_csv(index=False)  # You can customize options based on your needs\r\n",
                "        destination_url = self.oea.to_url(destination_path)\r\n",
                "        mssparkutils.fs.put(destination_url, data_str, True)\r\n",
                "        "
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "class EntityFrequencyProcessor:\r\n",
                "    def __init__(self, oea, filepath, highFrequentDelta = 1, moderateFrequentDelta = 5, lowFrequentDelta = 10, descriptorsDelta = 360):\r\n",
                "        self.oea = oea\r\n",
                "        self.filepath = filepath\r\n",
                "        self.highFrequentDelta = timedelta(days = highFrequentDelta)\r\n",
                "        self.moderateFrequentDelta = timedelta(days = moderateFrequentDelta)\r\n",
                "        self.lowFrequentDelta = timedelta(days = lowFrequentDelta)\r\n",
                "        self.descriptorsDelta = timedelta(days = descriptorsDelta)\r\n",
                "        \r\n",
                "    def load_lookup_df(self): \r\n",
                "        text_data = self.oea.get_text_from_path(self.filepath)\r\n",
                "        self.entity_freq_lookup_df = pd.read_csv(StringIO(text_data))\r\n",
                "\r\n",
                "    def set_freq_fiter_maps(self):\r\n",
                "        self.highFrequentMap = (self.entity_freq_lookup_df['resource_frequency_code'] == 'high') & (self.entity_freq_lookup_df['temp_timedelta'] >= self.highFrequentDelta)\r\n",
                "        self.moderateFrequentMap = (self.entity_freq_lookup_df['resource_frequency_code'] == 'moderate') & (self.entity_freq_lookup_df['temp_timedelta'] >= self.moderateFrequentDelta)\r\n",
                "        self.lowFrequentMap = (self.entity_freq_lookup_df['resource_frequency_code'] == 'low') & (self.entity_freq_lookup_df['temp_timedelta'] >= self.lowFrequentDelta)\r\n",
                "        self.descriptorsMap = (self.entity_freq_lookup_df['resource_frequency_code'] == 'descriptor') & (self.entity_freq_lookup_df['temp_timedelta'] >= self.descriptorsDelta)\r\n",
                "    \r\n",
                "    def return_entities_to_etl(self):\r\n",
                "        today_date = datetime.today() #.date()\r\n",
                "        self.entity_freq_lookup_df['temp_timedelta'] = today_date - pd.to_datetime(self.entity_freq_lookup_df['lastrundatetime']) #.dt.date\r\n",
                "        self.set_freq_fiter_maps()\r\n",
                "        \r\n",
                "        entities_to_etl_filter_map = self.highFrequentMap | self.lowFrequentMap | self.moderateFrequentMap | self.descriptorsMap\r\n",
                "        entities_to_etl = list(self.entity_freq_lookup_df[entities_to_etl_filter_map]['resource_full_name'].values)\r\n",
                "        self.entity_freq_lookup_df.drop(['temp_timedelta'], axis = 1, inplace = True)\r\n",
                "\r\n",
                "        entities_to_etl_dict = self.entity_freq_lookup_df.loc[entities_to_etl_filter_map,['resource_domain', 'resource_sub_name']].groupby('resource_domain').aggregate(list).to_dict()['resource_sub_name']\r\n",
                "        return entities_to_etl, entities_to_etl_dict\r\n",
                "\r\n",
                "    def edgraph_return_entities_to_etl(self):\r\n",
                "        today_date = datetime.today() #.date()\r\n",
                "        self.entity_freq_lookup_df['temp_timedelta'] = today_date - pd.to_datetime(self.entity_freq_lookup_df['lastrundatetime']) #.dt.date\r\n",
                "        self.set_freq_fiter_maps()\r\n",
                "        \r\n",
                "        entities_to_etl_filter_map = self.highFrequentMap | self.lowFrequentMap | self.moderateFrequentMap | self.descriptorsMap\r\n",
                "        entities_to_etl = list(self.entity_freq_lookup_df[entities_to_etl_filter_map]['resource_full_name'].values)\r\n",
                "        self.entity_freq_lookup_df.drop(['temp_timedelta'], axis = 1, inplace = True)\r\n",
                "\r\n",
                "        # entities_to_etl_dict = self.entity_freq_lookup_df.loc[entities_to_etl_filter_map,['resource_domain', 'resource_sub_name']].groupby('resource_domain').aggregate(list).to_dict()['resource_sub_name']\r\n",
                "        return entities_to_etl, None\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "    def update_lookup_df(self):\r\n",
                "        # TODO: WIP\r\n",
                "        today_date = datetime.today().date()\r\n",
                "        self.entity_freq_lookup_df.loc[self.highFrequentMap | self.lowFrequentMap | self.moderateFrequentMap | self.descriptorsMap, 'lastrundatetime'] = datetime.today()\r\n",
                "        self.entity_freq_lookup_df.loc[self.highFrequentMap | self.lowFrequentMap | self.moderateFrequentMap | self.descriptorsMap, 'lastrundate'] = today_date\r\n",
                "    \r\n",
                "    def write_lookup_df(self, destination_path):\r\n",
                "        data_str = self.entity_freq_lookup_df.to_csv(index=False)  # You can customize options based on your needs\r\n",
                "        destination_url = self.oea.to_url(destination_path)\r\n",
                "        mssparkutils.fs.put(destination_url, data_str, True)\r\n",
                "        "
            ],
            "outputs": []
        }
    ]
}