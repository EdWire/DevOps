{
    "nbformat": 4,
    "nbformat_minor": 2,
    "metadata": {
        "save_output": true,
        "kernelspec": {
            "name": "synapse_pyspark",
            "display_name": "Synapse PySpark"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "execution_count": 17,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "apiLimit = batchLimit #= 200\r\n",
                "prepareEdFiMetaData = prepareEdFiMetadata #= False"
            ],
            "outputs": []
        },
        {
            "execution_count": 18,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# NOTE: DO NOT REMOVE\r\n",
                "minChangeVer = None\r\n",
                "maxChangeVer = None\r\n",
                "parameterized = False"
            ],
            "outputs": []
        },
        {
            "execution_count": 19,
            "cell_type": "code",
            "source": [
                "from notebookutils import mssparkutils\r\n",
                "from datetime import datetime"
            ],
            "outputs": []
        },
        {
            "execution_count": 20,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "start_time = datetime.now()"
            ],
            "outputs": []
        },
        {
            "execution_count": 21,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "import random\r\n",
                "import string\r\n",
                "\r\n",
                "characters = string.ascii_letters + string.digits\r\n",
                "ten_digit_alphanumeric = ''.join(random.choice(characters) for _ in range(10))\r\n",
                "\r\n",
                "pipelineExecutionId = executionId = f'TEST_{ten_digit_alphanumeric}'"
            ],
            "outputs": []
        },
        {
            "execution_count": 22,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# FIXME: Do not pass uncessary params\r\n",
                "input_params = {\r\n",
                "        \"kvName\": kvName,\r\n",
                "        \"kVName\": kvName,\r\n",
                "        \"workspace\": workspace,\r\n",
                "        \"apiUrl\": apiUrl,\r\n",
                "        \"ApiUrl\": apiUrl,\r\n",
                "        \"instanceId\": instanceId,\r\n",
                "        \"InstanceId\": instanceId,\r\n",
                "        \"moduleName\": moduleName,\r\n",
                "        \"apiLimit\": apiLimit,\r\n",
                "        \"minChangeVer\": minChangeVer,\r\n",
                "        \"maxChangeVer\": maxChangeVer,\r\n",
                "        \"schoolYear\": schoolYear,\r\n",
                "        \"SchoolYear\": schoolYear,\r\n",
                "        \"districtId\": districtId,\r\n",
                "        \"districtID\": districtId,\r\n",
                "        \"DistrictId\": districtId,\r\n",
                "        \"districtPath\": districtId,\r\n",
                "        \"edfi_url\": apiUrl,\r\n",
                "        \"pipelineExecutionId\" : pipelineExecutionId,\r\n",
                "        \"batchLimit\": apiLimit,\r\n",
                "        \"metadataUrl\": metadataUrl,\r\n",
                "        \"client_id\": client_id,\r\n",
                "        \"client_secret_id\": client_secret_id,\r\n",
                "        \"prepareEdFiMetadata\": prepareEdFiMetadata,\r\n",
                "        \"parameterized\": parameterized,\r\n",
                "        \"highFrequentDelta\" : highFrequentDelta,\r\n",
                "        \"moderateFrequentDelta\" : moderateFrequentDelta,  \r\n",
                "        \"lowFrequentDelta\" : lowFrequentDelta,  \r\n",
                "        \"descriptorsDelta\" : descriptorsDelta\r\n",
                "    }"
            ],
            "outputs": []
        },
        {
            "execution_count": 23,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "%run OEA/modules/Ed-Fi/v0.6/src/utilities/edfi_v0_6_edfi_py"
            ],
            "outputs": []
        },
        {
            "execution_count": 24,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "from datetime import datetime\r\n",
                "oea = EdFiOEAChild()   \r\n",
                "oea.set_workspace(workspace)"
            ],
            "outputs": []
        },
        {
            "execution_count": 25,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "def execute_etl_step(condition,\r\n",
                "                     nb_path,\r\n",
                "                     nb_timeout,\r\n",
                "                     nb_params,\r\n",
                "                     nb_error,\r\n",
                "                     etl_stage):\r\n",
                "    global etl_status\r\n",
                "    # FIXME: Parameterize etl_status \r\n",
                "    stage_start_time = datetime.now()\r\n",
                "    if nb_error is None:\r\n",
                "        etl_status = f\"{etl_stage} - RUNNING\"\r\n",
                "        try:\r\n",
                "            if condition:\r\n",
                "                mssparkutils.notebook.run(path = nb_path,\r\n",
                "                                          timeout_seconds = nb_timeout,\r\n",
                "                                          arguments = nb_params)\r\n",
                "                etl_status = f\"{etl_stage} - SUCCEEDED\"\r\n",
                "            else:\r\n",
                "                etl_status = f\"{etl_stage} - SKIPPED\"\r\n",
                "        except Exception as error_msg:\r\n",
                "            etl_status = f\"{etl_stage} - FAILED\"\r\n",
                "            logger.info(f\"{error_msg}\")\r\n",
                "            nb_error = error_msg\r\n",
                "    else:\r\n",
                "        pass\r\n",
                "    \r\n",
                "    stage_end_time = datetime.now()\r\n",
                "    log_data = pipeline_error_logger.create_log_dict(uniqueId = pipeline_error_logger.generate_random_alphanumeric(10), # Generate a random 10-character alphanumeric value\r\n",
                "                                            pipelineExecutionId = pipelineExecutionId,#'TEST_1234',#executionId,\r\n",
                "                                            sparkSessionId = spark.sparkContext.applicationId,\r\n",
                "                                            stageName = etl_stage,\r\n",
                "                                            start_time = stage_start_time,\r\n",
                "                                            end_time = stage_end_time,\r\n",
                "                                            run_status = 'SUCCEEDED' if nb_error is None else 'FAILED',\r\n",
                "                                            etl_status = etl_status,\r\n",
                "                                            error_description = str(nb_error))\r\n",
                "    pipeline_error_logger.consolidate_logs(log_data,'stage')\r\n",
                "    if ' - SUCCEEDED' in etl_status or ' - SKIPPED' in etl_status:\r\n",
                "        return etl_status, None\r\n",
                "    else:\r\n",
                "        return etl_status, nb_error\r\n",
                "\r\n",
                "def get_entity_logs_status(df, executionId, stageName):\r\n",
                "    df = df.filter((F.col('pipelineExecutionId') == executionId) & (F.col('stageName') == stageName))\r\n",
                "    df = df.withColumn('numRecordsFailed', F.col('totalNumOutputRows') - F.col('numInputRows'))\r\n",
                "\r\n",
                "    if df.filter(F.col(\"numRecordsFailed\") == 0).count() == df.count():\r\n",
                "        return 'success'\r\n",
                "    \r\n",
                "    if df.filter(F.col(\"numRecordsFailed\") != 0).count() == df.count():\r\n",
                "        return 'failure'\r\n",
                "    \r\n",
                "    if df.agg(F.sum(\"numRecordsFailed\")).collect()[0][0] != 0:\r\n",
                "        return 'partial-success'\r\n",
                "    \r\n",
                "    return 'unknown'"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Main Code"
            ],
            "outputs": []
        },
        {
            "execution_count": 28,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "nb_root = \"OEA/modules/Ed-Fi/v0.6/src/main\"\r\n",
                "nb_version = \"edfi_v0_6\"\r\n",
                "nb_error = None\r\n",
                "\r\n",
                "run_status = \"INITIATED\"\r\n",
                "etl_status = \"\""
            ],
            "outputs": []
        },
        {
            "execution_count": 29,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "pipeline_error_logger = ErrorLogging(spark = spark, \r\n",
                "                                     oea = oea, \r\n",
                "                                     logger = logger)\r\n",
                "\r\n",
                "try:\r\n",
                "    run_status = \"RUNNING\"\r\n",
                "    # NOTE: Prepare Meta Data \r\n",
                "    etl_status, nb_error = execute_etl_step(condition = prepareEdFiMetadata,\r\n",
                "                                            nb_path = f\"{nb_root}/{nb_version}_prepare_metadata\",\r\n",
                "                                            nb_timeout = 300,\r\n",
                "                                            nb_params = input_params,\r\n",
                "                                            nb_error = nb_error,\r\n",
                "                                            etl_stage = \"ed-fi: Metadata Preparation\")\r\n",
                "    # NOTE: Landing \r\n",
                "    etl_status, nb_error = execute_etl_step(condition = True,\r\n",
                "                                            nb_path = f\"{nb_root}/{nb_version}_land\",\r\n",
                "                                            nb_timeout = 1200,\r\n",
                "                                            nb_params = input_params,\r\n",
                "                                            nb_error = nb_error,\r\n",
                "                                            etl_stage = \"ed-fi: Landing\")\r\n",
                "    # NOTE: Ingestion\r\n",
                "    etl_status, nb_error = execute_etl_step(condition = True,\r\n",
                "                                            nb_path = f\"{nb_root}/{nb_version}_ingest\",\r\n",
                "                                            nb_timeout = 3600,\r\n",
                "                                            nb_params = input_params,\r\n",
                "                                            nb_error = nb_error,\r\n",
                "                                            etl_stage = \"ed-fi: Ingestion\")\r\n",
                "     # NOTE: Refinement\r\n",
                "    etl_status, nb_error = execute_etl_step(condition = True,\r\n",
                "                                            nb_path = f\"{nb_root}/{nb_version}_refine\",\r\n",
                "                                            nb_timeout = 14400,\r\n",
                "                                            nb_params = input_params,\r\n",
                "                                            nb_error = nb_error,\r\n",
                "                                            etl_stage = \"ed-fi: Refinement\")\r\n",
                "    if nb_error is not None:\r\n",
                "        raise Exception(f\"{nb_error}\")\r\n",
                "    run_status = \"SUCCEEDED\"\r\n",
                "except Exception as e:\r\n",
                "    run_status = \"FAILED\"\r\n",
                "    print(f'Pipeline Executed with Error {e}')"
            ],
            "outputs": []
        },
        {
            "execution_count": 30,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "destination_path = f'stage1/Transactional/Ed-Fi/{apiVersion}/DistrictId={districtId}/SchoolYear={schoolYear}/metadata-assets/frequency_etl.csv'  \r\n",
                "processor = EntityFrequencyProcessor(oea = oea, \r\n",
                "                                     filepath = destination_path, \r\n",
                "                                     highFrequentDelta = highFrequentDelta,#0.005, \r\n",
                "                                     moderateFrequentDelta = moderateFrequentDelta, #5, \r\n",
                "                                     lowFrequentDelta = lowFrequentDelta, #10, \r\n",
                "                                     descriptorsDelta = descriptorsDelta) #360)\r\n",
                "\r\n",
                "processor.load_lookup_df()\r\n",
                "entities_to_etl = processor.return_entities_to_etl()\r\n",
                "processor.update_lookup_df()\r\n",
                "processor.write_lookup_df(destination_path)\r\n",
                ""
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Pipeline Level Logs"
            ],
            "outputs": []
        },
        {
            "execution_count": 31,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "log_type = 'pipeline'\r\n",
                "\r\n",
                "end_time = datetime.now()\r\n",
                "log_data = pipeline_error_logger.create_log_dict(uniqueId = pipeline_error_logger.generate_random_alphanumeric(10), # Generate a random 10-character alphanumeric value\r\n",
                "                                                pipelineExecutionId = pipelineExecutionId,#'TEST_1234',#executionId,\r\n",
                "                                                sparkSessionId = spark.sparkContext.applicationId,\r\n",
                "                                                start_time = start_time,\r\n",
                "                                                end_time = end_time,\r\n",
                "                                                run_status = run_status,\r\n",
                "                                                etl_status = str(etl_status))\r\n",
                "pipeline_error_logger.consolidate_logs(log_data,'pipeline')\r\n",
                "df = pipeline_error_logger.create_spark_df('pipeline')\r\n",
                "\r\n",
                "\r\n",
                "pipeline_error_logger.write_logs_to_delta_lake(df = df, \r\n",
                "                             log_type = 'pipeline',\r\n",
                "                             destination_url = pipeline_error_logger.to_logs_url('etl-logs/log_type=pipeline'))\r\n",
                "pipeline_error_logger.add_etl_logs_to_lake_db(db_name = 'edfi_etl_logs',\r\n",
                "                                     logs_base_path = 'etl-logs',\r\n",
                "                                     log_type = 'pipeline',\r\n",
                "                                     overwrite = False)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Stage Level Logs"
            ],
            "outputs": []
        },
        {
            "execution_count": 32,
            "cell_type": "code",
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# FIXME: WIP\r\n",
                "df = pipeline_error_logger.create_spark_df('stage')\r\n",
                "df = df.withColumn('stage_status', F.lit('Unknown'))\r\n",
                "\r\n",
                "log_type = 'stage'\r\n",
                "destination_url = pipeline_error_logger.to_logs_url('etl-logs/log_type=stage')\r\n",
                "\r\n",
                "pipeline_error_logger.write_logs_to_delta_lake(df = df, \r\n",
                "                                               log_type = log_type,\r\n",
                "                                               destination_url = destination_url)\r\n",
                "pipeline_error_logger.add_etl_logs_to_lake_db(db_name = 'edfi_etl_logs',\r\n",
                "                                     logs_base_path = 'etl-logs',\r\n",
                "                                     log_type = 'stage',\r\n",
                "                                     overwrite = False)"
            ],
            "outputs": []
        }
    ]
}